---
File: .github/workflows/deduplicate.yml
---
# .github/workflows/deduplicate.yml
# Workflow to automatically find and deduplicate objects in gh-store


name: Deduplicate Objects

on:
  schedule:
    - cron: '0 2 * * *' # Run daily at 2:00 UTC
  push:
    branches: [ canonicalizer-newlabels ]
    paths: [ ".github/workflows/deduplicate.yml" ]
  # Allow manual triggering
  workflow_dispatch:
    inputs:
      dry_run:
        description: 'Dry run (no changes)'
        type: boolean
        default: false
      specific_object:
        description: 'Process specific object ID only (leave blank for all)'
        type: string
        required: false

jobs:
  deduplicate:
    runs-on: ubuntu-latest
    permissions:
      issues: write  # Needed to modify issues
      contents: read  # Needed to read code
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e .
      
      - name: Run deduplication tool
        id: deduplicate
        run: |
          echo "Running deduplication process..."
          
          # Set up basic command
          CMD="python -m gh_store.tools.canonicalize --token ${{ secrets.GITHUB_TOKEN }} --repo ${{ github.repository }}"
          
          # Check if dry run
          if [[ "${{ github.event.inputs.dry_run }}" == "true" ]]; then
            CMD="$CMD --dry-run"
            echo "::notice::Running in dry-run mode - no changes will be made"
          fi
          
          # Check if specific object requested
          if [[ "${{ github.event.inputs.specific_object }}" != "" ]]; then
            OBJECT_ID="${{ github.event.inputs.specific_object }}"
            CMD="$CMD --object-id $OBJECT_ID"
            echo "::notice::Processing specific object: $OBJECT_ID"
          fi
          
          # First find duplicates and capture output
          echo "Finding duplicates..."
          FIND_OUTPUT=$(eval "$CMD --find-duplicates" 2>&1)
          echo "$FIND_OUTPUT"
          
          # Check if duplicates found
          if [[ "$FIND_OUTPUT" == *"No duplicate objects found"* ]]; then
            echo "::notice::No duplicate objects found!"
            echo "duplicates_found=false" >> $GITHUB_OUTPUT
            exit 0
          else
            echo "duplicates_found=true" >> $GITHUB_OUTPUT
          fi
          
          # Process duplicates
          echo "Processing duplicates..."
          DEDUP_OUTPUT=$(eval "$CMD --deduplicate" 2>&1)
          echo "$DEDUP_OUTPUT"
          
          # Extract metrics
          OBJECTS_PROCESSED=$(echo "$DEDUP_OUTPUT" | grep -o "Processed [0-9]* objects" | grep -o "[0-9]*")
          if [[ -z "$OBJECTS_PROCESSED" ]]; then
            OBJECTS_PROCESSED=0
          fi
          
          echo "objects_processed=$OBJECTS_PROCESSED" >> $GITHUB_OUTPUT
      
      - name: Create summary
        if: steps.deduplicate.outputs.duplicates_found == 'true'
        run: |
          echo "# Deduplication Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [[ "${{ github.event.inputs.dry_run }}" == "true" ]]; then
            echo "**DRY RUN** - No changes were made" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "Found duplicates for ${{ steps.deduplicate.outputs.objects_processed }} objects" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "The deduplication process:" >> $GITHUB_STEP_SUMMARY
          echo "1. Identified objects with multiple issues" >> $GITHUB_STEP_SUMMARY
          echo "2. Selected the oldest issue as canonical for each object" >> $GITHUB_STEP_SUMMARY
          echo "3. Marked other issues as deprecated duplicates" >> $GITHUB_STEP_SUMMARY
          echo "4. Processed virtual merges to ensure data consistency" >> $GITHUB_STEP_SUMMARY
      
      - name: Handle no duplicates
        if: steps.deduplicate.outputs.duplicates_found == 'false'
        run: |
          echo "# Deduplication Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "✅ No duplicate objects found in the repository." >> $GITHUB_STEP_SUMMARY



---
File: .github/workflows/lint.yml
---
name: Lint

on:
  # push:
  #   branches: [ main ]
  # pull_request:
  #   branches: [ main ]
  workflow_dispatch:

jobs:
  lint:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: "3.12"
        cache: 'pip'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"
        
    - name: Check formatting with ruff
      run: ruff format --check .
        
    - name: Lint with ruff
      run: ruff check .
        
    - name: Type check with mypy
      run: mypy github_store



---
File: .github/workflows/live-test.yml
---
name: Live Test

on:
  workflow_dispatch:  # Allow manual trigger

jobs:
  store-data:
    runs-on: ubuntu-latest
    permissions:
      issues: write  # Required for creating/updating issues
      contents: read # Required for checkout

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: "3.12"
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e .
        
    - name: Generate and store random data
      run: |
        python - <<EOF
        from gh_store.core.store import GitHubStore
        import random
        from datetime import datetime, timezone
        import os

        # Initialize store with GitHub token
        store = GitHubStore(
            token=os.environ["GITHUB_TOKEN"],
            repo=os.environ["GITHUB_REPOSITORY"]
        )

        # Generate random data
        data = {
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "metrics": {
                "temperature": round(random.uniform(15.0, 25.0), 2),
                "humidity": round(random.uniform(40.0, 60.0), 2),
                "pressure": round(random.uniform(980.0, 1020.0), 2),
            },
            "counters": {
                "visitors": random.randint(100, 1000),
                "actions": random.randint(500, 5000),
                "errors": random.randint(0, 50)
            },
            "status": random.choice(["green", "yellow", "red"])
        }

        # Create or update the object
        try:
            # Try to update if exists
            store.update("daily-metrics123", data)
        except:
            # Create new if doesn't exist
            store.create("daily-metrics123", data)

        print("Successfully stored random data")
        EOF
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
  
  test-public-mode:
    needs: store-data
    runs-on: ubuntu-latest
    permissions:
      contents: read # Only need read permissions
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '18'
        
    - name: Install TypeScript client
      run: |
        cd typescript
        npm install
        npm run build
        
    - name: Run public mode retrieval test
      run: |
        cd typescript
        cat > test-public-mode.cjs << 'EOF'
        const fs = require('fs');
        const path = require('path');
        
        // Dynamically import the ESM module
        async function main() {
          // Use dynamic import to load the ESM module
          const { GitHubStoreClient } = await import('./dist/index.mjs');
          
          try {
            // Initialize client with null token (public mode)
            const client = new GitHubStoreClient(null, process.env.GITHUB_REPOSITORY);
            
            console.log('Running in public mode:', client.isPublic());
            
            // Try to retrieve the object we just created/updated
            const object = await client.getObject('daily-metrics123');
            
            console.log('Successfully retrieved object:');
            console.log('Object ID:', object.meta.objectId);
            console.log('Created at:', object.meta.createdAt);
            console.log('Version:', object.meta.version);
            console.log('Data sample:', JSON.stringify(object.data).substring(0, 200) + '...');
            
            // Try to get object history
            const history = await client.getObjectHistory('daily-metrics123');
            console.log('History entries:', history.length);
            
            // Validate we received data
            if (!object.data || !object.meta) {
              throw new Error('Retrieved object is missing data or metadata');
            }
            
            console.log('Public mode retrieval test passed!');
            process.exit(0);
          } catch (error) {
            console.error('Public mode test failed:', error);
            process.exit(1);
          }
        }
        
        main().catch(err => {
          console.error(err);
          process.exit(1);
        });
        EOF
        
        node --experimental-vm-modules test-public-mode.cjs



---
File: .github/workflows/llamero-summary.yml
---
name: Llamero Summarization

on:
  #push:
  workflow_dispatch:

jobs:
  generate-summaries:
    runs-on: ubuntu-latest
    permissions:
      contents: write
      pull-requests: write

    steps:
    - uses: actions/checkout@v4
      with:
        fetch-depth: 0

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        cache: 'pip'

    - name: Install llamero
      run: pip install llamero

    - name: Generate summaries
      run: llamero summarize all



---
File: .github/workflows/merge-objects.yml
---
# .github/workflows/merge-objects.yml
# Workflow for manually merging/aliasing objects in gh-store

name: Merge or Alias Objects

on:
  workflow_dispatch:
    inputs:
      operation:
        description: 'Operation type'
        required: true
        type: choice
        options:
          - create-alias
          - deprecate-duplicate
          - deprecate-merged
          - deprecate-replaced
      source_id:
        description: 'Source object ID'
        required: true
        type: string
      target_id:
        description: 'Target object ID'
        required: true
        type: string
      dry_run:
        description: 'Dry run (no changes)'
        type: boolean
        default: false

jobs:
  manage-objects:
    runs-on: ubuntu-latest
    permissions:
      issues: write  # Needed to modify issues
      contents: read  # Needed to read code
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e .
      
      - name: Run object operation
        id: object_operation
        run: |
          echo "Running ${{ github.event.inputs.operation }} on ${{ github.event.inputs.source_id }} -> ${{ github.event.inputs.target_id }}"
          
          # Set up basic command
          CMD="python -m gh_store.tools.canonicalize --token ${{ secrets.GITHUB_TOKEN }} --repo ${{ github.repository }}"
          CMD="$CMD --source-id ${{ github.event.inputs.source_id }} --target-id ${{ github.event.inputs.target_id }}"
          
          # Check if dry run
          if [[ "${{ github.event.inputs.dry_run }}" == "true" ]]; then
            CMD="$CMD --dry-run"
            echo "::notice::Running in dry-run mode - no changes will be made"
          fi
          
          # Map operation type to command
          case "${{ github.event.inputs.operation }}" in
            "create-alias")
              CMD="$CMD --create-alias"
              OPERATION_DESC="Creating alias from ${{ github.event.inputs.source_id }} to ${{ github.event.inputs.target_id }}"
              ;;
              
            "deprecate-duplicate")
              CMD="$CMD --deprecate --reason duplicate"
              OPERATION_DESC="Deprecating ${{ github.event.inputs.source_id }} as duplicate of ${{ github.event.inputs.target_id }}"
              ;;
              
            "deprecate-merged")
              CMD="$CMD --deprecate --reason merged"
              OPERATION_DESC="Deprecating ${{ github.event.inputs.source_id }} as merged into ${{ github.event.inputs.target_id }}"
              ;;
              
            "deprecate-replaced")
              CMD="$CMD --deprecate --reason replaced"
              OPERATION_DESC="Deprecating ${{ github.event.inputs.source_id }} as replaced by ${{ github.event.inputs.target_id }}"
              ;;
              
            *)
              echo "::error::Invalid operation type: ${{ github.event.inputs.operation }}"
              exit 1
              ;;
          esac
          
          echo "$OPERATION_DESC"
          echo "operation_desc=$OPERATION_DESC" >> $GITHUB_OUTPUT
          
          # Execute command
          echo "Executing: $CMD"
          OUTPUT=$(eval "$CMD" 2>&1)
          echo "$OUTPUT"
          
          # Check for success
          if [[ "$OUTPUT" == *"success"*"true"* ]]; then
            echo "success=true" >> $GITHUB_OUTPUT
          else
            echo "success=false" >> $GITHUB_OUTPUT
          fi
      
      - name: Create summary
        run: |
          echo "# Object Operation Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [[ "${{ github.event.inputs.dry_run }}" == "true" ]]; then
            echo "**DRY RUN** - No changes were made" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "Operation: ${{ steps.object_operation.outputs.operation_desc }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [[ "${{ steps.object_operation.outputs.success }}" == "true" ]]; then
            echo "✅ **Success!** The operation was completed successfully." >> $GITHUB_STEP_SUMMARY
          else
            echo "❌ **Failed!** The operation encountered an error." >> $GITHUB_STEP_SUMMARY
          fi



---
File: .github/workflows/process_update.yml
---
# .github/workflows/process_update.yml

name: Process Object Updates

on:
  issues:
    types: [reopened]

jobs:
  process-updates:
    runs-on: ubuntu-latest
    if: contains(github.event.issue.labels.*.name, 'stored-object')
    permissions:
      issues: write 
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'
          
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e .
          
      - name: Process Updates
        run: |
          python -m gh_store process-updates \
            --issue ${{ github.event.issue.number }} \
            --token ${{ secrets.GITHUB_TOKEN }} \
            --repo ${{ github.repository }}



---
File: .github/workflows/release.yml
---
name: Release

on:
  release:
    types: [published]

jobs:
  release:
    if: ${{ !startsWith(github.ref_name, 'ts-') }}
    runs-on: ubuntu-latest
    environment: release
    permissions:
      id-token: write
      contents: read

    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: "3.12"
        cache: 'pip'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install build twine tomli

    - name: Verify version sync
      run: |
        echo "Checking Python version sync..."
        pkg_version=$(python -c "import tomli; print(tomli.load(open('pyproject.toml', 'rb'))['project']['version'])")
        code_version=$(python -c "from gh_store.core.version import __version__; print(__version__)")
        
        if [ "$pkg_version" != "$code_version" ]; then
          echo "Version mismatch: pyproject.toml ($pkg_version) != version.py ($code_version)"
          exit 1
        fi

    - name: Build package
      run: python -m build
        
    - name: Publish to PyPI
      uses: pypa/gh-action-pypi-publish@release/v1



---
File: .github/workflows/snapshot.yml
---
name: Snapshot Management

on:
  # schedule:
  #   # Run daily at midnight UTC
  #   - cron: '0 0 * * *'
  workflow_dispatch:
    # Allow manual triggering
    inputs:
      force_new:
        description: 'Force creation of new snapshot'
        required: false
        type: boolean
        default: false

env:
  SNAPSHOT_PATH: data/snapshots/store-snapshot.json
  CONFIG_PATH: config.yml

jobs:
  snapshot:
    runs-on: ubuntu-latest
    permissions:
      contents: write  # Needed for pushing snapshot changes

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e .

      - name: Check for existing snapshot
        id: check_snapshot
        run: |
          if [ -f "${{ env.SNAPSHOT_PATH }}" ] && [ "${{ github.event.inputs.force_new }}" != "true" ]; then
            echo "exists=true" >> $GITHUB_OUTPUT
          else
            echo "exists=false" >> $GITHUB_OUTPUT
          fi

      - name: Create new snapshot
        if: steps.check_snapshot.outputs.exists == 'false'
        run: |
          # Ensure directory exists
          mkdir -p $(dirname ${{ env.SNAPSHOT_PATH }})
          
          # Create snapshot using CLI
          python -m gh_store snapshot \
            --token ${{ secrets.GITHUB_TOKEN }} \
            --repo ${{ github.repository }} \
            --output ${{ env.SNAPSHOT_PATH }} \
            --config ${{ env.CONFIG_PATH }}

      - name: Update existing snapshot
        if: steps.check_snapshot.outputs.exists == 'true'
        run: |
          python -m gh_store update-snapshot \
            --token ${{ secrets.GITHUB_TOKEN }} \
            --repo ${{ github.repository }} \
            --snapshot-path ${{ env.SNAPSHOT_PATH }} \
            --config ${{ env.CONFIG_PATH }}
            
      - name: Commit changes
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "${{ steps.check_snapshot.outputs.exists == 'true' && 'chore: Update data store snapshot' || 'chore: Create initial data store snapshot' }}"
          file_pattern: "${{ env.SNAPSHOT_PATH }}"



---
File: .github/workflows/test.yml
---
name: Tests

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:

jobs:
  test:
    runs-on: ubuntu-latest
    continue-on-error: ${{ matrix.experimental }}
    strategy:
      matrix:
        python-version: ["3.11", "3.12"]
        experimental: [false]
        include:
          - python-version: 3.13
            experimental: true

    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v5
      with:
        python-version: ${{ matrix.python-version }}
        cache: 'pip'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"
        
    - name: Run tests
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      run: |
        pytest tests/unit



---
File: .github/workflows/typescript.yml
---
# .github/workflows/typescript.yml
name: TypeScript Client

on:
  push:
    paths:
      - 'typescript/**'
      - '.github/workflows/typescript.yml'
    branches: [ main, ts ]
  pull_request:
    paths:
      - 'typescript/**'
      - '.github/workflows/typescript.yml'
    branches: [ main ]
  release:
    types: [published]
  workflow_dispatch:

jobs:
  build-and-test:
    # Skip this job for non-TypeScript releases
    if: ${{ !github.event.release || startsWith(github.ref_name, 'ts-') }}
    runs-on: ubuntu-latest
    defaults:
      run:
        working-directory: typescript

    steps:
    - uses: actions/checkout@v4
    
    # Initial clean npm install to generate package-lock.json
    - name: Initialize npm
      run: |
        # Force rebuild of package-lock.json if it exists
        if [ -f "package-lock.json" ]; then
          rm -f package-lock.json
        fi
        npm install --package-lock-only --no-audit
        git status
    
    # Now setup Node with caching
    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '20'
        registry-url: 'https://registry.npmjs.org'
        cache: 'npm'
        cache-dependency-path: './typescript/package-lock.json'
        
    - name: Install dependencies
      run: npm ci --prefer-offline

    - name: Sync version
      run: npm run prebuild
        
    - name: Type check
      run: npm run type-check
        
    - name: Lint
      run: npm run lint
        
    - name: Test
      run: npm run test
        
    - name: Build
      run: npm run build

    # Show build outputs for debugging
    - name: Show dist contents
      run: |
        echo "Dist directory contents:"
        ls -la dist/
        echo "Package exports:"
        cat package.json | jq '.exports'

    # Run packaging test after build
    - name: Test packaging
      run: |
        if [ ! -f "scripts/test-packaging.js" ]; then
          echo "Error: packaging test script not found"
          exit 1
        fi
        # Run packaging tests after build
        npm run test:packaging
    
    # Only run publish step on releases with a tag starting with 'ts-v'
    - name: Publish to npm
      if: ${{ github.event.release && startsWith(github.ref_name, 'ts-') }}
      run: npm publish
      env:
        NODE_AUTH_TOKEN: ${{ secrets.NPM_TOKEN }}



---
File: README.md
---
![An octocat poorly disguised as a database](static/shitty-logo.png)

# Github Issues as a Data Store

The `gh-store` package provides a data store implementation that uses GitHub Issues as a backend. The primary intended use case is for "github native"  applications which are constrained to Github Actions as the only available runtime, and free-tier github resources.

The data storage pattern presented here is inspired by https://github.com/utterance/utterances

## Key Features
- Store and version JSON objects using GitHub Issues
- Atomic updates through a comment-based event system
- Point-in-time snapshots for static site generation
- Built-in GitHub Actions integration

## Installation

```bash
pip install gh-store  # Requires Python 3.12+
```

## Prerequisites
- GitHub repository with Issues enabled
- GitHub token with `repo` scope
- For GitHub Actions: `issues` write permission

## Basic Usage

```python
from gh_store.core.store import GitHubStore

store = GitHubStore(
    token="github-token",
    repo="username/repository"
)

# Create object
store.create("metrics", {
    "count": 0,
    "last_updated": "2025-01-16T00:00:00Z"
})

# Update object
store.update("metrics", {"count": 1})

# Get current state
obj = store.get("metrics")
print(f"Current count: {obj.data['count']}")
```

## System Architecture

gh-store uses GitHub Issues as a versioned data store. Here's how the components work together:

### 1. Object Storage Model

Each stored object is represented by a GitHub Issue:
```
Issue #123
├── Labels: ["stored-object", "UID:metrics"]
├── Body: Current object state (JSON)
└── Comments: Update history
    ├── Comment 1: Update {"count": 1}
    ├── Comment 2: Update {"field": "value"}
    └── Each comment includes the 👍 reaction when processed
```

Key components:
- **Base Label** ("stored-object"): Identifies issues managed by gh-store
- **UID Label** ("UID:{object-id}"): Uniquely identifies each stored object
- **Issue Body**: Contains the current state as JSON
- **Comments**: Store update history
- **Reactions**: Track processed updates (👍)

### 2. Update Process

When updating an object:
1. New update is added as a comment with JSON changes
2. Issue is reopened to trigger processing
3. GitHub Actions workflow processes updates:
   - Gets all unprocessed comments (no 👍 reaction)
   - Applies updates in chronological order
   - Adds 👍 reaction to mark comments as processed
   - Updates issue body with new state
   - Closes issue when complete

### 3. Core Components

- **GitHubStore**: Main interface for CRUD operations
- **IssueHandler**: Manages GitHub Issue operations
- **CommentHandler**: Processes update comments

## GitHub Actions Integration

### Process Updates

```yaml
# .github/workflows/process_update.yml
name: Process Updates

on:
  issues:
    types: [reopened]

jobs:
  process:
    runs-on: ubuntu-latest
    if: contains(github.event.issue.labels.*.name, 'stored-object')
    permissions:
      issues: write
    steps:
      - uses: actions/checkout@v4
      - name: Process Updates
        run: |
          gh-store process-updates \
            --issue ${{ github.event.issue.number }} \
            --token ${{ secrets.GITHUB_TOKEN }} \
            --repo ${{ github.repository }}
```

### Create Snapshots

```yaml
# .github/workflows/snapshot.yml
name: Snapshot

on:
  schedule:
    - cron: '0 0 * * *'  # Daily
  workflow_dispatch:

jobs:
  snapshot:
    runs-on: ubuntu-latest
    permissions:
      contents: write
    steps:
      - uses: actions/checkout@v4
      - name: Create Snapshot
        run: |
          gh-store snapshot \
            --token ${{ secrets.GITHUB_TOKEN }} \
            --repo ${{ github.repository }} \
            --output data/store-snapshot.json
```

## CLI Commands

```bash
# Process updates for an issue
gh-store process-updates \
  --issue <issue-number> \
  --token <github-token> \
  --repo <owner/repo>

# Create snapshot
gh-store snapshot \
  --token <github-token> \
  --repo <owner/repo> \
  --output <path>

# Update existing snapshot
gh-store update-snapshot \
  --token <github-token> \
  --repo <owner/repo> \
  --snapshot-path <path>
```

# Configuration

A default configuration is automatically created at `~/.config/gh-store/config.yml` when first using the tool. You can customize this file or specify a different config location:

```python
store = GitHubStore(
    token="github-token",
    repo="username/repository",
    config_path=Path("custom_config.yml")
)
```

Default configuration:

```yaml
# gh_store/default_config.yml

store:
  # Base label for all stored objects
  base_label: "stored-object"
  
  # Prefix for unique identifier labels
  uid_prefix: "UID:"
  
  # Reaction settings
  # Limited to: ["+1", "-1", "laugh", "confused", "heart", "hooray", "rocket", "eyes"]
  reactions:
    processed: "+1"
    initial_state: "rocket"
  
  # Retry settings for GitHub API calls
  retries:
    max_attempts: 3
    backoff_factor: 2
    
  # Rate limiting
  rate_limit:
    max_requests_per_hour: 1000
    
  # Logging
  log:
    level: "INFO"
    format: "{time} | {level} | {message}"
```

## Object History

Each object maintains a complete history from its initial state through all updates:

```python
# Get object history
history = store.issue_handler.get_object_history("metrics")

# History includes initial state and all updates
for entry in history:
    print(f"[{entry['timestamp']}] {entry['type']}")
    print(f"Data: {entry['data']}")
```

The history includes:
- Initial state with timestamp and data
- All updates in chronological order
- Each entry's comment ID for reference

History is tracked through:
- Initial state comment marked with 🚀
- Update comments marked with 👍 when processed
- All changes preserved in chronological order

## Limitations

- Not suitable for high volume or high velocity (GitHub API limits)
  - Unique objects per store is limited only by the number of unique issues and labels
  - Per experimentation by SO users, github supports at least 10k+ unique labels within a single repo
  - Even if this is theoretically unbounded, it is inadvisable to use this system if you plan to store more than 10k+ items
  - As concrete examples of undocumented github api limitations:
    - There is no limit to the number of repos a single user may star, but above 7k stars the native github frontend breaks
    - There is no limit to the number of stars that can be added to a single star list, but above 3k stars the number of pages exceeds 100, and newly added stars after the 3000th member of the list will not be retrievable via the star list endpoint.
- Objects limited to Issue size (~65KB)
  - Github supports 10MB attachments to issues/comments, so limited future blob support is feasible
- Updates processed asynchronously via GitHub Actions
- Sensitive data that should not be publicly visible

## Development

```bash
# Install dev dependencies
pip install -e ".[dev]"

# Run tests
pytest

# Type checking & linting
mypy .
ruff check .
```

## License

MIT License - see [LICENSE](LICENSE)



---
File: gh_store/__init__.py
---




---
File: gh_store/__main__.py
---
# gh_store/__main__.py

import fire
from pathlib import Path
from loguru import logger

from .cli import commands

class CLI:
    """GitHub Issue Store CLI"""
    
    def __init__(self):
        """Initialize CLI with default config path"""
        self.default_config_path = Path.home() / ".config" / "gh-store" / "config.yml"
    
    def process_updates(
        self,
        issue: int,
        token: str | None = None,
        repo: str | None = None,
        config: str | None = None,
    ) -> None:
        """Process pending updates for a stored object"""
        return commands.process_updates(issue, token, repo, config)

    def snapshot(
        self,
        token: str | None = None,
        repo: str | None = None,
        output: str = "snapshot.json",
        config: str | None = None,
    ) -> None:
        """Create a full snapshot of all objects in the store"""
        return commands.snapshot(token, repo, output, config)

    def update_snapshot(
        self,
        snapshot_path: str,
        token: str | None = None,
        repo: str | None = None,
        config: str | None = None,
    ) -> None:
        """Update an existing snapshot with changes since its creation"""
        return commands.update_snapshot(snapshot_path, token, repo, config)

    def init(
        self,
        config: str | None = None
    ) -> None:
        """Initialize a new configuration file"""
        config_path = Path(config) if config else self.default_config_path
        commands.ensure_config_exists(config_path)
        logger.info(f"Configuration initialized at {config_path}")

    def create(
        self,
        object_id: str,
        data: str,
        token: str | None = None,
        repo: str | None = None,
        config: str | None = None,
    ) -> None:
        """Create a new object in the store
        
        Args:
            object_id: Unique identifier for the object
            data: JSON string containing object data
            token: GitHub token (optional)
            repo: GitHub repository (optional)
            config: Path to config file (optional)
        """
        return commands.create(object_id, data, token, repo, config)

    def get(
        self,
        object_id: str,
        output: str | None = None,
        token: str | None = None,
        repo: str | None = None,
        config: str | None = None,
    ) -> None:
        """Retrieve an object from the store
        
        Args:
            object_id: Unique identifier for the object
            output: Path to write output (optional)
            token: GitHub token (optional)
            repo: GitHub repository (optional)
            config: Path to config file (optional)
        """
        return commands.get(object_id, output, token, repo, config)

    def update(
        self,
        object_id: str,
        changes: str,
        token: str | None = None,
        repo: str | None = None,
        config: str | None = None,
    ) -> None:
        """Update an existing object
        
        Args:
            object_id: Unique identifier for the object
            changes: JSON string containing update data
            token: GitHub token (optional)
            repo: GitHub repository (optional)
            config: Path to config file (optional)
        """
        return commands.update(object_id, changes, token, repo, config)

    def delete(
        self,
        object_id: str,
        token: str | None = None,
        repo: str | None = None,
        config: str | None = None,
    ) -> None:
        """Delete an object from the store
        
        Args:
            object_id: Unique identifier for the object
            token: GitHub token (optional)
            repo: GitHub repository (optional)
            config: Path to config file (optional)
        """
        return commands.delete(object_id, token, repo, config)

    def history(
        self,
        object_id: str,
        output: str | None = None,
        token: str | None = None,
        repo: str | None = None,
        config: str | None = None,
    ) -> None:
        """Get complete history of an object
        
        Args:
            object_id: Unique identifier for the object
            output: Path to write output (optional)
            token: GitHub token (optional)
            repo: GitHub repository (optional)
            config: Path to config file (optional)
        """
        return commands.get_history(object_id, output, token, repo, config)

def main():
    fire.Fire(CLI)

if __name__ == "__main__":
    main()



---
File: gh_store/cli/commands.py
---
# gh_store/cli/commands.py

import os
import json
from pathlib import Path
from datetime import datetime
from zoneinfo import ZoneInfo
import shutil
import importlib.resources
from typing import Any
from loguru import logger

from ..core.store import GitHubStore
from ..core.exceptions import GitHubStoreError, ConfigurationError
from ..core.types import Json


def ensure_config_exists(config_path: Path) -> None:
    """Create default config file if it doesn't exist"""
    if not config_path.exists():
        logger.info(f"Creating default configuration at {config_path}")
        config_path.parent.mkdir(parents=True, exist_ok=True)
        
        # Copy default config from package
        with importlib.resources.files('gh_store').joinpath('default_config.yml').open('rb') as src:
            with open(config_path, 'wb') as dst:
                shutil.copyfileobj(src, dst)
        
        logger.info("Default configuration created. You can modify it at any time.")

def get_store(token: str | None = None, repo: str | None = None, config: str | None = None) -> GitHubStore:
    """Helper to create GitHubStore instance with CLI parameters using keyword arguments"""
    token = token or os.environ["GITHUB_TOKEN"]
    repo = repo or os.environ["GITHUB_REPOSITORY"]
    config_path = Path(config) if config else None
    
    if config_path:
        ensure_config_exists(config_path)
        
    return GitHubStore(token=token, repo=repo, config_path=config_path)

def get(
    object_id: str,
    output: str | None = None,
    token: str | None = None,
    repo: str | None = None,
    config: str | None = None,
) -> None:
    """Retrieve an object from the store"""
    try:
        store = get_store(token=token, repo=repo, config=config)
        obj = store.get(object_id)
        
        # Format output
        result = {
            "object_id": obj.meta.object_id,
            "created_at": obj.meta.created_at.isoformat(),
            "updated_at": obj.meta.updated_at.isoformat(),
            "version": obj.meta.version,
            "data": obj.data
        }
        
        if output:
            Path(output).write_text(json.dumps(result, indent=2))
            logger.info(f"Object written to {output}")
        else:
            print(json.dumps(result, indent=2))
            
    except Exception as e:
        logger.exception("Failed to get object")
        raise

def create(
    object_id: str,
    data: str,
    token: str | None = None,
    repo: str | None = None,
    config: str | None = None,
) -> None:
    """Create a new object in the store"""
    try:
        store = get_store(token, repo, config)
        # Parse data as JSON
        data_dict = json.loads(data)
        obj = store.create(object_id, data_dict)
        logger.info(f"Created object {obj.meta.object_id}")
        
    except json.JSONDecodeError:
        logger.error("Invalid JSON data provided")
        raise
    except Exception as e:
        logger.exception("Failed to create object")
        raise

def update(
    object_id: str,
    changes: str,
    token: str | None = None,
    repo: str | None = None,
    config: str | None = None,
) -> None:
    """Update an existing object"""
    try:
        store = get_store(token, repo, config)
        # Parse changes as JSON
        changes_dict = json.loads(changes)
        obj = store.update(object_id, changes_dict)
        logger.info(f"Updated object {obj.meta.object_id}")
        
    except json.JSONDecodeError:
        logger.error("Invalid JSON changes provided")
        raise
    except Exception as e:
        logger.exception("Failed to update object")
        raise

def delete(
    object_id: str,
    token: str | None = None,
    repo: str | None = None,
    config: str | None = None,
) -> None:
    """Delete an object from the store"""
    try:
        store = get_store(token, repo, config)
        store.delete(object_id)
        logger.info(f"Deleted object {object_id}")
        
    except Exception as e:
        logger.exception("Failed to delete object")
        raise

def get_history(
    object_id: str,
    output: str | None = None,
    token: str | None = None,
    repo: str | None = None,
    config: str | None = None,
) -> None:
    """Get complete history of an object"""
    try:
        store = get_store(token, repo, config)
        history = store.get_object_history(object_id)
        
        if output:
            Path(output).write_text(json.dumps(history, indent=2))
            logger.info(f"History written to {output}")
        else:
            print(json.dumps(history, indent=2))
            
    except Exception as e:
        logger.exception("Failed to get object history")
        raise

def process_updates(
    issue: int,
    token: str | None = None,
    repo: str | None = None,
    config: str | None = None,
) -> None:
    """Process pending updates for a stored object"""
    try:
        store = get_store(token, repo, config)
        obj = store.process_updates(issue)
        logger.info(f"Successfully processed updates for {obj.meta.object_id}")
        
    except GitHubStoreError as e:
        logger.error(f"Failed to process updates: {e}")
        raise SystemExit(1)
    except Exception as e:
        logger.exception("Unexpected error occurred")
        raise SystemExit(1)

def snapshot(
    token: str | None = None,
    repo: str | None = None,
    output: str = "snapshot.json",
    config: str | None = None,
) -> None:
    """Create a full snapshot of all objects in the store, including relationship info."""
    try:
        store = get_store(token, repo, config)
        
        # Use CanonicalStore if available for enhanced relationship handling
        has_canonical = False
        canonical_store = None
        
        try:
            from gh_store.tools.canonicalize import CanonicalStore
            canonical_store = CanonicalStore(token, repo, config_path=Path(config) if config else None)
            has_canonical = True
        except ImportError:
            logger.warning("Canonical store functionality not available")
        except Exception as e:
            logger.warning(f"Error initializing canonical store: {e}")
        
        # Create snapshot data
        snapshot_data = {
            "snapshot_time": datetime.now(ZoneInfo("UTC")).isoformat(),
            "repository": repo or os.environ.get("GITHUB_REPOSITORY", ""),
            "objects": {},
        }
        
        # Add relationships data if CanonicalStore is available
        if has_canonical and canonical_store:
            try:
                # Find all aliases
                aliases = canonical_store.find_aliases()
                if aliases:
                    snapshot_data["relationships"] = {
                        "aliases": aliases
                    }
            except Exception as e:
                logger.warning(f"Error finding aliases: {e}")
        
        # Add objects to snapshot
        object_count = 0
        for obj in store.list_all():
            object_count += 1
            snapshot_data["objects"][obj.meta.object_id] = {
                "data": obj.data,
                "meta": {
                    "issue_number": obj.meta.issue_number,
                    "object_id": obj.meta.object_id, # there's also an obj.meta.label field we can probably just drop?
                    "created_at": obj.meta.created_at.isoformat(),
                    "updated_at": obj.meta.updated_at.isoformat(),
                    "version": obj.meta.version,
                }
            }
        
        # Write to file
        output_path = Path(output)
        output_path.write_text(json.dumps(snapshot_data, indent=2))
        logger.info(f"Snapshot written to {output_path}")
        logger.info(f"Captured {object_count} objects")
        
        if has_canonical and "relationships" in snapshot_data:
            aliases_count = len(snapshot_data["relationships"].get("aliases", {}))
            logger.info(f"Included {aliases_count} alias relationships")
        
    except GitHubStoreError as e:
        logger.error(f"Failed to create snapshot: {e}")
        raise
    except Exception as e:
        logger.exception("Unexpected error occurred")
        raise

def update_snapshot(
    snapshot_path: str,
    token: str | None = None,
    repo: str | None = None,
    config: str | None = None,
) -> None:
    """Update an existing snapshot with changes since its creation"""
    try:
        store = get_store(token, repo, config)
        
        # Read existing snapshot
        snapshot_path = Path(snapshot_path)
        if not snapshot_path.exists():
            raise FileNotFoundError(f"Snapshot file not found: {snapshot_path}")
        
        with open(snapshot_path) as f:
            snapshot_data = json.loads(f.read())
        
        # Parse snapshot timestamp
        last_snapshot = datetime.fromisoformat(snapshot_data["snapshot_time"])
        logger.info(f"Updating snapshot from {last_snapshot}")
        
        # Track updated objects count
        updated_count = 0
        
        # Get updated objects and add them to snapshot
        for obj in store.list_updated_since(last_snapshot):
            # We only get here if the object passed the timestamp check in list_updated_since
            updated_count += 1
            snapshot_data["objects"][obj.meta.object_id] = {
                "data": obj.data,
                "meta": {
                    "issue_number": obj.meta.issue_number,
                    "object_id": obj.meta.object_id, # there's also an obj.meta.label field we can probably just drop?
                    "created_at": obj.meta.created_at.isoformat(),
                    "updated_at": obj.meta.updated_at.isoformat(),
                    "version": obj.meta.version,
                }
            }
        
        # Only update snapshot timestamp if we actually updated objects
        if updated_count > 0:
            snapshot_data["snapshot_time"] = datetime.now(ZoneInfo("UTC")).isoformat()
            
            # Write updated snapshot
            snapshot_path.write_text(json.dumps(snapshot_data, indent=2))
            logger.info(f"Updated {updated_count} objects in snapshot")
        else:
            logger.info("No updates found since last snapshot")
        
    except GitHubStoreError as e:
        logger.error(f"Failed to update snapshot: {e}")
        raise
    except Exception as e:
        logger.exception("Unexpected error occurred")
        raise



---
File: gh_store/core/access.py
---
# gh_store/core/access.py

from typing import TypedDict, Set
from pathlib import Path
import re
from github import Repository, Issue, IssueComment, GithubException
from loguru import logger

class UserInfo(TypedDict):
    login: str
    type: str

class AccessControl:
    """Handles access control validation for GitHub store operations"""
    
    CODEOWNERS_PATHS = [
        '.github/CODEOWNERS',
        'docs/CODEOWNERS',
        'CODEOWNERS'
    ]
    
    def __init__(self, repo: Repository.Repository):
        self.repo = repo
        self._owner_info: UserInfo | None = None
        self._codeowners: Set[str] | None = None

    def _get_owner_info(self) -> UserInfo:
        """Get repository owner information, caching the result"""
        if not self._owner_info:
            #owner = self.repo._owner
            owner = self.repo.owner
            # PyGithub returns ValuedAttribute objects, so we need to get their values
            self._owner_info = {
                'login': str(owner.login),  # Convert to string to ensure we have a plain value
                'type': str(owner.type)
            }
        return self._owner_info

    def _get_codeowners(self) -> Set[str]:
        """Parse CODEOWNERS file and extract authorized users"""
        if self._codeowners is not None:
            return self._codeowners

        content = self._find_codeowners_file()
        if not content:
            return set()

        self._codeowners = self._parse_codeowners_content(content)
        return self._codeowners
    
    def _find_codeowners_file(self) -> str | None:
        """Find and read the CODEOWNERS file content"""
        for path in self.CODEOWNERS_PATHS:
            try:
                content = self.repo.get_contents(path)
                if content:
                    return content.decoded_content.decode('utf-8')
            except GithubException:
                logger.debug(f"No CODEOWNERS found at {path}")
        return None
    
    def _parse_codeowners_content(self, content: str) -> Set[str]:
        """Parse CODEOWNERS content and extract authorized users"""
        codeowners = set()
        
        for line in content.splitlines():
            if self._should_skip_line(line):
                continue
                
            codeowners.update(self._extract_users_from_line(line))
                
        return codeowners
    
    def _should_skip_line(self, line: str) -> bool:
        """Check if line should be skipped (empty or comment)"""
        line = line.strip()
        return not line or line.startswith('#')
    
    def _extract_users_from_line(self, line: str) -> Set[str]:
        """Extract user and team names from a CODEOWNERS line"""
        users = set()
        parts = line.split()
        
        # Skip the path (first element)
        for part in parts[1:]:
            if part.startswith('@'):
                owner = part[1:]  # Remove @ prefix
                if '/' in owner:
                    # Handle team syntax (@org/team)
                    users.update(self._get_team_members(owner))
                else:
                    users.add(owner)
                    
        return users
    
    def _get_team_members(self, team_spec: str) -> Set[str]:
        """Get members of a team from GitHub API"""
        try:
            org, team = team_spec.split('/')
            team_obj = self.repo.organization.get_team_by_slug(team)
            return {member.login for member in team_obj.get_members()}
        except Exception as e:
            logger.warning(f"Failed to fetch team members for {team_spec}: {e}")
            return set()

    def _is_authorized(self, username: str | None) -> bool:
        """Check if a user is authorized (owner or in CODEOWNERS)"""
        if not username:
            return False
            
        # Repository owner is always authorized
        owner = self._get_owner_info()
        if username == owner['login']:
            return True
            
        # Check CODEOWNERS
        codeowners = self._get_codeowners()
        return username in codeowners

    def validate_issue_creator(self, issue: Issue.Issue) -> bool:
        """Check if issue was created by authorized user"""
        creator = issue.user.login if issue.user else None
        
        if not self._is_authorized(creator):
            logger.warning(
                f"Unauthorized creator for issue #{issue.number}: {creator}"
            )
            return False
            
        return True

    def validate_comment_author(self, comment: IssueComment.IssueComment) -> bool:
        """Check if comment was created by authorized user"""
        author = comment.user.login if comment.user else None
        
        if not self._is_authorized(author):
            logger.warning(
                f"Unauthorized author for comment {comment.id}: {author}"
            )
            return False
            
        return True

    def clear_cache(self) -> None:
        """Clear cached owner and CODEOWNERS information"""
        self._owner_info = None
        self._codeowners = None



---
File: gh_store/core/constants.py
---
# gh_store/core/constants.py

from enum import StrEnum # python 3.11

class LabelNames(StrEnum):
    """
    Constants for label names used by the gh-store system.
    
    Using str as a base class allows the enum values to be used directly as strings
    while still maintaining the benefits of an enumeration.
    """
    GH_STORE = "gh-store"  # System namespace label
    STORED_OBJECT = "stored-object"  # Active object label
    DEPRECATED = "deprecated-object"  # Deprecated object label
    UID_PREFIX = "UID:"  # Prefix for unique identifier labels
    ALIAS_TO_PREFIX = "ALIAS-TO:"  # Prefix for alias labels
    MERGED_INTO_PREFIX = "MERGED-INTO:"  # Prefix for merged object labels
    DEPRECATED_BY_PREFIX = "DEPRECATED-BY:"  # Prefix for referencing canonical issue
    DELETED = "archived"
    
    # def __str__(self) -> str:
    #     """Allow direct string usage in string contexts."""
    #     return self.value


class DeprecationReason(StrEnum):
    """Constants for deprecation reasons stored in metadata."""
    DUPLICATE = "duplicate"
    MERGED = "merged"
    REPLACED = "replaced"
    
    # def __str__(self) -> str:
    #     """Allow direct string usage in string contexts."""
    #     return self.value



---
File: gh_store/core/exceptions.py
---
# gh_store/core/exceptions.py

class GitHubStoreError(Exception):
    """Base exception for GitHub store errors"""
    pass

class ObjectNotFound(GitHubStoreError):
    """Raised when attempting to access a non-existent object"""
    pass

class InvalidUpdate(GitHubStoreError):
    """Raised when an update comment contains invalid JSON or schema"""
    pass

class ConcurrentUpdateError(GitHubStoreError):
    """Raised when concurrent updates are detected"""
    pass

class ConfigurationError(GitHubStoreError):
    """Raised when there's an error in the store configuration"""
    pass

class DuplicateUIDError(GitHubStoreError):
    """Raised when multiple issues have the same UID label"""
    pass

class AccessDeniedError(GitHubStoreError):
    pass



---
File: gh_store/core/store.py
---
# gh_store/core/store.py

from collections.abc import Iterator
from datetime import datetime
from pathlib import Path
import importlib.resources

from loguru import logger
from github import Github
from omegaconf import OmegaConf

from ..core.access import AccessControl
from ..core.constants import LabelNames
from ..handlers.issue import IssueHandler
from ..handlers.comment import CommentHandler
from .exceptions import AccessDeniedError, ConcurrentUpdateError
from .types import StoredObject, Update, Json


DEFAULT_CONFIG_PATH = Path.home() / ".config" / "gh-store" / "config.yml"

class GitHubStore:
    """Interface for storing and retrieving objects using GitHub Issues"""
    
    def __init__(
        self, 
        repo: str, 
        token: str|None = None,
        config_path: Path | None = None,
        max_concurrent_updates: int = 2, # upper limit number of comments to be processed on an issue before we stop adding updates
    ):
        """Initialize the store with GitHub credentials and optional config"""
        self.gh = Github(token)
        self.repo = self.gh.get_repo(repo)
        self.access_control = AccessControl(self.repo)
        self.max_concurrent_updates = max_concurrent_updates
        
        config_path = config_path or DEFAULT_CONFIG_PATH
        if not config_path.exists():
            # If default config doesn't exist, but we have a packaged default, use that
            if config_path == DEFAULT_CONFIG_PATH:
                with importlib.resources.files('gh_store').joinpath('default_config.yml').open('rb') as f:
                    self.config = OmegaConf.load(f)
            else:
                raise FileNotFoundError(f"Config file not found: {config_path}")
        else:
            self.config = OmegaConf.load(config_path)
        
        self.issue_handler = IssueHandler(self.repo, self.config)
        self.comment_handler = CommentHandler(self.repo, self.config)
        
        logger.info(f"Initialized GitHub store for repository: {repo}")

    def create(self, object_id: str, data: Json) -> StoredObject:
        """Create a new object in the store"""
        return self.issue_handler.create_object(object_id, data)

    def get(self, object_id: str) -> StoredObject:
        """Retrieve an object from the store"""
        return self.issue_handler.get_object(object_id)

    def update(self, object_id: str, changes: Json) -> StoredObject:
        """Update an existing object"""
        # Check if object is already being processed
        open_issue = None
        for open_issue in self.repo.get_issues(
            labels=[LabelNames.GH_STORE, self.config.store.base_label, f"UID:{object_id}"],
            state="open"): # TODO: use canonicalization machinery?
            break
        
        if open_issue: # count open comments, check against self.max_concurrent_updates
            issue_number = open_issue.meta.issue_number
            n_concurrent_updates = len(self.comment_handler.get_unprocessed_updates(issue_number))
            if n_concurrent_updates > self.max_concurrent_updates:
                raise ConcurrentUpdateError(
                    f"Object {object_id} already has {n_concurrent_updates} updates queued to be processed"
                )
        
        return self.issue_handler.update_object(object_id, changes)

    def delete(self, object_id: str) -> None:
        """Delete an object from the store"""
        self.issue_handler.delete_object(object_id)
        
    def process_updates(self, issue_number: int) -> StoredObject:
        """Process any unhandled updates on an issue"""
        logger.info(f"Processing updates for issue #{issue_number}")
        
        issue = self.repo.get_issue(issue_number)
        if not self.access_control.validate_issue_creator(issue):
            raise AccessDeniedError(
                "Updates can only be processed for issues created by "
                "repository owner or authorized CODEOWNERS"
            )
        
        # Get all unprocessed comments - this handles comment-level auth
        updates = self.comment_handler.get_unprocessed_updates(issue_number)
        
        # Apply updates in sequence
        obj = self.issue_handler.get_object_by_number(issue_number)
        for update in updates:
            obj = self.comment_handler.apply_update(obj, update)
        
        # Persist final state and mark comments as processed
        self.issue_handler.update_issue_body(issue_number, obj)
        self.comment_handler.mark_processed(issue_number, updates)
        
        return obj
    
    def list_all(self) -> Iterator[StoredObject]:
        """List all objects in the store, indexed by object ID"""
        logger.info("Fetching all stored objects")
        
        # Get all closed issues with base label (active objects)
        issues_generator = self.repo.get_issues(
            state="closed",
            labels=[LabelNames.GH_STORE, self.config.store.base_label]
        )
        
        for idx, issue in enumerate(issues_generator):
            if any(label.name == "archived" for label in issue.labels):
                continue
            try:
                yield StoredObject.from_issue(issue)
            except ValueError as e:
                logger.warning(f"Skipping issue #{issue.number}: {e}")        
        logger.info(f"Found {idx+1} stored objects")
    
    def list_updated_since(self, timestamp: datetime) -> Iterator[StoredObject]:
        """
        List objects updated since given timestamp.

        The main purpose of this function is for delta updating snapshots.
        The use of "updated" here specifically refers to updates *which have already been processed*
        with respect to the "view" on the object provided by the issue description body, i.e. it
        only fetches closed issued.
        
        Issues that have updates pending processing (i.e. which are open and have unreacted update comments) 
        are processed on an issue-by-issue basis by `GitHubStore.process_updates`.
        """
        logger.info(f"Fetching objects updated since {timestamp}")
        
        # Get all objects with base label that are closed (active objects)
        # on the `since` parameter:
        #     "Only show results that were last updated after the given time. This is a timestamp in ISO 8601 format: YYYY-MM-DDTHH:MM:SSZ."
        # https://docs.github.com/en/rest/issues/issues?apiVersion=2022-11-28#list-repository-issues
        issues_generator = self.repo.get_issues(
            state="closed",
            labels=[LabelNames.GH_STORE, self.config.store.base_label],
            since=timestamp 
        )
    
        found_count = 0
        yielded_count = 0
                    
        for idx, issue in enumerate(issues_generator):
            found_count += 1
            # Skip archived issues
            if any(label.name == "archived" for label in issue.labels):
                continue
                
            try:
                obj = StoredObject.from_issue(issue)
                # Double check the timestamp (since GitHub's since parameter includes issues with comments after the timestamp)
                if obj.meta.updated_at > timestamp:
                    yielded_count += 1
                    yield obj
                else:
                    logger.debug(f"Skipping issue #{issue.number}: last updated at {obj.meta.updated_at}, before {timestamp}")
            except ValueError as e:
                logger.warning(f"Skipping issue #{issue.number}: {e}")
        
        logger.info(f"Found {found_count} issues, yielded {yielded_count} updated objects")
        
    def get_object_history(self, object_id: str) -> list[dict]:
        """Get complete history of an object"""
        return self.issue_handler.get_object_history(object_id)



---
File: gh_store/core/types.py
---
# gh_store/core/types.py

from dataclasses import dataclass, asdict
from datetime import datetime
from typing import Self, TypeAlias
import json

from github import Issue

from .constants import LabelNames

Json: TypeAlias = dict[str, "Json"] | list["Json"] | str | int | float | bool | None


# This one method feels like it belongs on the IssueHandler, but really it pairs with StoredObject.from_issue
def get_object_id_from_labels(issue: Issue) -> str:
    """
    Extract bare object ID from issue labels, removing any prefix.
    
    Args:
        issue: GitHub issue object with labels attribute
        
    Returns:
        str: Object ID without prefix
        
    Raises:
        ValueError: If no matching label is found
    """
    for label in issue.labels:
        # Get the actual label name, handling both string and Mock objects
        # ... or are we just mocking poorly?
        label_name = getattr(label, 'name', label)
        
        if (isinstance(label_name, str) and label_name.startswith(LabelNames.UID_PREFIX)):
            return label_name[len(LabelNames.UID_PREFIX):]
            
    raise ValueError(f"No UID label found with prefix {LabelNames.UID_PREFIX}")

@dataclass
class ObjectMeta:
    """Metadata for a stored object"""
    object_id: str
    label: str
    issue_number: int  # Added field to track GitHub issue number
    created_at: datetime
    updated_at: datetime
    version: int

@dataclass
class StoredObject:
    """An object stored in the GitHub Issues store"""
    meta: ObjectMeta
    data: Json

    @classmethod
    def from_issue(cls, issue: Issue, version: int = 1) -> Self:
        object_id = get_object_id_from_labels(issue)
        data = json.loads(issue.body)
        meta = ObjectMeta(
            object_id=object_id,
            label=object_id,
            issue_number=issue.number,  # Include issue number
            created_at=issue.created_at,
            updated_at=issue.updated_at,
            version=version,
        )
        return cls(meta=meta, data=data)

@dataclass
class Update:
    """An update to be applied to a stored object"""
    comment_id: int
    timestamp: datetime
    changes: Json

@dataclass
class CommentMeta:
    """Metadata included with each comment"""
    client_version: str
    timestamp: str
    update_mode: str
    issue_number: int  # Added field to track GitHub issue number
    
    def to_dict(self) -> dict:
        """Convert to dictionary for JSON serialization"""
        return asdict(self)

@dataclass
class CommentPayload:
    """Full comment payload structure"""
    _data: Json
    _meta: CommentMeta
    type: str | None = None

    def to_dict(self) -> dict:
        """Convert to dictionary for JSON serialization"""
        return {
            "_data": self._data,
            "_meta": self._meta.to_dict(),
            **({"type": self.type} if self.type is not None else {})
        }



---
File: gh_store/core/version.py
---
# gh_store/core/version.py
import importlib.metadata
import os
from pathlib import Path

def get_version() -> str:
    """Get version from pyproject.toml metadata or fallback to manual version"""
    try:
        return importlib.metadata.version("gh-store")
    except importlib.metadata.PackageNotFoundError:
        # During development, read directly from pyproject.toml
        root_dir = Path(__file__).parent.parent.parent
        pyproject_path = root_dir / "pyproject.toml"
        
        if pyproject_path.exists():
            import tomli
            with open(pyproject_path, "rb") as f:
                pyproject = tomli.load(f)
                return pyproject["project"]["version"]
        
        return "0.5.1"  # Fallback version

__version__ = get_version()
CLIENT_VERSION = __version__



---
File: gh_store/default_config.yml
---
# gh_store/default_config.yml

store:
  # Base label for all stored objects
  base_label: "stored-object"
  
  # Prefix for unique identifier labels
  uid_prefix: "UID:"
  
  # Reaction settings
  # Limited to: ["+1", "-1", "laugh", "confused", "heart", "hooray", "rocket", "eyes"]
  reactions:
    processed: "+1"
    initial_state: "rocket"
  
  # Retry settings for GitHub API calls
  retries:
    max_attempts: 3
    backoff_factor: 2
    
  # Rate limiting
  rate_limit:
    max_requests_per_hour: 1000
    
  # Logging
  log:
    level: "INFO"
    format: "{time} | {level} | {message}"



---
File: gh_store/handlers/comment.py
---
# gh_store/handlers/comment.py

import json
from typing import Sequence
from datetime import datetime, timezone
from loguru import logger
from github import Repository, IssueComment
from omegaconf import DictConfig

from ..core.types import StoredObject, Update, CommentPayload, CommentMeta
from ..core.exceptions import InvalidUpdate
from ..core.access import AccessControl
from ..core.version import CLIENT_VERSION

class CommentHandler:
    """Handles processing of update comments"""
    
    def __init__(self, repo: Repository.Repository, config: DictConfig):
        self.repo = repo
        self.config = config
        self.processed_reaction = config.store.reactions.processed
        self.initial_state_reaction = config.store.reactions.initial_state
        self.access_control = AccessControl(repo)

    def _validate_metadata(self, metadata: dict) -> bool:
        """Validate that metadata contains all required fields"""
        return all(
            key in metadata and metadata[key] is not None
            for key in ['client_version', 'timestamp', 'update_mode']
        )

    def get_unprocessed_updates(self, issue_number: int) -> list[Update]:
        """Get all unprocessed updates from issue comments"""
        logger.info(f"Fetching unprocessed updates for issue #{issue_number}")
        
        issue = self.repo.get_issue(issue_number)
        updates = []
        
        for comment in issue.get_comments():
            if self._is_processed(comment):
                continue
                
            try:
                comment_payload = json.loads(comment.body)
                
                # Handle old format comments (backwards compatibility)
                if not isinstance(comment_payload, dict) or ('_data' not in comment_payload):
                    comment_payload = {
                        '_data': comment_payload,
                        '_meta': {
                            'client_version': 'legacy',
                            'timestamp': comment.created_at.isoformat(),
                            'update_mode': 'append'
                        }
                    }
                elif not self._validate_metadata(comment_payload.get('_meta', {})):
                    logger.warning(f"Skipping comment {comment.id} due to invalid metadata")
                    continue

                # Skip initial state comments
                if comment_payload.get('type') == 'initial_state':
                    logger.debug(f"Skipping initial state comment {comment.id}")
                    continue
                    
                # Skip comments from unauthorized users
                if not self.access_control.validate_comment_author(comment):
                    logger.debug(f"Skipping unauthorized comment {comment.id}")
                    continue
                    
                updates.append(Update(
                    comment_id=comment.id,
                    timestamp=comment.created_at,
                    changes=comment_payload['_data']
                ))
            except json.JSONDecodeError:
                # Not JSON, skip it
                logger.debug(f"Skipping non-JSON comment {comment.id}")
                continue
            except KeyError as e:
                logger.warning(f"Malformed comment payload in {comment.id}: {e}")
                continue
        
        return sorted(updates, key=lambda u: u.timestamp)

    def apply_update(self, obj: StoredObject, update: Update) -> StoredObject:
        """Apply an update to an object"""
        logger.info(f"Applying update {update.comment_id} to {obj.meta.object_id}")
        
        # Deep merge the changes into the existing data
        updated_data = self._deep_merge(obj.data, update.changes)
        
        # Create new object with updated data and incremented version
        return StoredObject(
            meta=obj.meta,
            data=updated_data
        )

    def mark_processed(
        self, 
        issue_number: int,
        updates: Sequence[Update]
    ) -> None:
        """Mark comments as processed by adding reactions"""
        logger.info(f"Marking {len(updates)} comments as processed")
        
        issue = self.repo.get_issue(issue_number)
        
        for update in updates:
            for comment in issue.get_comments():
                if comment.id == update.comment_id:
                    comment.create_reaction(self.processed_reaction)
                    break

    @staticmethod
    def create_comment_payload(data: dict, issue_number: int, comment_type: str | None = None, update_mode: str = "append") -> CommentPayload:
        """Create a properly structured comment payload"""
        meta = CommentMeta(
            client_version=CLIENT_VERSION,
            timestamp=datetime.now(timezone.utc).isoformat(),
            update_mode=update_mode,
            issue_number=issue_number  # Include issue number in metadata
        )
        
        return CommentPayload(
            _data=data,
            _meta=meta,
            type=comment_type
        )

    def _is_processed(self, comment: IssueComment.IssueComment) -> bool:
        """Check if a comment has been processed"""
        for reaction in comment.get_reactions():
            if reaction.content == self.processed_reaction:
                return True
        return False

    def _deep_merge(self, base: dict, update: dict) -> dict:
        """Deep merge two dictionaries"""
        result = base.copy()
        
        for key, value in update.items():
            if key in result and isinstance(result[key], dict) and isinstance(value, dict):
                result[key] = self._deep_merge(result[key], value)
            else:
                result[key] = value
                
        return result



---
File: gh_store/handlers/issue.py
---
# gh_store/handlers/issue.py

from datetime import datetime, timezone
from loguru import logger
from github import Repository, Issue
import json
from omegaconf import DictConfig

from ..core.constants import LabelNames
from ..core.exceptions import ObjectNotFound, DuplicateUIDError
from ..core.types import StoredObject, ObjectMeta, Json, CommentPayload, CommentMeta
from ..core.version import CLIENT_VERSION
from .comment import CommentHandler


from time import sleep
from github.GithubException import RateLimitExceededException


class IssueHandler:
    """Handles GitHub Issue operations for stored objects"""
    
    def __init__(self, repo: Repository.Repository, config: DictConfig):
        self.repo = repo
        self.config = config
        self.base_label = LabelNames.STORED_OBJECT # could this be an OR?
            
    def create_object(self, object_id: str, data: Json) -> StoredObject:
        """Create a new issue to store an object"""
        logger.info(f"Creating new object: {object_id}")
        
        # Create uid label with prefix
        uid_label = f"{LabelNames.UID_PREFIX}{object_id}"
        
        # Get labels to apply - includes LabelNames.GH_STORE for system boundary
        # Note: The str() conversion is handled automatically due to our __str__ method
        labels_to_apply = [LabelNames.GH_STORE, self.base_label, uid_label]
        
        # Ensure required labels exist
        self._ensure_labels_exist(labels_to_apply)
        
        # Create issue with object data and all required labels
        issue = self.repo.create_issue(
            title=f"Stored Object: {object_id}",
            body=json.dumps(data, indent=2),
            labels=labels_to_apply
        )
        
        # Create initial state comment with metadata including issue number
        initial_state_comment = CommentHandler.create_comment_payload(
            data=data,
            issue_number=issue.number,  # Include issue number
            comment_type='initial_state'
        )
        
        comment = issue.create_comment(json.dumps(initial_state_comment.to_dict(), indent=2))
        
        # Mark as processed to prevent update processing
        comment.create_reaction(self.config.store.reactions.processed)
        comment.create_reaction(self.config.store.reactions.initial_state)

        # Close issue immediately to indicate no processing needed
        issue.edit(state="closed")

        # # Create metadata
        # meta = ObjectMeta(
        #     object_id=object_id,
        #     label=uid_label,
        #     issue_number=issue.number,  # Include issue number
        #     created_at=issue.created_at,
        #     updated_at=issue.updated_at,
        #     version=1
        # )
        
        # return StoredObject(meta=meta, data=data)
    
        return StoredObject.from_issue(issue, version=1)

    def _ensure_labels_exist(self, labels: list[str]) -> None:
        """Create labels if they don't exist"""
        existing_labels = {label.name for label in self.repo.get_labels()}
        
        for label in labels:
            if label not in existing_labels:
                logger.info(f"Creating label: {label}")
                self.repo.create_label(
                    name=label,
                    color="0366d6"  # GitHub's default blue
                )

    def _with_retry(self, func, *args, **kwargs):
        """Execute a function with retries on rate limit"""
        max_attempts = self.config.store.retries.max_attempts
        backoff = self.config.store.retries.backoff_factor
        
        for attempt in range(max_attempts):
            try:
                return func(*args, **kwargs)
            except RateLimitExceededException:
                if attempt == max_attempts - 1:
                    raise
                sleep(backoff ** attempt)
        
        raise RuntimeError("Should not reach here")

    def get_object(self, object_id: str) -> StoredObject:
        """Retrieve an object by its ID"""
        logger.info(f"Retrieving object: {object_id}")
        
        uid_label = f"{LabelNames.UID_PREFIX}{object_id}"
        
        # Query for issue with matching labels - must have stored-object (active)
        issues = list(self._with_retry(
            self.repo.get_issues,
            labels=[LabelNames.GH_STORE, self.base_label, uid_label],
            state="closed"
        ))
        
        if not issues:
            raise ObjectNotFound(f"No object found with ID: {object_id}")
        elif len(issues) > 1:
            issue_numbers = [i.number for i in issues]
            raise DuplicateUIDError(
                f"Found multiple issues ({issue_numbers}) with label: {uid_label}"
            )
        
        issue = issues[0]
        return StoredObject.from_issue(issue, version=self._get_version(issue))

    def get_object_history(self, object_id: str) -> list[dict]:
        """Get complete history of an object, including initial state"""
        logger.info(f"Retrieving history for object: {object_id}")
        
        uid_label = f"{LabelNames.UID_PREFIX}{object_id}"
        
        # Query for issue with matching labels
        issues = list(self._with_retry(
            self.repo.get_issues,
            labels=[LabelNames.GH_STORE, self.base_label, uid_label],
            state="all"
        ))
        
        if not issues:
            raise ObjectNotFound(f"No object found with ID: {object_id}")
            
        issue = issues[0]
        history = []
        
        # Process all comments chronologically
        for comment in issue.get_comments():
            try:
                comment_data = json.loads(comment.body)
                
                # Handle old format comments (backwards compatibility)
                if isinstance(comment_data, dict) and 'type' in comment_data and comment_data['type'] == 'initial_state':
                    # Old initial state format
                    comment_type = 'initial_state'
                    data = comment_data['data']
                elif isinstance(comment_data, dict) and '_data' in comment_data:
                    # New format
                    comment_type = comment_data.get('type', 'update')
                    data = comment_data['_data']
                else:
                    # Legacy update format (raw data)
                    comment_type = 'update'
                    data = comment_data

                # Build metadata
                if isinstance(comment_data, dict) and '_meta' in comment_data:
                    metadata = comment_data['_meta']
                else:
                    metadata = {
                        'client_version': 'legacy',
                        'timestamp': comment.created_at.isoformat(),
                        'update_mode': 'append'
                    }
                
                history.append({
                    "timestamp": comment.created_at.isoformat(),
                    "type": comment_type,
                    "data": data,
                    "comment_id": comment.id,
                    "metadata": metadata
                })
            except (json.JSONDecodeError, KeyError) as e:
                logger.warning(f"Skipping comment {comment.id}: {e}")
                
        return history
        
    def get_object_by_number(self, issue_number: int) -> StoredObject:
        """Retrieve an object by issue number"""
        logger.info(f"Retrieving object by issue #{issue_number}")
        
        issue = self.repo.get_issue(issue_number)
        return StoredObject.from_issue(issue, version=self._get_version(issue))


    def update_issue_body(self, issue_number: int, obj: StoredObject) -> None:
        """Update the issue body with new object state"""
        logger.info(f"Updating issue #{issue_number} with new state")
        
        issue = self.repo.get_issue(issue_number)
        issue.edit(
            body=json.dumps(obj.data, indent=2),
            state="closed"
        )

    def update_object(self, object_id: str, changes: Json) -> StoredObject:
        """Update an object by adding a comment and reopening the issue"""
        logger.info(f"Updating object: {object_id}")
        
        # Get the object's issue
        issues = list(self.repo.get_issues(
            labels=[LabelNames.GH_STORE, self.base_label, f"{LabelNames.UID_PREFIX}{object_id}"],
            state="closed"
        ))
        
        if not issues:
            raise ObjectNotFound(f"No object found with ID: {object_id}")
        
        issue = issues[0]
        
        # Create update payload with metadata
        update_payload = CommentPayload(
            _data=changes,
            _meta=CommentMeta(
                client_version=CLIENT_VERSION,
                timestamp=datetime.now(timezone.utc).isoformat(),
                issue_number=issue.number,  # Include issue number
                update_mode="append"
            ),
            type=None
        )
        
        # Add update comment
        issue.create_comment(json.dumps(update_payload.to_dict(), indent=2))
        
        # Reopen issue to trigger processing
        issue.edit(state="open")
        
        # Return current state
        return self.get_object(object_id)
    
    def delete_object(self, object_id: str) -> None:
        """Delete an object by closing and archiving its issue"""
        logger.info(f"Deleting object: {object_id}")
        
        issues = list(self.repo.get_issues(
            labels=[LabelNames.GH_STORE, self.base_label, f"{LabelNames.UID_PREFIX}{object_id}"],
            state="all"
        ))
        
        if not issues:
            raise ObjectNotFound(f"No object found with ID: {object_id}")
        
        issue = issues[0]
        issue.edit(
            state="closed",
            labels=[LabelNames.DELETED, LabelNames.GH_STORE, f"{LabelNames.UID_PREFIX}{object_id}"]
        )
        
        # Remove stored-object label to mark as inactive
        issue.remove_from_labels(self.base_label)

    def _get_version(self, issue) -> int:
        """Extract version number from issue"""
        comments = list(issue.get_comments())
        return len(comments) + 1



---
File: gh_store/tools/canonicalize.py
---
# gh_store/tools/canonicalize.py
"""
Tool for managing object canonicalization, aliasing, and deduplication in gh-store.

This module provides functionality to:
1. Find duplicate objects
2. Establish canonical objects with aliases
3. Handle virtual merging of data from multiple related issues
"""

import argparse
import json
from collections import defaultdict
from datetime import datetime, timezone
from pathlib import Path
from typing import Dict, List, Optional, Set, Any, Tuple

from loguru import logger
from github import Github
from github.Issue import Issue
from github.Repository import Repository

from ..core.constants import LabelNames, DeprecationReason
from ..core.exceptions import ObjectNotFound
from ..core.store import GitHubStore
from ..core.types import StoredObject, ObjectMeta, Json, CommentPayload, CommentMeta
from ..core.version import CLIENT_VERSION


class CanonicalStore(GitHubStore):
    """Extended GitHub store with canonicalization and aliasing support."""
    
    def __init__(self, token: str, repo: str, config_path: Path | None = None):
        """Initialize with GitHub credentials."""
        super().__init__(token, repo, config_path)
        self._ensure_special_labels()
    
    def _ensure_special_labels(self) -> None:
        """Create special labels used by the canonicalization system if needed."""
        special_labels = [
            (LabelNames.GH_STORE, "6f42c1", "All issues managed by gh-store system"),
            (LabelNames.DEPRECATED, "999999", "Deprecated objects that have been merged into others"),
            # Add others as needed
        ]
        
        try:
            existing_labels = {label.name for label in self.repo.get_labels()}
            
            for name, color, description in special_labels:
                if name not in existing_labels:
                    try:
                        self.repo.create_label(name=name, color=color, description=description)
                    except Exception as e:
                        logger.warning(f"Could not create label {name}: {e}")
        except Exception as e:
            logger.warning(f"Could not ensure special labels exist: {e}")
            # Continue anyway - this allows tests to run without proper mocking
                
    def resolve_canonical_object_id(self, object_id: str, max_depth: int = 5) -> str:
        """
        Resolve an object ID to its canonical object ID with loop prevention.
        
        Args:
            object_id: Object ID to resolve
            max_depth: Maximum depth to prevent infinite loops with circular references
            
        Returns:
            The canonical object ID
        """
        if max_depth <= 0:
            logger.warning(f"Maximum alias resolution depth reached for {object_id}")
            return object_id
            
        # Check if this is an alias
        uid_label = f"{LabelNames.UID_PREFIX}{object_id}"
        alias_prefix = f"{LabelNames.ALIAS_TO_PREFIX}*"
        
        alias_issues = list(self.repo.get_issues(
            labels=[uid_label, alias_prefix],
            state="all"
        ))
        
        if alias_issues:
            for issue in alias_issues:
                for label in issue.labels:
                    # type protection for mocks
                    if hasattr(label, 'name') and isinstance(label.name, str) and label.name.startswith(LabelNames.ALIAS_TO_PREFIX):
                        # Extract canonical object ID from label
                        canonical_id = label.name[len(LabelNames.ALIAS_TO_PREFIX):]
                        
                        # Prevent self-referential loops
                        if canonical_id == object_id:
                            logger.error(f"Self-referential alias detected for {object_id}")
                            return object_id
                            
                        # Recurse to follow alias chain
                        return self.resolve_canonical_object_id(canonical_id, max_depth - 1)
        
        # Not an alias, or couldn't resolve - assume it's canonical
        return object_id
    
    def _extract_comment_metadata(self, comment, issue_number: int, object_id: str) -> dict:
        """Extract metadata from a comment."""
        try:
            data = json.loads(comment.body)
            
            # Try to extract timestamp from metadata
            timestamp = comment.created_at
            if isinstance(data, dict) and '_meta' in data and 'timestamp' in data['_meta']:
                try:
                    ts_str = data['_meta']['timestamp']
                    # Handle various ISO format variations
                    if ts_str.endswith('Z'):
                        ts_str = ts_str[:-1] + '+00:00'
                    timestamp = datetime.fromisoformat(ts_str)
                except (ValueError, AttributeError):
                    pass
            
            return {
                "data": data,
                "created_at": comment.created_at,
                "timestamp": timestamp,
                "id": comment.id,
                "source_issue": issue_number,
                "source_object_id": object_id,
                "body": comment.body,
                "reactions": {r.content: r.id for r in comment.get_reactions()},
            }
        except json.JSONDecodeError:
            # Skip non-JSON comments
            logger.warning(f"Skipping non-JSON comment {comment.id} in issue #{issue_number}")
            return None
    
    def collect_all_comments(self, object_id: str) -> List[Dict[str, Any]]:
        """Collect comments from canonical issue and all aliases."""
        canonical_id = self.resolve_canonical_object_id(object_id)
        
        # Get the canonical issue - look for stored-object label for active objects
        canonical_issues = list(self.repo.get_issues(
            labels=[f"{LabelNames.UID_PREFIX}{canonical_id}", LabelNames.STORED_OBJECT],
            state="all"
        ))
        
        if not canonical_issues:
            raise ObjectNotFound(f"No canonical object found with ID: {canonical_id}")
        
        canonical_issue = canonical_issues[0]
        comments = []
        visited_issues = set() # sort of hacky way to make sure we only collect comments for a given issue once
        
        # Get comments from canonical issue
        for comment in canonical_issue.get_comments():
            metadata = self._extract_comment_metadata(comment, canonical_issue.number, canonical_id)
            if metadata:
                comments.append(metadata)
        visited_issues.add(canonical_issue.id)
        
        # Get all aliases of this canonical object
        alias_issues = list(self.repo.get_issues(
            labels=[f"{LabelNames.ALIAS_TO_PREFIX}{canonical_id}"],
            state="all"
        ))
        
        # Get comments from each alias
        for alias_issue in alias_issues:
            if alias_issue.id in visited_issues:
                continue
            visited_issues.add(alias_issue.id)
            alias_id = None
            for label in alias_issue.labels:
                if label.name.startswith(LabelNames.UID_PREFIX):
                    alias_id = label.name[len(LabelNames.UID_PREFIX):]
                    break
            
            if not alias_id:
                continue  # Skip aliases without proper UID
                
            for comment in alias_issue.get_comments():
                metadata = self._extract_comment_metadata(comment, alias_issue.number, alias_id)
                if metadata:
                    comments.append(metadata)
        
        # Get deprecated issues (for virtual merging)
        deprecated_issues = list(self.repo.get_issues(
            labels=[LabelNames.GH_STORE, f"{LabelNames.UID_PREFIX}{canonical_id}", LabelNames.DEPRECATED],
            state="all"
        ))
        
        # Get comments from deprecated issues
        for dep_issue in deprecated_issues:
            if dep_issue.id in visited_issues:
                continue
            visited_issues.add(dep_issue.id)
            for comment in dep_issue.get_comments():
                metadata = self._extract_comment_metadata(comment, dep_issue.number, canonical_id)
                if metadata:
                    comments.append(metadata)
        
        # Sort by metadata timestamp
        return sorted(comments, key=lambda c: c["timestamp"])
            
    def process_with_virtual_merge(self, object_id: str) -> StoredObject:
        """Process an object with virtual merging of related issues."""
        canonical_id = self.resolve_canonical_object_id(object_id)
        
        # Collect all comments
        all_comments = self.collect_all_comments(canonical_id)
        
        # Find initial state
        initial_state = next(
            (c for c in all_comments if c["data"].get("type") == "initial_state"),
            None
        )
        
        # If no initial state found, try to find data from issue body
        if not initial_state:
            # Get canonical issue
            canonical_issues = list(self.repo.get_issues(
                labels=[f"{LabelNames.UID_PREFIX}{canonical_id}", LabelNames.STORED_OBJECT],
                state="all"
            ))
            
            if not canonical_issues:
                raise ObjectNotFound(f"No canonical object found with ID: {canonical_id}")
            
            canonical_issue = canonical_issues[0]
            
            try:
                body_data = json.loads(canonical_issue.body)
                # Create a synthetic initial state
                initial_state = {
                    "data": {
                        "type": "initial_state",
                        "_data": body_data,
                        "_meta": {
                            "client_version": CLIENT_VERSION,
                            "timestamp": canonical_issue.created_at.isoformat(),
                            "update_mode": "append",
                            "issue_number": canonical_issue.number,
                        }
                    },
                    "timestamp": canonical_issue.created_at,
                    "id": 0,  # Use 0 for synthetic initial state
                    "source_issue": canonical_issue.number,
                    "source_object_id": canonical_id
                }
            except (json.JSONDecodeError, AttributeError):
                raise ValueError(f"No initial state found for {canonical_id}")
        
        # Start with initial data
        current_state = initial_state["data"].get("_data", {})
        
        # Apply all updates in order
        for comment in all_comments:
            if comment["data"].get("type") == "initial_state":
                continue
            
            # Skip system comments
            # if comment["data"].get("type", "").startswith("system_"):
            #     continue
            # TODO: `type` should be a _meta attribute, not _data
            
            data = comment["data"]
            if isinstance(data, dict) and "_data" in data:
                update_data = data["_data"]
                update_mode = data.get("_meta", {}).get("update_mode", "append")
            else:
                # Legacy format
                update_data = data
                update_mode = "append"
            
            if update_mode == "append":
                current_state = self._deep_merge(current_state, update_data)
            elif update_mode == "replace":
                current_state = update_data
        
        # Get canonical issue for metadata
        canonical_issues = list(self.repo.get_issues(
            labels=[f"{LabelNames.UID_PREFIX}{canonical_id}", LabelNames.STORED_OBJECT],
            state="all"
        ))
        
        if not canonical_issues:
            raise ObjectNotFound(f"No canonical object found with ID: {canonical_id}")
        
        canonical_issue = canonical_issues[0]
        
        # Create object metadata
        meta = ObjectMeta(
            object_id=canonical_id,
            label=f"{LabelNames.UID_PREFIX}{canonical_id}",
            created_at=canonical_issue.created_at,
            issue_number=canonical_issue.number,
            updated_at=max(c["timestamp"] for c in all_comments) if all_comments else canonical_issue.updated_at,
            version=len(all_comments) if all_comments else 1
        )
        
        # Update canonical issue body with current state if not in test mode
        try:
            canonical_issue.edit(body=json.dumps(current_state, indent=2))
        except Exception as e:
            logger.warning(f"Could not update canonical issue body: {e}")
        
        return StoredObject(meta=meta, data=current_state)
    
    def _deep_merge(self, base: dict, update: dict) -> dict:
        """Deep merge two dictionaries."""
        result = base.copy()
        
        for key, value in update.items():
            if key in result and isinstance(result[key], dict) and isinstance(value, dict):
                result[key] = self._deep_merge(result[key], value)
            else:
                result[key] = value
                
        return result

    
    def get_object(self, object_id: str, canonicalize: bool = True) -> StoredObject:
        """
        Retrieve an object.
        - If canonicalize=True (default), follow the alias chain and merge updates from all related issues.
        - If canonicalize=False, return the object as stored for the given object_id without alias resolution.
        """
        canonical_id=None
        if canonicalize:
            canonical_id = self.resolve_canonical_object_id(object_id)
            if canonical_id != object_id:
                logger.info(f"Object {object_id} resolved to canonical object {canonical_id}")
            return self.process_with_virtual_merge(canonical_id)
        else:
            # Direct fetch: use only the issue with the UID label matching object_id.
            issues = list(self.repo.get_issues(
                labels=[f"{LabelNames.UID_PREFIX}{object_id}", LabelNames.STORED_OBJECT],
                state="all"
            ))
            if not issues:
                # Check if it's a deprecated object
                dep_issues = list(self.repo.get_issues(
                    labels=[f"{LabelNames.UID_PREFIX}{object_id}", LabelNames.DEPRECATED],
                    state="all"
                ))
                if dep_issues:
                    issue = dep_issues[0]
                else:
                    raise ObjectNotFound(f"No object found with ID: {object_id}")
            else:
                issue = issues[0]
            
            data = json.loads(issue.body)
            meta = ObjectMeta(
                object_id=object_id,
                label=f"{LabelNames.UID_PREFIX}{object_id}",
                issue_number=canonical_id,
                created_at=issue.created_at,
                updated_at=issue.updated_at,
                version=len(list(issue.get_comments())) + 1
            )
            return StoredObject(meta=meta, data=data)
    
    
    # In update_object, change the return so that we return the direct (alias‐preserving) object:
    def update_object(self, object_id: str, changes: Json) -> StoredObject:
        """Update an object by adding a comment to the appropriate issue."""
        # (Existing deprecation checks omitted for brevity.)
        # Check if this is an alias or direct match
        alias_issues = list(self.repo.get_issues(
            labels=[f"{LabelNames.UID_PREFIX}{object_id}", LabelNames.STORED_OBJECT],
            state="all"
        ))

        if not alias_issues:
            # Not a direct match, check for canonical object via aliases.
            canonical_id = self.resolve_canonical_object_id(object_id)
            canonical_issues = list(self.repo.get_issues(
                labels=[f"{LabelNames.UID_PREFIX}{canonical_id}", LabelNames.STORED_OBJECT],
                state="all"
            ))
            if not canonical_issues:
                raise ObjectNotFound(f"No object found with ID: {object_id}")
            issue = canonical_issues[0]
        else:
            issue = alias_issues[0]
        
        # Create update payload with metadata
        update_payload = CommentPayload(
            _data=changes,
            _meta=CommentMeta(
                client_version=CLIENT_VERSION,
                issue_number=issue.number,
                timestamp=datetime.now(timezone.utc).isoformat(),
                update_mode="append"
            )
        )
        
        # Add update comment and reopen issue
        issue.create_comment(json.dumps(update_payload.to_dict(), indent=2))
        issue.edit(state="open")
        
        # Return the updated object in direct mode so that the alias-specific state is preserved.
        return self.get_object(object_id, canonicalize=False)

    
    def create_alias(self, source_id: str, target_id: str) -> dict:
        """Create an alias from source_id to target_id."""
        # Verify source object exists
        source_issues = list(self.repo.get_issues(
            labels=[f"{LabelNames.UID_PREFIX}{source_id}", LabelNames.STORED_OBJECT],
            state="all"
        ))
        
        if not source_issues:
            raise ObjectNotFound(f"Source object not found: {source_id}")
            
        source_issue = source_issues[0]
        
        # Verify target object exists
        target_issues = list(self.repo.get_issues(
            labels=[f"{LabelNames.UID_PREFIX}{target_id}", LabelNames.STORED_OBJECT],
            state="all"
        ))
        
        if not target_issues:
            raise ObjectNotFound(f"Target object not found: {target_id}")
            
        target_issue = target_issues[0]
        
        # Check if this is already an alias
        for label in source_issue.labels:
            if label.name.startswith(LabelNames.ALIAS_TO_PREFIX):
                raise ValueError(f"Object {source_id} is already an alias")
        
        # Add alias label
        alias_label = f"{LabelNames.ALIAS_TO_PREFIX}{target_id}"
        
        try:
            # Create label if it doesn't exist
            try:
                self.repo.create_label(alias_label, "fbca04")
            except:
                pass  # Label already exists
                
            source_issue.add_to_labels(alias_label)
        except Exception as e:
            raise ValueError(f"Failed to create alias: {e}")
        
        # ... You know what? We don't actualy need these "system comments". 
        # Adding labels is already tracked within github issues anyway.
        # # Add system comments
        # source_comment = {
        #     "_data": {
        #         "alias_to": target_id,
        #         "timestamp": datetime.now(timezone.utc).isoformat()
        #     },
        #     "_meta": {
        #         "client_version": CLIENT_VERSION,
        #         "timestamp": datetime.now(timezone.utc).isoformat(),
        #         "update_mode": "append",
        #         "system": True
        #     },
        #     "type": "system_alias"
        # }
        # source_issue.create_comment(json.dumps(source_comment, indent=2))
        
        # Add reference comment to target
        # target_comment = {
        #     "_data": {
        #         "aliased_by": source_id,
        #         "timestamp": datetime.now(timezone.utc).isoformat()
        #     },
        #     "_meta": {
        #         "client_version": CLIENT_VERSION,
        #         "timestamp": datetime.now(timezone.utc).isoformat(),
        #         "update_mode": "append",
        #         "system": True
        #     },
        #     "type": "system_alias_reference"
        # }
        # target_issue.create_comment(json.dumps(target_comment, indent=2))
        
        return {
            "success": True,
            "source_id": source_id,
            "target_id": target_id
        }
    
    def deprecate_issue(self, issue_number: int, target_issue_number: int, reason: str) -> dict:
        """
        Deprecate a specific issue by making another issue canonical.
        
        Args:
            issue_number: The number of the issue to deprecate
            target_issue_number: The number of the canonical issue
            reason: Reason for deprecation ("duplicate", "merged", "replaced")
        """
        # Get source issue
        try:
            source_issue = self.repo.get_issue(issue_number)
        except Exception as e:
            raise ValueError(f"Source issue #{issue_number} not found: {e}")
        
        # Get target issue
        try:
            target_issue = self.repo.get_issue(target_issue_number)
        except Exception as e:
            raise ValueError(f"Target issue #{target_issue_number} not found: {e}")
            
        # Get object IDs from both issues
        source_object_id = self._get_object_id(source_issue)
        target_object_id = self._get_object_id(target_issue)
        
        # Make sure GH_STORE label is on both issues
        try:
            if not any(label.name == LabelNames.GH_STORE for label in source_issue.labels):
                source_issue.add_to_labels(LabelNames.GH_STORE)
            if not any(label.name == LabelNames.GH_STORE for label in target_issue.labels):
                target_issue.add_to_labels(LabelNames.GH_STORE)
        except Exception as e:
            logger.warning(f"Failed to ensure GH_STORE label: {e}")
        
        # Remove stored-object label from source
        if any(label.name == LabelNames.STORED_OBJECT for label in source_issue.labels):
            source_issue.remove_from_labels(LabelNames.STORED_OBJECT)
        
        # Add merge and deprecated labels
        try:
            # Create labels if they don't exist
            merge_label = f"{LabelNames.MERGED_INTO_PREFIX}{target_object_id}"
            deprecated_by_label = f"{LabelNames.DEPRECATED_BY_PREFIX}{target_issue_number}"
            
            try:
                self.repo.create_label(merge_label, "d73a49")
            except:
                pass  # Label already exists
                
            try:
                self.repo.create_label(deprecated_by_label, "d73a49")
            except:
                pass  # Label already exists
                
            try:
                self.repo.create_label(LabelNames.DEPRECATED, "999999")
            except:
                pass  # Label already exists
                
            # Add labels to source issue
            source_issue.add_to_labels(LabelNames.DEPRECATED, merge_label, deprecated_by_label)
        except Exception as e:
            # If we fail, try to restore stored-object label
            try:
                source_issue.add_to_labels(LabelNames.STORED_OBJECT)
            except:
                pass
            raise ValueError(f"Failed to deprecate issue: {e}")
        
        # # Add system comments
        # source_comment = {
        #     "_data": {
        #         "status": "deprecated",
        #         "canonical_object_id": target_object_id,
        #         "canonical_issue": target_issue_number,
        #         "reason": reason,
        #         "timestamp": datetime.now(timezone.utc).isoformat()
        #     },
        #     "_meta": {
        #         "client_version": CLIENT_VERSION,
        #         "timestamp": datetime.now(timezone.utc).isoformat(),
        #         "update_mode": "append",
        #         "system": True
        #     },
        #     "type": "system_deprecation"
        # }
        # source_issue.create_comment(json.dumps(source_comment, indent=2))
        
        # # Add reference comment to target
        # target_comment = {
        #     "_data": {
        #         "status": "merged_reference",
        #         "merged_object_id": source_object_id,
        #         "merged_issue": issue_number,
        #         "reason": reason,
        #         "timestamp": datetime.now(timezone.utc).isoformat()
        #     },
        #     "_meta": {
        #         "client_version": CLIENT_VERSION,
        #         "timestamp": datetime.now(timezone.utc).isoformat(),
        #         "update_mode": "append",
        #         "system": True
        #     },
        #     "type": "system_reference"
        # }
        # target_issue.create_comment(json.dumps(target_comment, indent=2))
        
        return {
            "success": True,
            "source_issue": issue_number,
            "source_object_id": source_object_id,
            "target_issue": target_issue_number, 
            "target_object_id": target_object_id,
            "reason": reason
        }
    
    def deprecate_object(self, object_id: str, target_id: str, reason: str) -> dict:
        """
        Deprecate an object by merging it into a target object.
        
        Args:
            object_id: The ID of the object to deprecate
            target_id: The ID of the canonical object to merge into
            reason: Reason for deprecation ("duplicate", "merged", "replaced")
        """
        # Verify objects exist
        source_issues = list(self.repo.get_issues(
            labels=[f"{LabelNames.UID_PREFIX}{object_id}", LabelNames.STORED_OBJECT],
            state="all"
        ))
        
        if not source_issues:
            raise ObjectNotFound(f"Source object not found: {object_id}")
            
        source_issue = source_issues[0]
        
        # Verify target object exists
        target_issues = list(self.repo.get_issues(
            labels=[f"{LabelNames.UID_PREFIX}{target_id}", LabelNames.STORED_OBJECT],
            state="all"
        ))
        
        if not target_issues:
            raise ObjectNotFound(f"Target object not found: {target_id}")
            
        target_issue = target_issues[0]
        
        # Validate that we're not trying to deprecate an object as itself
        if object_id == target_id and source_issue.number == target_issue.number:
            raise ValueError(f"Cannot deprecate an object as itself: {object_id}")
        
        # Use the issue-based deprecation function
        return self.deprecate_issue(
            issue_number=source_issue.number,
            target_issue_number=target_issue.number,
            reason=reason
        )
    
    def deduplicate_object(self, object_id: str, canonical_id: str = None) -> dict:
        """
        Handle duplicate issues for an object ID by choosing one as canonical
        and deprecating the others.
        
        Args:
            object_id: The object ID to deduplicate
            canonical_id: Optional specific canonical object ID to use
                         (must match object_id unless aliasing)
                         
        Returns:
            Dictionary with deduplication results
        """
        # Find all issues with this UID that are active (have stored-object label)
        issues = list(self.repo.get_issues(
            labels=[f"{LabelNames.UID_PREFIX}{object_id}", LabelNames.STORED_OBJECT],
            state="all"
        ))
        
        if len(issues) <= 1:
            return {"success": True, "message": "No duplicates found"}
        
        # Sort issues by creation date (oldest first)
        sorted_issues = sorted(issues, key=lambda i: i.created_at)
        
        # Select canonical issue
        if canonical_id and canonical_id != object_id:
            # If user specified a different canonical ID, find its issue
            canonical_issues = list(self.repo.get_issues(
                labels=[f"{LabelNames.UID_PREFIX}{canonical_id}", LabelNames.STORED_OBJECT],
                state="all"
            ))
            if not canonical_issues:
                raise ValueError(f"Specified canonical object {canonical_id} not found")
            canonical_issue = canonical_issues[0]
        else:
            # Default to oldest issue for this object ID
            canonical_issue = sorted_issues[0]
            canonical_id = object_id  # Keep same object ID unless aliasing
        
        canonical_issue_number = canonical_issue.number
        logger.info(f"Selected issue #{canonical_issue_number} as canonical for {object_id}")
        
        # Process duplicates - compare by issue number, not object ID
        results = []
        for issue in sorted_issues:
            # Skip the canonical issue
            if issue.number == canonical_issue_number:
                continue
            
            logger.info(f"Processing duplicate issue #{issue.number}")
            
            # Deprecate as duplicate - using issue numbers
            result = self.deprecate_issue(
                issue_number=issue.number,
                target_issue_number=canonical_issue_number,
                reason=DeprecationReason.DUPLICATE
            )
            results.append(result)
        
        return {
            "success": True,
            "canonical_object_id": self._get_object_id(canonical_issue),
            "canonical_issue": canonical_issue_number,
            "duplicates_processed": len(results),
            "results": results
        }
    
    def _get_object_id(self, issue) -> str:
        """Extract object ID from an issue's labels."""
        for label in issue.labels:
            if label.name.startswith(LabelNames.UID_PREFIX):
                return label.name[len(LabelNames.UID_PREFIX):]
        return None
        
    def find_duplicates(self) -> Dict[str, List[Issue]]:
        """Find all duplicate objects in the store."""
        # Get all issues with a UID label and stored-object label
        try:
            all_issues = list(self.repo.get_issues(
                labels=[LabelNames.STORED_OBJECT],
                state="all"
            ))
            
            # Group by UID
            issues_by_uid = defaultdict(list)
            
            for issue in all_issues:
                try:
                    for label in issue.labels:
                        # Check if this is a name attribute (real GitHub API object)
                        # or a string (test mock)
                        label_name = getattr(label, 'name', label)
                        if isinstance(label_name, str) and label_name.startswith(LabelNames.UID_PREFIX):
                            uid = label_name
                            issues_by_uid[uid].append(issue)
                            break
                except (AttributeError, TypeError):
                    # Skip issues that don't have proper label structure
                    continue
            
            # Filter to only those with duplicates
            duplicates = {uid: issues for uid, issues in issues_by_uid.items() if len(issues) > 1}
            
            return duplicates
        except Exception as e:
            logger.warning(f"Error finding duplicates: {e}")
            return {}  # Return empty dict on error
    
    def find_aliases(self, object_id: str = None) -> Dict[str, str]:
        """
        Find all aliases, or aliases for a specific object.
        
        Args:
            object_id: Optional object ID to find aliases for
            
        Returns:
            Dictionary mapping alias_id -> canonical_id
        """
        aliases = {}
        
        if object_id:
            # Find aliases for specific object
            alias_issues = list(self.repo.get_issues(
                labels=[f"{LabelNames.ALIAS_TO_PREFIX}{object_id}"],
                state="all"
            ))
            
            for issue in alias_issues:
                alias_id = self._get_object_id(issue)
                if alias_id:
                    aliases[alias_id] = object_id
        else:
            # Find all aliases
            alias_issues = list(self.repo.get_issues(
                labels=[f"{LabelNames.ALIAS_TO_PREFIX}*"],
                state="all"
            ))
            
            for issue in alias_issues:
                alias_id = self._get_object_id(issue)
                if not alias_id:
                    continue
                    
                # Find target of alias
                for label in issue.labels:
                    if label.name.startswith(LabelNames.ALIAS_TO_PREFIX):
                        canonical_id = label.name[len(LabelNames.ALIAS_TO_PREFIX):]
                        aliases[alias_id] = canonical_id
                        break
        
        return aliases


def main():
    """Command line interface for canonicalization tools."""
    parser = argparse.ArgumentParser(description="Object Canonicalization and Alias Management")
    
    # Required credentials
    parser.add_argument("--token", required=True, help="GitHub token")
    parser.add_argument("--repo", required=True, help="Repository in owner/repo format")
    
    # Action groups
    actions = parser.add_argument_group("Actions")
    actions.add_argument("--find-duplicates", action="store_true", help="Find duplicate objects")
    actions.add_argument("--deduplicate", action="store_true", help="Process all duplicates")
    actions.add_argument("--create-alias", action="store_true", help="Create an alias relationship")
    actions.add_argument("--deprecate", action="store_true", help="Deprecate and merge an object")
    
    # Object parameters
    objects = parser.add_argument_group("Object Parameters")
    objects.add_argument("--source-id", help="Source object ID for alias or deprecation")
    objects.add_argument("--target-id", help="Target object ID for alias or deprecation")
    objects.add_argument("--object-id", help="Object ID for operations on a single object")
    objects.add_argument("--reason", default=DeprecationReason.DUPLICATE, 
                        choices=[DeprecationReason.DUPLICATE, DeprecationReason.MERGED, DeprecationReason.REPLACED],
                        help="Reason for deprecation")
    
    # Other options
    parser.add_argument("--dry-run", action="store_true", help="Show actions without performing them")
    
    args = parser.parse_args()
    
    # Initialize store
    store = CanonicalStore(token=args.token, repo=args.repo)
    
    # Handle actions
    if args.find_duplicates:
        duplicates = store.find_duplicates()
        
        if not duplicates:
            logger.info("No duplicate objects found")
            return
            
        logger.info(f"Found {len(duplicates)} objects with duplicates:")
        
        for uid, issues in duplicates.items():
            object_id = uid[len(LabelNames.UID_PREFIX):]
            issue_numbers = [i.number for i in issues]
            logger.info(f"  Object {object_id}: {len(issues)} issues - {issue_numbers}")
    
    elif args.deduplicate:
        if args.object_id:
            # Deduplicate specific object
            result = store.deduplicate_object(args.object_id)
            logger.info(f"Deduplication result: {result}")
        else:
            # Find and deduplicate all
            duplicates = store.find_duplicates()
            
            if not duplicates:
                logger.info("No duplicate objects found")
                return
                
            results = []
            for uid, issues in duplicates.items():
                object_id = uid[len(LabelNames.UID_PREFIX):]
                logger.info(f"Deduplicating {object_id}...")
                
                if args.dry_run:
                    logger.info(f"  [DRY RUN] Would deduplicate {len(issues)} issues")
                    continue
                    
                result = store.deduplicate_object(object_id)
                results.append(result)
                logger.info(f"  Result: {result['duplicates_processed']} duplicates processed")
            
            logger.info(f"Processed {len(results)} objects with duplicates")
    
    elif args.create_alias:
        if not args.source_id or not args.target_id:
            logger.error("--source-id and --target-id are required for --create-alias")
            return
            
        logger.info(f"Creating alias from {args.source_id} to {args.target_id}")
        
        if args.dry_run:
            logger.info("[DRY RUN] Would create alias relationship")
            return
            
        result = store.create_alias(args.source_id, args.target_id)
        logger.info(f"Alias created: {result}")
    
    elif args.deprecate:
        if not args.source_id or not args.target_id:
            logger.error("--source-id and --target-id are required for --deprecate")
            return
            
        logger.info(f"Deprecating {args.source_id} into {args.target_id} (reason: {args.reason})")
        
        if args.dry_run:
            logger.info("[DRY RUN] Would deprecate object")
            return
            
        result = store.deprecate_object(args.source_id, args.target_id, args.reason)
        logger.info(f"Deprecation complete: {result}")


if __name__ == "__main__":
    main()



---
File: pyproject.toml
---
[project]
name = "gh-store"
version = "0.10.4"
description = "A lightweight data store using GitHub Issues as a backend"
authors = [
    {name = "David Marx", email = "david.marx84@gmail.com"},
]
dependencies = [
    "PyGithub>=2.1.1",
    "fire>=0.5.0",
    "loguru>=0.7.2",
    "omegaconf>=2.3.0",
    "pyyaml>=6.0.1",
]
requires-python = ">=3.11.0, <3.14"
readme = "README.md"
license = {text = "MIT"}

[project.scripts]
gh-store = "gh_store.__main__:main"

# [tool.hatch.build]
# include = [
#     "gh_store/default_config.yml",
# ]

[project.optional-dependencies]
dev = [
    "pytest>=7.4.0",
    "pytest-asyncio>=0.23.0",
    "pytest-cov>=4.1.0",
    "mypy>=1.8.0",
    "ruff>=0.1.9",
    "black>=23.12.0",
    "isort>=5.13.0",
    "fastapi>=0.109.0",  # For mock GitHub API
    "uvicorn>=0.27.0",   # For mock GitHub API
    "httpx>=0.26.0",     # For async HTTP client
]

docs = [
    "mkdocs>=1.5.0",
    "mkdocs-material>=9.5.0",
    "mkdocstrings[python]>=0.24.0",
]

[tool.pytest.ini_options]
testpaths = ["tests"]
addopts = "--cov=gh_store --cov-report=term-missing -vvv"
asyncio_mode = "auto"
markers = [
    "integration: marks tests as integration tests",
]

[tool.mypy]
python_version = "3.12"
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = true
disallow_incomplete_defs = true
check_untyped_defs = true
disallow_untyped_decorators = true
no_implicit_optional = true
warn_redundant_casts = true
warn_unused_ignores = true
warn_no_return = true
warn_unreachable = true

[tool.ruff]
select = ["E", "F", "B", "I"]
ignore = ["E501"]
line-length = 88
target-version = "py312"

[tool.ruff.per-file-ignores]
"__init__.py" = ["F401"]
"tests/*" = ["F841"]

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

##########

# llamero stuff

[tool.summary]
max_file_size_kb = 500

exclude_patterns = [
    '.git',
    '.gitignore',
    '.pytest_cache',
    '__pycache__',
    'SUMMARY',
    '.coverage',
    '.env',
    '.venv',
    '.idea',
    '.vscode',
    '.bundle.js',
    '.bundle.ts'
]

include_extensions = [
    '.py',
    '.md',
    '.txt',
    '.yml',
    '.yaml',
    '.toml',
    '.json',
    '.html',
    '.css',
    '.js',
    '.ts',
    '.tsx',
    '.j2'
]

exclude_directories = [
    '.git',
    '__pycache__',
    '.pytest_cache',
    '.venv',
    '.idea',
    '.vscode',
    'data'
]



---
File: requirements.txt
---
PyGithub>=2.1.1
fire>=0.5.0
loguru>=0.7.2
omegaconf>=2.3.0
pyyaml>=6.0.1



---
File: tests/unit/conftest.py
---
# tests/unit/conftest.py
"""Pytest configuration and shared fixtures for gh-store unit tests."""

# Re-export all fixtures to make them available to tests
from tests.unit.fixtures.config import *
from tests.unit.fixtures.github import *
from tests.unit.fixtures.cli import *
from tests.unit.fixtures.store import *
from tests.unit.fixtures.canonical import * 



---
File: tests/unit/fixtures/__init__.py
---




---
File: tests/unit/fixtures/canonical.py
---
# tests/unit/fixtures/canonical.py
"""Fixtures for canonicalization tests"""

import json
from datetime import datetime, timezone
import pytest
from unittest.mock import Mock, patch, MagicMock

from gh_store.tools.canonicalize import CanonicalStore, LabelNames

@pytest.fixture
def mock_canonical_store():
    """Create a mock for CanonicalStore class."""
    with patch('gh_store.cli.commands.CanonicalStore') as mock_canonical:
        canonical_instance = Mock()
        mock_canonical.return_value = canonical_instance
        
        # Mock commonly used methods
        canonical_instance.find_aliases.return_value = {"alias-obj": "canonical-obj"}
        
        yield mock_canonical

@pytest.fixture
def mock_labels_response():
    """Mock the response for get_labels to return iterable labels."""
    labels = [
        Mock(name="stored-object"),
        Mock(name="deprecated-object"),
        Mock(name="UID:test-123")
    ]
    return labels

@pytest.fixture
def canonical_store_with_mocks(mock_repo_factory, default_config, mock_labels_response):
    """Create a CanonicalStore instance with mocked repo and methods."""
    # Create mock repo
    repo = mock_repo_factory(
        name="owner/repo",
        owner_login="repo-owner",
        owner_type="User",
        labels=["stored-object", "deprecated-object"]
    )
    
    # Setup get_labels to return iterable
    repo.get_labels.return_value = mock_labels_response
    
    # Create CanonicalStore with mocked repo
    with patch('gh_store.core.store.Github') as mock_gh:
        mock_gh.return_value.get_repo.return_value = repo
        
        store = CanonicalStore(token="fake-token", repo="owner/repo")
        store.repo = repo
        store.access_control.repo = repo
        store.config = default_config
        
        # Mock common methods
        store._extract_comment_metadata = Mock(side_effect=lambda comment, issue_number, object_id: {
            "data": json.loads(comment.body) if hasattr(comment, 'body') else {},
            "timestamp": getattr(comment, 'created_at', datetime.now(timezone.utc)),
            "id": getattr(comment, 'id', 1),
            "source_issue": issue_number,
            "source_object_id": object_id
        })
        
        # Setup for find_duplicates
        store.repo.get_issues = Mock(return_value=[])
        
        # Mock methods to avoid real API calls
        store._ensure_special_labels = Mock()
        
        return store

@pytest.fixture
def mock_issue_with_initial_state(mock_issue_factory, mock_comment_factory):
    """Create a mock issue with initial state for canonicalization tests."""
    # Create initial state comment
    initial_comment = mock_comment_factory(
        body={
            "type": "initial_state",
            "_data": {"name": "test", "value": 42},
            "_meta": {
                "client_version": "0.7.0",
                "timestamp": "2025-01-01T00:00:00Z",
                "update_mode": "append",
                "issue_number": 123  # Add issue number
            }
        },
        comment_id=1,
        created_at=datetime(2025, 1, 1, tzinfo=timezone.utc)
    )
    
    # Create issue with initial state comment
    return mock_issue_factory(
        number=123,
        body=json.dumps({"name": "test", "value": 42}),
        labels=["stored-object", "UID:metrics"],
        comments=[initial_comment]
    )



---
File: tests/unit/fixtures/cli.py
---
# tests/unit/fixtures/cli.py
"""CLI-specific fixtures for gh-store unit tests."""

import os
import sys
import logging
from pathlib import Path
from datetime import datetime, timedelta, timezone
import json
import pytest
from unittest.mock import Mock, patch
from loguru import logger

from gh_store.__main__ import CLI

@pytest.fixture(autouse=True)
def cli_env_vars(monkeypatch):
    """Setup environment variables for CLI testing."""
    monkeypatch.setenv('GITHUB_TOKEN', 'test-token')
    monkeypatch.setenv('GITHUB_REPOSITORY', 'owner/repo')
    yield

@pytest.fixture
def mock_config(tmp_path):
    """Create a mock config file for testing."""
    config_dir = tmp_path / ".config" / "gh-store"
    config_dir.mkdir(parents=True)
    config_path = config_dir / "config.yml"
    
    # Create default config
    default_config = """
store:
  base_label: "stored-object"
  uid_prefix: "UID:"
  reactions:
    processed: "+1"
    initial_state: "rocket"
  retries:
    max_attempts: 3
    backoff_factor: 2
  rate_limit:
    max_requests_per_hour: 1000
  log:
    level: "INFO"
    format: "{time} | {level} | {message}"
"""
    config_path.write_text(default_config)
    return config_path

@pytest.fixture
def mock_gh_repo():
    """Create a mocked GitHub repo for testing."""
    mock_repo = Mock()
    with patch('gh_store.core.store.Github') as MockGithub:
        # Setup mock repo
        mock_repo = Mock()
        mock_repo.get_issue.return_value = Mock(state="closed")
        mock_repo.get_issues.return_value = []
        mock_repo.owner = Mock(login="owner", type="User")
        
        # Set up mock Github client
        MockGithub.return_value.get_repo.return_value = mock_repo
        yield mock_repo


class InterceptHandler(logging.Handler):
    def emit(self, record):
        # Try to find caller's module path
        try:
            frame = logging.currentframe()
            depth = 6  # Adjust if needed to find the correct caller
            while frame and depth > 0:
                frame = frame.f_back
                depth -= 1
            module_path = frame.f_code.co_filename if frame else ""
            function_name = frame.f_code.co_name if frame else ""
        except (AttributeError, ValueError):
            module_path = ""
            function_name = ""

        # Safely format the message
        try:
            msg = self.format(record)
        except Exception:
            msg = record.getMessage()

        # Write directly to caplog's handler instead of going through loguru
        logging.getLogger(record.name).handle(
            logging.LogRecord(
                name=record.name,
                level=record.levelno,
                pathname=module_path,
                lineno=record.lineno,
                msg=msg,
                args=(),
                exc_info=record.exc_info,
                func=function_name
            )
        )

# tests/unit/fixtures/cli.py - Update logging setup

@pytest.fixture(autouse=True)
def setup_loguru(caplog):
    """Configure loguru for testing with pytest caplog."""
    # Remove any existing handlers
    logger.remove()
    
    # Set up caplog
    caplog.set_level(logging.INFO)
    
    # Add a test handler that writes directly to caplog
    def log_to_caplog(message):
        logging.getLogger().info(message)
    
    handler_id = logger.add(log_to_caplog, format="{message}")
    
    yield
    
    # Cleanup
    logger.remove(handler_id)

@pytest.fixture
def mock_cli(mock_config, mock_gh_repo):
    """Create a CLI instance with mocked dependencies."""
    with patch('gh_store.cli.commands.ensure_config_exists') as mock_ensure:  # Updated path
        cli = CLI()
        # Mock HOME to point to our test config
        with patch.dict(os.environ, {'HOME': str(mock_config.parent.parent.parent)}):
            yield cli

@pytest.fixture
def mock_store_response():
    """Mock common GitHubStore responses."""
    mock_obj = Mock()
    mock_obj.meta = Mock(
        object_id="test-123",
        issue_number=42,  # Added issue_number field
        created_at=datetime(2025, 1, 1, tzinfo=timezone.utc),
        updated_at=datetime(2025, 1, 2, tzinfo=timezone.utc),
        version=1
    )
    mock_obj.data = {"name": "test", "value": 42}
    return mock_obj

@pytest.fixture
def mock_stored_objects():
    """Create mock stored objects for testing."""
    objects = []
    for i in range(1, 3):
        mock_obj = Mock()
        mock_obj.meta = Mock(
            object_id=f"test-obj-{i}",
            issue_number=100 + i,  # Added issue_number field
            created_at=datetime(2025, 1, i, tzinfo=timezone.utc),
            updated_at=datetime(2025, 1, i+1, tzinfo=timezone.utc),
            version=1
        )
        mock_obj.data = {
            "name": f"test{i}",
            "value": i * 42
        }
        objects.append(mock_obj)
    return objects

@pytest.fixture
def mock_snapshot_file_factory(tmp_path, mock_stored_objects):
    """Factory for creating snapshot files with configurable timestamps."""
    def _create_snapshot(snapshot_time=None, include_objects=None):
        """
        Create a mock snapshot file with configurable timestamp and objects.
        
        Args:
            snapshot_time: Custom snapshot timestamp (defaults to 1 day ago)
            include_objects: List of indices from mock_stored_objects to include
                            (defaults to all objects)
        
        Returns:
            Path to the created snapshot file
        """
        # Default timestamp is 1 day ago
        if snapshot_time is None:
            snapshot_time = datetime.now(timezone.utc) - timedelta(days=1)
        
        snapshot_path = tmp_path / f"snapshot_{int(datetime.now().timestamp())}.json"
        
        # Convert objects to serializable format
        snapshot_data = {
            "snapshot_time": snapshot_time.isoformat(),
            "repository": "owner/repo",
            "objects": {}
        }
        
        # Determine which objects to include
        objects_to_include = mock_stored_objects
        if include_objects is not None:
            # sort of a weird way to go about this...
            objects_to_include = [mock_stored_objects[i] for i in include_objects if i < len(mock_stored_objects)]
        
        # Add objects to snapshot
        for obj in objects_to_include:
            snapshot_data["objects"][obj.meta.object_id] = {
                "data": obj.data,
                "meta": {
                    "object_id": obj.meta.object_id,
                    "issue_number": obj.meta.issue_number,
                    "created_at": obj.meta.created_at.isoformat(),
                    "updated_at": obj.meta.updated_at.isoformat(),
                    "version": obj.meta.version
                }
            }
        
        snapshot_path.write_text(json.dumps(snapshot_data, indent=2))
        return snapshot_path
    
    return _create_snapshot

@pytest.fixture
def mock_snapshot_file(mock_snapshot_file_factory):
    """Create a default mock snapshot file for testing."""
    return mock_snapshot_file_factory()



---
File: tests/unit/fixtures/config.py
---
# tests/unit/fixtures/config.py
"""Configuration fixtures for gh-store unit tests."""

from datetime import datetime, timezone
from pathlib import Path
import pytest
from unittest.mock import patch, mock_open
from omegaconf import OmegaConf

@pytest.fixture
def default_config():
    """Create a consistent default config for testing."""
    return OmegaConf.create({
        "store": {
            "base_label": "stored-object",
            "uid_prefix": "UID:",
            "reactions": {
                "processed": "+1",
                "initial_state": "rocket"
            },
            "retries": {
                "max_attempts": 3,
                "backoff_factor": 2
            },
            "rate_limit": {
                "max_requests_per_hour": 1000
            },
            "log": {
                "level": "INFO",
                "format": "{time} | {level} | {message}"
            }
        }
    })

@pytest.fixture(autouse=True)
def mock_config_file(default_config):
    """Mock OmegaConf config loading."""
    with patch('omegaconf.OmegaConf.load', return_value=default_config) as mock_load:
        yield mock_load

@pytest.fixture
def test_config_dir(tmp_path: Path) -> Path:
    """Provide a temporary directory for config files during testing."""
    config_dir = tmp_path / ".config" / "gh-store"
    config_dir.mkdir(parents=True)
    return config_dir

@pytest.fixture
def test_config_file(test_config_dir: Path, default_config: OmegaConf) -> Path:
    """Create a test config file with minimal valid content."""
    config_path = test_config_dir / "config.yml"
    config_path.write_text(OmegaConf.to_yaml(default_config))
    return config_path



---
File: tests/unit/fixtures/github.py
---
# tests/unit/fixtures/github.py
"""GitHub API mocks for gh-store unit tests."""

from datetime import datetime, timezone
import json
from typing import Any, Callable, Literal, TypedDict
import pytest
from unittest.mock import Mock, patch
from github import GithubException

from gh_store.core.constants import LabelNames

@pytest.fixture
def mock_label_factory():
    """
    Create GitHub-style label objects.
    
    Example:
        label = mock_label_factory("enhancement")
        label = mock_label_factory("bug", "fc2929")
        label = mock_label_factory("bug", "fc2929", "Bug description")
    """
    def create_label(name: str, color: str = "0366d6", description: str = None) -> Mock:
        """
        Create a mock label with GitHub-like structure.
        
        Args:
            name: Name of the label
            color: Color hex code without #
            description: Optional description for the label
        """
        label = Mock()
        label.name = name
        label.color = color
        label.description = description
        return label
    
    return create_label

class CommentMetadata(TypedDict, total=False):
    """Metadata for comment creation."""
    client_version: str
    timestamp: str
    update_mode: Literal['append', 'replace']

class CommentBody(TypedDict, total=False):
    """Structure for comment body data."""
    _data: dict[str, Any]
    _meta: CommentMetadata
    type: Literal['initial_state'] | None

@pytest.fixture
def mock_comment_factory():
    """
    Create GitHub comment mocks with standard structure.

    This factory creates mock comment objects that mirror GitHub's API structure,
    with proper typing and validation for reactions and metadata.

    Args in create_comment:
        body: Comment body (dict will be JSON serialized)
        user_login: GitHub username of comment author
        comment_id: Unique comment ID (auto-generated if None)
        reactions: List of reaction types or mock reactions
        created_at: Comment creation timestamp
        **kwargs: Additional attributes to set on the comment

    Examples:
        # Basic comment with data
        comment = mock_comment_factory(
            body={"value": 42},
            user_login="owner"
        )

        # Comment with metadata
        comment = mock_comment_factory(
            body={
                "_data": {"value": 42},
                "_meta": {
                    "client_version": "0.5.1",
                    "timestamp": "2025-01-01T00:00:00Z",
                    "update_mode": "append"
                }
            }
        )

        # Initial state comment
        comment = mock_comment_factory(
            body={
                "type": "initial_state",
                "_data": {"initial": "state"},
                "_meta": {
                    "client_version": "0.5.1",
                    "timestamp": "2025-01-01T00:00:00Z",
                    "update_mode": "append"
                }
            }
        )

        # Comment with reactions
        comment = mock_comment_factory(
            body={"value": 42},
            reactions=["+1", "rocket"]
        )
    """
    def create_comment(
        body: dict[str, Any] | CommentBody,
        user_login: str = "repo-owner",
        comment_id: int | None = None,
        reactions: list[str | Mock] | None = None,
        created_at: datetime | None = None,
        **kwargs
    ) -> Mock:
        """Create a mock comment with GitHub-like structure."""
        # Validate body structure if it's meant to be a CommentBody
        if isinstance(body, dict) and "_meta" in body:
            if "update_mode" in body["_meta"] and body["_meta"]["update_mode"] not in ["append", "replace"]:
                raise ValueError("update_mode must be 'append' or 'replace'")
            if "type" in body and body["type"] not in [None, "initial_state"]:
                raise ValueError("type must be None or 'initial_state'")

        comment = Mock()
        
        # Set basic attributes
        comment.id = comment_id or 1
        comment.body = json.dumps(body)
        comment.created_at = created_at or datetime(2025, 1, 1, tzinfo=timezone.utc)
        
        # Set up user
        user = Mock()
        user.login = user_login
        comment.user = user
        
        # Set up reactions with validation
        mock_reactions = []
        if reactions:
            for reaction in reactions:
                if isinstance(reaction, Mock):
                    if not hasattr(reaction, 'content'):
                        raise ValueError("Mock reaction must have 'content' attribute")
                    mock_reactions.append(reaction)
                else:
                    mock_reaction = Mock()
                    mock_reaction.content = str(reaction)
                    mock_reactions.append(mock_reaction)
        
        comment.get_reactions = Mock(return_value=mock_reactions)
        comment.create_reaction = Mock()
        
        # Add any additional attributes
        for key, value in kwargs.items():
            setattr(comment, key, value)
        
        return comment
    
    return create_comment


mock_comment = mock_comment_factory


@pytest.fixture
def mock_issue_factory(mock_comment_factory, mock_label_factory):
    """
    Create GitHub issue mocks with standard structure.

    Examples:
        # Basic issue
        issue = mock_issue_factory(
            body={"test": "data"}
        )

        # Issue with explicit number
        issue = mock_issue_factory(
            number=123,
            labels=["stored-object", "UID:test-123"]
        )

        # Issue with comments
        issue = mock_issue_factory(
            comments=[
                mock_comment_factory(
                    body={"value": 42},
                    comment_id=1
                )
            ]
        )
    """
    def create_issue(
        number: int | None = None,
        body: dict[str, Any] | str | None = None,
        labels: list[str] | None = None,
        comments: list[Mock] | None = None,
        state: str = "closed",
        user_login: str = "repo-owner",
        created_at: datetime | None = None,
        updated_at: datetime | None = None,
        **kwargs
    ) -> Mock:
        """
        Create a mock issue with GitHub-like structure.

        Args:
            number: Issue number (defaults to 1 if not provided)
            body: Issue body content (dict will be JSON serialized)
            labels: List of label names to add
            comments: List of mock comments
            state: Issue state (open/closed)
            user_login: GitHub username of issue creator
            created_at: Issue creation timestamp
            updated_at: Issue last update timestamp
            **kwargs: Additional attributes to set
        """
        issue = Mock()
        
        # Set basic attributes
        issue.number = number or 1  # Default to 1 if not provided
        issue.body = json.dumps(body) if isinstance(body, dict) else (body or "{}")
        issue.state = state
        issue.created_at = created_at or datetime(2025, 1, 1, tzinfo=timezone.utc)
        issue.updated_at = updated_at or datetime(2025, 1, 2, tzinfo=timezone.utc)
        
        # Set up user
        user = Mock()
        user.login = user_login
        issue.user = user
        
        # Set up labels
        issue_labels = []
        issue.labels = issue_labels
        if labels:
            for label_name in labels:
                issue.labels.append(mock_label_factory(label_name))
                #issue.labels.append(label_name)
        
        
        # Set up comments
        mock_comments = list(comments) if comments is not None else []
        issue.get_comments = Mock(return_value=mock_comments)
        issue.create_comment = Mock()

        # Set up proper owner permissions
        repo = Mock()
        owner = Mock()
        owner.login = user_login
        owner.type = "User"
        repo.owner = owner
        issue.repository = repo  # Needed for access control checks
        
        # Set up issue editing
        issue.edit = Mock()
        
        # Add any additional attributes
        for key, value in kwargs.items():
            setattr(issue, key, value)
        
        return issue
    
    return create_issue

# Keep backward compatibility
mock_issue = mock_issue_factory


@pytest.fixture
def mock_repo_factory(mock_label_factory):
    """
    Create GitHub repository mocks with standard structure.
    
    Note: Creates basic repository structure. Labels, issues, and permissions
    should be explicitly set up in tests where they matter.
    """
    def create_repo(
        name: str = "owner/repo",
        owner_login: str = "repo-owner",
        owner_type: str = "User",
        labels: list[str] | None = None,
        issues: list[Mock] | None = None,
        **kwargs
    ) -> Mock:
        """
        Create a mock repository with GitHub-like structure.
        Args:
            name: Repository name in owner/repo format
            owner_login: Repository owner's login
            owner_type: Owner type ("User" or "Organization")
            labels: Initial repository labels
            issues: Initial repository issues
            **kwargs: Additional attributes to set
        """
        repo = Mock()
        
        # Set basic attributes
        repo.full_name = name
        
        # Set up owner - making it more explicit
        owner = Mock(spec=['login', 'type'])  # Specify expected attributes
        owner.login = owner_login
        owner.type = owner_type
        repo.owner = owner
        
        # Set up labels - include gh-store by default unless specified otherwise
        repo_labels = []
        if labels:
            default_labels = [LabelNames.GH_STORE.value, LabelNames.STORED_OBJECT.value] \
                                if LabelNames.GH_STORE.value not in labels and LabelNames.STORED_OBJECT.value not in labels else []
            for name in default_labels + labels:
                repo_labels.append(mock_label_factory(name))
        repo.get_labels = Mock(return_value=repo_labels)
        
        # Set up label creation
        def create_label(name: str, color: str = "0366d6", description: str = None) -> Mock:
            label = mock_label_factory(name, color, description)
            repo_labels.append(label)
            return label
        repo.create_label = Mock(side_effect=create_label)
        
        # Set up issues
        repo_issues = issues or []
        def get_issue(number):
            matching = [i for i in repo_issues if i.number == number]
            if matching:
                return matching[0]
            mock_issue = Mock()
            mock_issue.state = "closed"
            return mock_issue
        repo.get_issue = Mock(side_effect=get_issue)
        repo.get_issues = Mock(return_value=repo_issues)
        
        # Set up CODEOWNERS handling
        def get_contents(path: str) -> Mock:
            if path in ['.github/CODEOWNERS', 'docs/CODEOWNERS', 'CODEOWNERS']:
                content = Mock()
                content.decoded_content = f"* @{owner_login}".encode()
                return content
            raise GithubException(404, "Not found")
        repo.get_contents = Mock(side_effect=get_contents)
        
        # Add any additional attributes
        for key, value in kwargs.items():
            setattr(repo, key, value)
        
        return repo
    
    return create_repo

@pytest.fixture
def mock_github():
    """Create a mock Github instance with proper repository structure."""
    with patch('gh_store.core.store.Github') as mock_gh:
        # Setup mock repo
        mock_repo = Mock()
        
        # Setup owner
        owner = Mock()
        owner.login = "repo-owner"
        owner.type = "User"
        mock_repo.owner = owner
        
        # Setup labels
        mock_labels = [Mock(name=LabelNames.STORED_OBJECT.value), Mock(name=LabelNames.GH_STORE.value)]
        mock_repo.get_labels = Mock(return_value=mock_labels)
        
        def create_label(name: str, color: str = "0366d6") -> Mock:
            new_label = Mock(name=name)
            mock_labels.append(new_label)
            return new_label
        mock_repo.create_label = Mock(side_effect=create_label)
        
        # Mock CODEOWNERS access
        mock_content = Mock()
        mock_content.decoded_content = b"* @repo-owner"
        def get_contents_side_effect(path: str) -> Mock:
            if path in ['.github/CODEOWNERS', 'docs/CODEOWNERS', 'CODEOWNERS']:
                return mock_content
            raise GithubException(404, "Not found")
        mock_repo.get_contents = Mock(side_effect=get_contents_side_effect)
        
        mock_gh.return_value.get_repo.return_value = mock_repo
        yield mock_gh, mock_repo



---
File: tests/unit/fixtures/store.py
---
# tests/unit/fixtures/store.py
"""Store-related fixtures for gh-store unit tests."""

from datetime import datetime, timezone
from typing import Sequence
from unittest.mock import Mock, patch

import pytest

from gh_store.core.constants import LabelNames
from gh_store.core.exceptions import ObjectNotFound
from gh_store.core.store import GitHubStore
from gh_store.core.version import CLIENT_VERSION


def setup_mock_auth(store, authorized_users: Sequence[str] | None = None):
    """Set up mocked authorization for testing.
    
    Args:
        store: GitHubStore instance to configure
        authorized_users: List of usernames to authorize (defaults to ['repo-owner'])
    """
    if authorized_users is None:
        authorized_users = ['repo-owner']
    
    # Pre-populate owner info cache
    store.access_control._owner_info = {
        'login': 'repo-owner',
        'type': 'User'
    }
    
    # If we have additional authorized users via CODEOWNERS
    if len(authorized_users) > 1:
        # Mock CODEOWNERS content
        codeowners_content = "* " + " ".join(f"@{user}" for user in authorized_users)
        mock_content = Mock()
        mock_content.decoded_content = codeowners_content.encode()
        store.repo.get_contents = Mock(return_value=mock_content)
        
        # Clear codeowners cache to force reload
        store.access_control._codeowners = None


@pytest.fixture
def store(mock_repo_factory, default_config):
    """Create GitHubStore instance with mocked dependencies."""
    repo = mock_repo_factory(
        name="owner/repo",
        owner_login="repo-owner",
        owner_type="User",
        labels=[LabelNames.GH_STORE.value, LabelNames.STORED_OBJECT.value]
    )
    
    with patch('gh_store.core.store.Github') as mock_gh:
        mock_gh.return_value.get_repo.return_value = repo
        
        store = GitHubStore(token="fake-token", repo="owner/repo")
        store.repo = repo
        store.access_control.repo = repo
        store.config = default_config
        
        # Set up default authorization
        setup_mock_auth(store)
        
        return store

@pytest.fixture
def authorized_store(store):
    """Create store with additional authorized users for testing."""
    def _authorized_store(authorized_users: Sequence[str]):
        setup_mock_auth(store, authorized_users=authorized_users)
        return store
    return _authorized_store


@pytest.fixture
def history_mock_comments(mock_comment):
    """Create series of comments representing object history."""
    comments = []
    
    # Initial state
    comments.append(mock_comment(
        user_login="repo-owner",
        body={
            "type": "initial_state",
            "_data": {"name": "test", "value": 42},
            "_meta": {
                "client_version": CLIENT_VERSION,
                "timestamp": "2025-01-01T00:00:00Z",
                "update_mode": "append",
                "issue_number": 123,
            }
        },
        comment_id=1,
        created_at=datetime(2025, 1, 1, tzinfo=timezone.utc)
    ))
    
    # First update
    comments.append(mock_comment(
        user_login="repo-owner",
        body={
            "_data": {"value": 43},
            "_meta": {
                "client_version": CLIENT_VERSION,
                "timestamp": "2025-01-02T00:00:00Z",
                "update_mode": "append",
                "issue_number": 123,
            }
        },
        comment_id=2,
        created_at=datetime(2025, 1, 2, tzinfo=timezone.utc)
    ))
    
    # Second update
    comments.append(mock_comment(
        user_login="repo-owner",
        body={
            "_data": {"value": 44},
            "_meta": {
                "client_version": CLIENT_VERSION,
                "timestamp": "2025-01-03T00:00:00Z",
                "update_mode": "append",
                "issue_number": 123,
            }
        },
        comment_id=3,
        created_at=datetime(2025, 1, 3, tzinfo=timezone.utc)
    ))
    
    return comments



---
File: tests/unit/test_canonicalization.py
---
# tests/unit/test_canonicalization.py
"""Tests for the canonicalization and aliasing functionality."""

import json
from datetime import datetime, timezone
import pytest
from unittest.mock import Mock, patch

from gh_store.core.constants import LabelNames
from gh_store.tools.canonicalize import CanonicalStore, DeprecationReason


@pytest.fixture
def canonical_store(store, mock_repo_factory, default_config):
    """Create a CanonicalStore with mocked dependencies."""
    repo = mock_repo_factory(
        name="owner/repo",
        owner_login="repo-owner",
        owner_type="User",
        labels=["stored-object"]
    )
    
    with patch('gh_store.core.store.Github') as mock_gh:
        mock_gh.return_value.get_repo.return_value = repo
        
        store = CanonicalStore(token="fake-token", repo="owner/repo")
        store.repo = repo
        store.access_control.repo = repo
        store.config = default_config
        
        # Mock the _ensure_special_labels method to avoid API calls
        store._ensure_special_labels = Mock()
        
        return store

@pytest.fixture
def mock_alias_issue(mock_issue_factory):
    """Create a mock issue that is an alias to another object."""
    return mock_issue_factory(
        number=789,
        labels=[
            LabelNames.STORED_OBJECT,
            f"{LabelNames.UID_PREFIX}daily-metrics",
            f"{LabelNames.ALIAS_TO_PREFIX}metrics"
        ],
        body=json.dumps({"period": "daily"}),
        created_at=datetime(2025, 1, 10, tzinfo=timezone.utc),
        updated_at=datetime(2025, 1, 12, tzinfo=timezone.utc)
    )

@pytest.fixture
def mock_canonical_issue(mock_issue_factory):
    """Create a mock issue that is the canonical version of an object."""
    return mock_issue_factory(
        number=123,
        labels=[
            LabelNames.STORED_OBJECT,
            f"{LabelNames.UID_PREFIX}metrics"
        ],
        body=json.dumps({"count": 42}),
        created_at=datetime(2025, 1, 1, tzinfo=timezone.utc),
        updated_at=datetime(2025, 1, 15, tzinfo=timezone.utc)
    )

@pytest.fixture
def mock_duplicate_issue(mock_issue_factory, mock_label_factory):
    """Create a mock issue that is a duplicate to be deprecated."""
    return mock_issue_factory(
        number=456,
        labels=[
            mock_label_factory(LabelNames.STORED_OBJECT),
            mock_label_factory(f"{LabelNames.UID_PREFIX}metrics")
        ],
        body=json.dumps({"count": 15}),
        created_at=datetime(2025, 1, 5, tzinfo=timezone.utc),
        updated_at=datetime(2025, 1, 5, tzinfo=timezone.utc)
    )

@pytest.fixture
def mock_deprecated_issue(mock_issue_factory, mock_label_factory):
    """Create a mock issue that has already been deprecated."""
    return mock_issue_factory(
        number=457,
        labels=[
            mock_label_factory(LabelNames.DEPRECATED),
            mock_label_factory(f"{LabelNames.MERGED_INTO_PREFIX}metrics")
        ],
        body=json.dumps({"old": "data"}),
        created_at=datetime(2025, 1, 6, tzinfo=timezone.utc),
        updated_at=datetime(2025, 1, 6, tzinfo=timezone.utc)
    )

class TestCanonicalStoreObjectResolution:
    """Test object resolution functionality."""
    
    def test_resolve_canonical_object_id_direct(self, canonical_store, mock_canonical_issue):
        """Test resolving a canonical object ID (direct match)."""
        # Set up repository to return our canonical issue
        canonical_store.repo.get_issues.return_value = [mock_canonical_issue]
        
        # Should return the same ID since it's canonical
        result = canonical_store.resolve_canonical_object_id("metrics")
        assert result == "metrics"
        
        # Verify correct query was made - using string labels as the real implementation does
        canonical_store.repo.get_issues.assert_called_with(
            labels=[f"{LabelNames.UID_PREFIX}metrics", f"{LabelNames.ALIAS_TO_PREFIX}*"],
            state="all"
        )
        
    def test_resolve_canonical_object_id_alias(self, canonical_store, mock_alias_issue):
        """Test resolving an alias to find its canonical object ID."""
        # Set up repository to return our alias issue
        canonical_store.repo.get_issues.return_value = [mock_alias_issue]
        
        # Should return the canonical ID that the alias points to
        result = canonical_store.resolve_canonical_object_id("daily-metrics")
        assert result == "metrics"

    def test_resolve_canonical_object_id_nonexistent(self, canonical_store):
        """Test resolving a non-existent object ID."""
        # Set up repository to return no issues
        canonical_store.repo.get_issues.return_value = []
        
        # Should return the same ID since no alias was found
        result = canonical_store.resolve_canonical_object_id("nonexistent")
        assert result == "nonexistent"

    def test_resolve_canonical_object_id_circular_prevention(self, canonical_store, mock_label_factory):
        """Test prevention of circular references in alias resolution."""
        # Create a circular reference scenario
        circular_alias_1 = Mock()
        circular_alias_1.labels = [
            mock_label_factory(f"{LabelNames.UID_PREFIX}object-a"),
            mock_label_factory(f"{LabelNames.ALIAS_TO_PREFIX}object-b")
        ]
        
        circular_alias_2 = Mock()
        circular_alias_2.labels = [
            mock_label_factory(f"{LabelNames.UID_PREFIX}object-b"),
            mock_label_factory(f"{LabelNames.ALIAS_TO_PREFIX}object-a")
        ]
        
        # Set up repository to simulate circular references
        def mock_get_issues_side_effect(**kwargs):
            labels = kwargs.get('labels', [])
            if f"{LabelNames.UID_PREFIX}object-a" in labels:
                return [circular_alias_1]
            elif f"{LabelNames.UID_PREFIX}object-b" in labels:
                return [circular_alias_2]
            return []
            
        canonical_store.repo.get_issues.side_effect = mock_get_issues_side_effect
        
        # Should detect circular reference and return original ID
        result = canonical_store.resolve_canonical_object_id("object-a")
        assert result == "object-b"  # It should follow at least one level

class TestCanonicalStoreAliasing:
    """Test alias creation and handling."""

    def test_create_alias(self, canonical_store, mock_canonical_issue, mock_alias_issue, mock_label_factory, mock_issue_factory):
        """Test creating an alias relationship."""
        # Set up repository to find source and target objects
        def mock_get_issues_side_effect(**kwargs):
            labels = kwargs.get('labels', [])
            if f"{LabelNames.UID_PREFIX}weekly-metrics" in labels:
                # Source object
                return [mock_issue_factory(
                    number=101,
                    labels=[
                        LabelNames.STORED_OBJECT,
                        f"{LabelNames.UID_PREFIX}weekly-metrics"
                    ]
                )]
            elif f"{LabelNames.UID_PREFIX}metrics" in labels:
                # Target object
                return [mock_canonical_issue]
            return []
            
        canonical_store.repo.get_issues.side_effect = mock_get_issues_side_effect
        
        # Mock the add_to_labels method
        source_issue_mock = Mock()
        canonical_store.repo.get_issues.return_value = [source_issue_mock]
        
        # Mock the create_comment method
        source_issue_mock.create_comment = Mock()
        mock_canonical_issue.create_comment = Mock()
        
        # Create label if needed
        canonical_store.repo.create_label = Mock()
        
        # Execute create_alias
        result = canonical_store.create_alias("weekly-metrics", "metrics")
        
        # Verify result
        assert result["success"] is True
        assert result["source_id"] == "weekly-metrics"
        assert result["target_id"] == "metrics"
        
        # Verify label was created
        canonical_store.repo.create_label.assert_called_once()
        
        # Verify label was added to source issue
        #source_issue_mock.add_to_labels.assert_called_with(f"{LabelNames.ALIAS_TO_PREFIX}metrics")
        #assert f"{LabelNames.ALIAS_TO_PREFIX}metrics" in source_issue_mock.labels
        #assert source_issue_mock.labels.append.assert_called_with(f"{LabelNames.ALIAS_TO_PREFIX}metrics")
        
        # Verify system comments were added
        #source_issue_mock.create_comment.assert_called_once()
        #mock_canonical_issue.create_comment.assert_called_once()

    def test_create_alias_already_alias(self, canonical_store, mock_alias_issue):
        """Test error when creating an alias for an object that is already an alias."""
        # Set up repository to return an issue that's already an alias
        canonical_store.repo.get_issues.return_value = [mock_alias_issue]
        
        # Should raise ValueError
        with pytest.raises(ValueError, match="Object daily-metrics is already an alias"):
            canonical_store.create_alias("daily-metrics", "metrics")

    def test_create_alias_source_not_found(self, canonical_store):
        """Test error when source object is not found."""
        # Set up repository to return no issues
        canonical_store.repo.get_issues.return_value = []
        
        # Should raise ObjectNotFound
        with pytest.raises(Exception, match="Source object not found"):
            canonical_store.create_alias("nonexistent", "metrics")

    def test_create_alias_target_not_found(self, canonical_store, mock_duplicate_issue):
        """Test error when target object is not found."""
        # Set up repository to find source but not target
        def mock_get_issues_side_effect(**kwargs):
            labels = kwargs.get('labels', [])
            if f"{LabelNames.UID_PREFIX}duplicate-metrics" in labels:
                return [mock_duplicate_issue]
            return []
            
        canonical_store.repo.get_issues.side_effect = mock_get_issues_side_effect
        
        # Should raise ObjectNotFound
        with pytest.raises(Exception, match="Target object not found"):
            canonical_store.create_alias("duplicate-metrics", "nonexistent")

class TestCanonicalStoreDeprecation:
    """Test object deprecation functionality."""
    
    # Update test for deprecate_object to use the new deprecate_issue method
    def test_deprecate_object(self, canonical_store_with_mocks, mock_issue_factory):
        """Test deprecating an object properly calls deprecate_issue."""
        store = canonical_store_with_mocks
        
        # Create source and target issues
        source_issue = mock_issue_factory(
            number=123,
            labels=[LabelNames.GH_STORE, LabelNames.STORED_OBJECT, "UID:old-metrics"],
            created_at=datetime(2025, 1, 5, tzinfo=timezone.utc)
        )
        
        target_issue = mock_issue_factory(
            number=456,
            labels=[LabelNames.GH_STORE, LabelNames.STORED_OBJECT, "UID:metrics"],
            created_at=datetime(2025, 1, 1, tzinfo=timezone.utc)
        )
        
        # Setup get_issues mock
        def mock_get_issues(**kwargs):
            labels = kwargs.get('labels', [])
            if len(labels) > 0:
                if "UID:old-metrics" in labels[0]:
                    return [source_issue]
                elif "UID:metrics" in labels[0]:
                    return [target_issue]
            return []
        
        store.repo.get_issues = Mock(side_effect=mock_get_issues)
        
        # Mock deprecate_issue
        expected_result = {
            "success": True,
            "source_issue": 123,
            "source_object_id": "old-metrics",
            "target_issue": 456,
            "target_object_id": "metrics",
            "reason": DeprecationReason.REPLACED
        }
        store.deprecate_issue = Mock(return_value=expected_result)
        
        # Execute deprecate_object
        result = store.deprecate_object("old-metrics", "metrics", DeprecationReason.REPLACED)
        
        # Verify result
        assert result == expected_result
        
        # Verify deprecate_issue was called with correct params
        store.deprecate_issue.assert_called_once_with(
            issue_number=123,
            target_issue_number=456,
            reason=DeprecationReason.REPLACED
        )

    
    # Test for attempting to deprecate an object as itself
    def test_deprecate_object_self_reference(self, canonical_store_with_mocks, mock_issue_factory):
        """Test that deprecating an object as itself raises an error."""
        store = canonical_store_with_mocks
        
        # Create a test issue
        issue = mock_issue_factory(
            number=123,
            labels=[LabelNames.GH_STORE, LabelNames.STORED_OBJECT, "UID:metrics"],
            created_at=datetime(2025, 1, 1, tzinfo=timezone.utc)
        )
        
        # Setup mocks
        store.repo.get_issues.return_value = [issue]
        
        # Verify that deprecate_object raises ValueError for self-reference
        with pytest.raises(ValueError, match="Cannot deprecate an object as itself"):
            store.deprecate_object("metrics", "metrics", DeprecationReason.REPLACED)
        
    
    # Modified test for deduplicate_object
    def test_deduplicate_object(self, canonical_store_with_mocks, mock_issue_factory):
        """Test deduplication of an object with multiple issues."""
        store = canonical_store_with_mocks
        
        # Create two issues with same UID and stored-object labels
        canonical_issue = mock_issue_factory(
            number=101,
            labels=[LabelNames.GH_STORE, LabelNames.STORED_OBJECT, "UID:metrics"],
            created_at=datetime(2025, 1, 1, tzinfo=timezone.utc)
        )
        
        duplicate_issue = mock_issue_factory(
            number=102,
            labels=[LabelNames.GH_STORE, LabelNames.STORED_OBJECT, "UID:metrics"],
            created_at=datetime(2025, 1, 2, tzinfo=timezone.utc)
        )
        
        # Setup mock for get_issues to return our test issues
        store.repo.get_issues.return_value = [canonical_issue, duplicate_issue]
        
        # Mock get_issue to return the correct issue by number
        def mock_get_issue(issue_number):
            if issue_number == 101:
                return canonical_issue
            elif issue_number == 102:
                return duplicate_issue
            return Mock()
            
        store.repo.get_issue = Mock(side_effect=mock_get_issue)
        
        # Mock _get_object_id to return the correct object ID
        store._get_object_id = Mock(return_value="metrics")
        
        # Mock deprecate_issue to simulate the deprecation and return success
        store.deprecate_issue = Mock(return_value={
            "success": True,
            "source_issue": 102,
            "source_object_id": "metrics",
            "target_issue": 101,
            "target_object_id": "metrics",
            "reason": DeprecationReason.DUPLICATE
        })
        
        # Execute deduplicate_object
        result = store.deduplicate_object("metrics")
        
        # Verify result
        assert result["success"] is True
        assert result["canonical_object_id"] == "metrics"
        assert result["canonical_issue"] == 101
        assert result["duplicates_processed"] == 1
        
        # Verify deprecate_issue was called with correct params
        store.deprecate_issue.assert_called_once_with(
            issue_number=102,
            target_issue_number=101,
            reason=DeprecationReason.DUPLICATE
        )
    
    # New test to verify deprecate_issue
    def test_deprecate_issue(self, canonical_store_with_mocks, mock_issue_factory):
        """Test deprecating a specific issue."""
        store = canonical_store_with_mocks
        
        # Create source and target issues
        source_issue = mock_issue_factory(
            number=123,
            labels=[LabelNames.GH_STORE, LabelNames.STORED_OBJECT, "UID:old-metrics"],
            created_at=datetime(2025, 1, 5, tzinfo=timezone.utc)
        )
        
        target_issue = mock_issue_factory(
            number=456,
            labels=[LabelNames.GH_STORE, LabelNames.STORED_OBJECT, "UID:metrics"],
            created_at=datetime(2025, 1, 1, tzinfo=timezone.utc)
        )
        
        # Setup get_issue mock
        def mock_get_issue(issue_number):
            if issue_number == 123:
                return source_issue
            elif issue_number == 456:
                return target_issue
            raise ValueError(f"Unknown issue number: {issue_number}")
        
        store.repo.get_issue = Mock(side_effect=mock_get_issue)
        
        # Mock _get_object_id to return the correct IDs
        def mock_get_object_id(issue):
            if issue.number == 123:
                return "old-metrics"
            elif issue.number == 456:
                return "metrics"
            return None
        
        store._get_object_id = Mock(side_effect=mock_get_object_id)
        
        # Mock label creation
        store.repo.create_label = Mock()
        
        # Mock adding/removing labels
        source_issue.add_to_labels = Mock()
        source_issue.remove_from_labels = Mock()
        
        # Execute deprecate_issue
        result = store.deprecate_issue(
            issue_number=123,
            target_issue_number=456,
            reason=DeprecationReason.MERGED
        )
        
        # Verify result
        assert result["success"] is True
        assert result["source_issue"] == 123
        assert result["source_object_id"] == "old-metrics"
        assert result["target_issue"] == 456
        assert result["target_object_id"] == "metrics"
        assert result["reason"] == DeprecationReason.MERGED
        
        # Verify labels were removed/added
        source_issue.remove_from_labels.assert_called_with(LabelNames.STORED_OBJECT)
        source_issue.add_to_labels.assert_called_with(
            LabelNames.DEPRECATED, 
            f"{LabelNames.MERGED_INTO_PREFIX}metrics",
            f"{LabelNames.DEPRECATED_BY_PREFIX}456"
        )


    def test_deduplicate_object_no_duplicates(self, canonical_store, mock_canonical_issue):
        """Test deduplication when no duplicates exist."""
        # Set up repository to find only one issue
        canonical_store.repo.get_issues.return_value = [mock_canonical_issue]
        
        # Execute deduplicate_object
        result = canonical_store.deduplicate_object("metrics")
        
        # Verify result
        assert result["success"] is True
        assert "message" in result
        assert "No duplicates found" in result["message"]

class TestCanonicalStoreVirtualMerge:
    """Test virtual merge processing."""

    def test_collect_all_comments(self, canonical_store, mock_canonical_issue, mock_alias_issue, mock_comment_factory):
        """Test collecting comments from canonical and alias issues."""
        # Create mock comments for each issue
        canonical_comments = [
            mock_comment_factory(
                body={
                    "type": "initial_state",
                    "_data": {"count": 0},
                    "_meta": {
                        "client_version": "0.7.0",
                        "timestamp": "2025-01-01T00:00:00Z",
                        "update_mode": "append",
                        "issue_number": 123  # Include issue number
                    }
                },
                comment_id=1,
                created_at=datetime(2025, 1, 1, tzinfo=timezone.utc)
            ),
            mock_comment_factory(
                body={
                    "_data": {"count": 10},
                    "_meta": {
                        "client_version": "0.7.0",
                        "timestamp": "2025-01-02T00:00:00Z",
                        "update_mode": "append",
                        "issue_number": 123  # Include issue number
                    }
                },
                comment_id=2,
                created_at=datetime(2025, 1, 2, tzinfo=timezone.utc)
            )
        ]
        
        alias_comments = [
            mock_comment_factory(
                body={
                    "_data": {"period": "daily"},
                    "_meta": {
                        "client_version": "0.7.0",
                        "timestamp": "2025-01-10T00:00:00Z",
                        "update_mode": "append",
                        "issue_number": 789  # Different issue number
                    }
                },
                comment_id=3,
                created_at=datetime(2025, 1, 10, tzinfo=timezone.utc)
            )
        ]
        
        # Set up mock_comments method returns
        mock_canonical_issue.get_comments.return_value = canonical_comments
        mock_alias_issue.get_comments.return_value = alias_comments
        
        # Set up repository to find canonical and alias issues
        def mock_get_issues_side_effect(**kwargs):
            labels = kwargs.get('labels', [])
            if f"{LabelNames.UID_PREFIX}metrics" in labels and f"{LabelNames.ALIAS_TO_PREFIX}*" not in labels:
                # When searching for canonical
                return [mock_canonical_issue]
            elif f"{LabelNames.ALIAS_TO_PREFIX}metrics" in labels:
                # When searching for aliases
                return [mock_alias_issue]
            return []
            
        canonical_store.repo.get_issues.side_effect = mock_get_issues_side_effect
        
        # Mock _extract_comment_metadata to return minimal test data
        def mock_extract_metadata(comment, issue_number, object_id):
            # Just return basic information directly from comment for testing
            try:
                data = json.loads(comment.body)
                return {
                    "data": data,
                    "timestamp": comment.created_at,
                    "id": comment.id,
                    "issue_number":issue_number,
                    "source_issue": issue_number,
                    "source_object_id": object_id
                }
            except:
                return None
                
        canonical_store._extract_comment_metadata = mock_extract_metadata
        
        # Execute collect_all_comments
        comments = canonical_store.collect_all_comments("metrics")
        
        # Verify results
        assert len(comments) == 3
        
        # Verify chronological order
        timestamps = [c["timestamp"] for c in comments]
        assert timestamps == sorted(timestamps)
        
        # Verify comment sources
        assert comments[0]["source_issue"] == mock_canonical_issue.number
        assert comments[1]["source_issue"] == mock_canonical_issue.number
        assert comments[2]["source_issue"] == mock_alias_issue.number

    def test_process_with_virtual_merge(self, canonical_store, mock_canonical_issue, mock_comment_factory):
        """Test processing virtual merge to build object state."""
        # Set mock_canonical_issue number
        mock_canonical_issue.number = 123
        
        # Create mock comments with proper structure
        comments = [
            {
                "data": {
                    "type": "initial_state",
                    "_data": {"count": 0, "name": "test"},
                    "_meta": {
                        "client_version": "0.7.0",
                        "timestamp": "2025-01-01T00:00:00Z",
                        "update_mode": "append",
                        "issue_number": 123  # Include issue number
                    }
                },
                "timestamp": datetime(2025, 1, 1, tzinfo=timezone.utc),
                "id": 1,
                "source_issue": 123,
                "source_object_id": "metrics"
            },
            {
                "data": {
                    "_data": {"count": 10},
                    "_meta": {
                        "client_version": "0.7.0",
                        "timestamp": "2025-01-02T00:00:00Z",
                        "update_mode": "append",
                        "issue_number": 123  # Include issue number
                    }
                },
                "timestamp": datetime(2025, 1, 2, tzinfo=timezone.utc),
                "id": 2,
                "source_issue": 123,
                "source_object_id": "metrics"
            },
            {
                "data": {
                    "_data": {"period": "daily"},
                    "_meta": {
                        "client_version": "0.7.0",
                        "timestamp": "2025-01-10T00:00:00Z",
                        "update_mode": "append",
                        "issue_number": 789  # Different issue number
                    }
                },
                "timestamp": datetime(2025, 1, 10, tzinfo=timezone.utc),
                "id": 3,
                "source_issue": 789,
                "source_object_id": "daily-metrics"
            },
            {
                "data": {
                    "_data": {"count": 42},
                    "_meta": {
                        "client_version": "0.7.0",
                        "timestamp": "2025-01-15T00:00:00Z",
                        "update_mode": "append",
                        "issue_number": 123  # Include issue number
                    }
                },
                "timestamp": datetime(2025, 1, 15, tzinfo=timezone.utc),
                "id": 4,
                "source_issue": 123,
                "source_object_id": "metrics"
            }
        ]
        
        # Mock collect_all_comments to return our preset comments
        canonical_store.collect_all_comments = Mock(return_value=comments)
        canonical_store.resolve_canonical_object_id = Mock(return_value="metrics")
        
        # Set up repository to find canonical issue
        canonical_store.repo.get_issues.return_value = [mock_canonical_issue]
        
        # Mock issue edit method
        mock_canonical_issue.edit = Mock()
        
        # Execute process_with_virtual_merge
        result = canonical_store.process_with_virtual_merge("metrics")
        
        # Verify results
        assert result.meta.object_id == "metrics"
        assert result.meta.issue_number == 123  # Verify issue number in result metadata
        
        # Verify data was merged correctly
        assert result.data["count"] == 42
        assert result.data["name"] == "test"
        assert result.data["period"] == "daily"
        
        # Verify canonical issue was updated
        mock_canonical_issue.edit.assert_called_once()

class TestCanonicalStoreGetUpdate:
    """Test get and update object operations with virtual merging."""

    def test_get_object_direct(self, canonical_store, mock_canonical_issue):
        """Test getting an object directly."""
        # Set mock_canonical_issue number
        mock_canonical_issue.number = 123
        
        # Set up resolve_canonical_object_id to return same ID
        canonical_store.resolve_canonical_object_id = Mock(return_value="metrics")
        
        # Set up process_with_virtual_merge to return a mock object
        mock_obj = Mock()
        mock_obj.meta.object_id = "metrics"
        mock_obj.meta.issue_number = 123  # Set issue number in returned object
        mock_obj.data = {"count": 42, "name": "test"}
        canonical_store.process_with_virtual_merge = Mock(return_value=mock_obj)
        
        # Execute get_object
        result = canonical_store.get_object("metrics")
        
        # Verify results
        assert result.meta.object_id == "metrics"
        assert result.meta.issue_number == 123  # Verify issue number
        assert result.data["count"] == 42
        
        # Verify correct methods were called
        canonical_store.resolve_canonical_object_id.assert_called_with("metrics")
        canonical_store.process_with_virtual_merge.assert_called_with("metrics")

    def test_get_object_via_alias(self, canonical_store):
        """Test getting an object via its alias."""
        # Set up resolve_canonical_object_id to return canonical ID
        canonical_store.resolve_canonical_object_id = Mock(return_value="metrics")
        
        # Set up process_with_virtual_merge to return a mock object
        mock_obj = Mock()
        mock_obj.meta.object_id = "metrics"
        mock_obj.meta.issue_number = 123  # Set issue number in returned object
        mock_obj.data = {"count": 42, "name": "test"}
        canonical_store.process_with_virtual_merge = Mock(return_value=mock_obj)
        
        # Execute get_object with alias ID
        result = canonical_store.get_object("daily-metrics")
        
        # Verify results
        assert result.meta.object_id == "metrics"
        assert result.meta.issue_number == 123  # Verify issue number
        assert result.data["count"] == 42
        
        # Verify correct methods were called
        canonical_store.resolve_canonical_object_id.assert_called_with("daily-metrics")
        canonical_store.process_with_virtual_merge.assert_called_with("metrics")

    def test_update_object_alias(self, canonical_store, mock_alias_issue):
        """Test updating an object via its alias."""
        # Set mock_alias_issue number
        mock_alias_issue.number = 789
        
        # Setup to find the alias issue
        canonical_store.repo.get_issues.return_value = [mock_alias_issue]
        
        # Mock issue create_comment and edit methods
        mock_alias_issue.create_comment = Mock()
        mock_alias_issue.edit = Mock()
        
        # Mock get_object to return a result after update
        mock_obj = Mock()
        mock_obj.meta.object_id = "metrics"
        mock_obj.meta.issue_number = 123  # Set issue number in result
        mock_obj.data = {"count": 42, "name": "test", "period": "daily", "new_field": "value"}
        canonical_store.get_object = Mock(return_value=mock_obj)
        
        # Execute update_object on the alias
        changes = {"new_field": "value"}
        result = canonical_store.update_object("daily-metrics", changes)
        
        # Verify results
        assert result.meta.object_id == "metrics"
        assert result.meta.issue_number == 123  # Verify issue number
        assert result.data["new_field"] == "value"
        
        # Verify comment was added to alias issue
        mock_alias_issue.create_comment.assert_called_once()
        
        # Verify issue was reopened
        mock_alias_issue.edit.assert_called_with(state="open")
        
        # Verify comment payload included issue number
        call_args = mock_alias_issue.create_comment.call_args[0]
        comment_payload = json.loads(call_args[0])
        assert "issue_number" in comment_payload["_meta"]
        assert comment_payload["_meta"]["issue_number"] == 789  # Should use alias issue number

    def test_update_object_deprecated(self, canonical_store, mock_deprecated_issue, mock_canonical_issue, mock_label_factory):
        """Test updating a deprecated object."""
        # Set mock issue numbers
        mock_deprecated_issue.number = 457
        mock_canonical_issue.number = 123
        
        # Setup to find a deprecated issue pointing to a canonical object
        def mock_get_issues_side_effect(**kwargs):
            labels = kwargs.get('labels', [])
            if f"{LabelNames.MERGED_INTO_PREFIX}*" in labels and LabelNames.DEPRECATED in labels:
                return [mock_deprecated_issue]
            elif f"{LabelNames.UID_PREFIX}metrics" in labels:
                return [mock_canonical_issue]
            return []
            
        canonical_store.repo.get_issues.side_effect = mock_get_issues_side_effect
        
        # Setup mock_deprecated_issue to have proper labels
        mock_deprecated_issue.labels = [
            mock_label_factory(name=LabelNames.DEPRECATED),
            mock_label_factory(name=f"{LabelNames.MERGED_INTO_PREFIX}metrics")
        ]
        
        # Mock issue create_comment and edit methods
        mock_canonical_issue.create_comment = Mock()
        mock_canonical_issue.edit = Mock()
        
        # Mock get_object to return a result after update
        mock_obj = Mock()
        mock_obj.meta.object_id = "metrics"
        mock_obj.meta.issue_number = 123  # Set issue number
        mock_obj.data = {"count": 42, "name": "test", "new_field": "value"}
        canonical_store.get_object = Mock(return_value=mock_obj)
        canonical_store.resolve_canonical_object_id = Mock(return_value="metrics")
        
        # Execute update_object
        changes = {"new_field": "value"}
        result = canonical_store.update_object("old-metrics", changes)
        
        # Verify results
        assert result.meta.object_id == "metrics"
        assert result.meta.issue_number == 123  # Verify issue number
        assert result.data["new_field"] == "value"
    
    def test_update_object_on_alias_preserves_identity(self, canonical_store, mock_alias_issue):
        """
        Test that an update on an alias returns the object without merging into the canonical record.
        """
        # Set mock_alias_issue number
        mock_alias_issue.number = 789
        
        # Setup: mock_alias_issue should represent the alias "daily-metrics" pointing to "metrics".
        canonical_store.repo.get_issues.return_value = [mock_alias_issue]
        
        # Mock update behavior
        mock_alias_issue.create_comment = Mock()
        mock_alias_issue.edit = Mock()
        
        # Mock get_object with canonicalize=False to return the alias object
        alias_obj = Mock()
        alias_obj.meta.object_id = "daily-metrics"
        alias_obj.meta.issue_number = 789  # Set alias issue number
        alias_obj.data = {"period": "daily", "additional": "info"}
        
        canonical_store.get_object = Mock(return_value=alias_obj)
        
        # Assume update_object is called with changes.
        changes = {"additional": "info"}
        updated_obj = canonical_store.update_object("daily-metrics", changes)
        
        # Since update_object now returns get_object(..., canonicalize=False),
        # the alias identity should be preserved.
        assert updated_obj.meta.object_id == "daily-metrics"
        assert updated_obj.meta.issue_number == 789  # Verify issue number
    

class TestCanonicalStoreFinding:
    """Test finding duplicates and aliases."""
    
    def test_find_duplicates(self, canonical_store_with_mocks, mock_issue_factory):
        """Test finding duplicate objects."""
        store = canonical_store_with_mocks
        
        # Create issues with same UID
        issue1 = mock_issue_factory(
            number=101,
            labels=[LabelNames.GH_STORE, LabelNames.STORED_OBJECT, "UID:metrics"]
        )
        
        issue2 = mock_issue_factory(
            number=102,
            labels=[LabelNames.GH_STORE, LabelNames.STORED_OBJECT, "UID:metrics"]
        )
        
        # Setup mock for get_issues
        store.repo.get_issues.return_value = [issue1, issue2]
        
        # Execute find_duplicates
        duplicates = store.find_duplicates()
        
        # Verify results - should find duplicates for "UID:metrics"
        assert len(duplicates) == 1
        assert "UID:metrics" in duplicates
        assert len(duplicates["UID:metrics"]) == 2

    def test_find_aliases(self, canonical_store, mock_alias_issue):
        """Test finding aliases for objects."""
        # Set mock_alias_issue number
        mock_alias_issue.number = 789
        
        # Set up repository to return a list of alias issues
        canonical_store.repo.get_issues.return_value = [mock_alias_issue]
        
        # Mock _get_object_id method
        canonical_store._get_object_id = Mock(return_value="daily-metrics")
        
        # Execute find_aliases
        aliases = canonical_store.find_aliases()
        
        # Verify results
        assert len(aliases) == 1
        assert aliases["daily-metrics"] == "metrics"

    def test_find_aliases_for_specific_object(self, canonical_store, mock_alias_issue):
        """Test finding aliases for a specific object."""
        # Set mock_alias_issue number
        mock_alias_issue.number = 789
        
        # Set up repository to return a list of alias issues
        canonical_store.repo.get_issues.return_value = [mock_alias_issue]
        
        # Mock _get_object_id method
        canonical_store._get_object_id = Mock(return_value="daily-metrics")
        
        # Execute find_aliases with specific object
        aliases = canonical_store.find_aliases("metrics")
        
        # Verify results
        assert len(aliases) == 1
        assert aliases["daily-metrics"] == "metrics"
        
        # Verify correct query was made
        canonical_store.repo.get_issues.assert_called_with(
            labels=[f"{LabelNames.ALIAS_TO_PREFIX}metrics"],
            state="all"
        )
    
    # def test_get_object_canonicalize_modes(self, canonical_store_with_mocks, mock_issue_factory, mock_comment_factory):
    #     """Test different canonicalization modes in get_object."""
    #     store = canonical_store_with_mocks
        
    #     # Create a mock alias issue (daily-metrics -> metrics)
    #     alias_issue = mock_issue_factory(
    #         number=101,
    #         labels=["stored-object", "UID:daily-metrics", "ALIAS-TO:metrics"],
    #         body=json.dumps({"period": "daily"})
    #     )
        
    #     # Create a mock canonical issue with initial state
    #     initial_state_comment = mock_comment_factory(
    #         body={
    #             "type": "initial_state",
    #             "_data": {"count": 42},
    #             "_meta": {
    #                 "client_version": "0.7.0", 
    #                 "timestamp": "2025-01-01T00:00:00Z",
    #                 "update_mode": "append",
    #                 "issue_number": 102  # Include issue number
    #             }
    #         },
    #         comment_id=1
    #     )
        
    #     canonical_issue = mock_issue_factory(
    #         number=102,
    #         labels=["stored-object", "UID:metrics"],
    #         body=json.dumps({"count": 42}),
    #         comments=[initial_state_comment]
    #     )
        
    #     # Setup mocks for get_issues
    #     def mock_get_issues(**kwargs):
    #         labels = kwargs.get('labels', [])
    #         if isinstance(labels, list):
    #             if any(label == "UID:daily-metrics" for label in labels):
    #                 return [alias_issue]
    #             elif any(label == "UID:metrics" for label in labels):
    #                 return [canonical_issue]
    #             elif any(label == "ALIAS-TO:metrics" for label in labels):
    #                 return [alias_issue]
    #         return []
        
    #     store.repo.get_issues = Mock(side_effect=mock_get_issues)
        
    #     # Setup resolve_canonical_object_id to correctly resolve the alias
    #     store.resolve_canonical_object_id = Mock(side_effect=lambda obj_id: "metrics" if obj_id == "daily-metrics" else obj_id)
        
    #     # Setup get_object_by_number
    #     def get_object_by_number(number):
    #         if number == 101:  # alias
    #             meta = Mock(
    #                 object_id="daily-metrics",
    #                 label="daily-metrics",
    #                 issue_number=101,  # Include issue number
    #                 created_at=datetime(2025, 1, 1, tzinfo=timezone.utc),
    #                 updated_at=datetime(2025, 1, 2, tzinfo=timezone.utc),
    #                 version=1
    #             )
    #             return Mock(meta=meta, data={"period": "daily"})
    #         else:  # canonical
    #             meta = Mock(
    #                 object_id="metrics",
    #                 label="metrics",
    #                 issue_number=102,  # Include issue number
    #                 created_at=datetime(2025, 1, 1, tzinfo=timezone.utc),
    #                 updated_at=datetime(2025, 1, 2, tzinfo=timezone.utc),
    #                 version=1
    #             )
    #             return Mock(meta=meta, data={"count": 42})
        
    #     store.issue_handler.get_object_by_number = Mock(side_effect=get_object_by_number)
        
    #     # Mock collect_all_comments to include the initial state
    #     store.collect_all_comments = Mock(return_value=[
    #         {
    #             "data": {
    #                 "type": "initial_state",
    #                 "_data": {"count": 42},
    #                 "_meta": {
    #                     "client_version": "0.7.0",
    #                     "timestamp": "2025-01-01T00:00:00Z",
    #                     "update_mode": "append",
    #                     "issue_number": 102  # Include issue number
    #                 }
    #             },
    #             "timestamp": datetime(2025, 1, 1, tzinfo=timezone.utc),
    #             "id": 1,
    #             "source_issue": 102,
    #             "source_object_id": "metrics"
    #         }
    #     ])
        
    #     # Mock process_with_virtual_merge
    #     store.process_with_virtual_merge = Mock(return_value=Mock(
    #         meta=Mock(
    #             object_id="metrics",
    #             issue_number=102  # Include issue number
    #         ),
    #         data={"count": 42}
    #     ))
        
    #     # Test canonicalize=True (default) - should return canonical object
    #     obj_canonical = store.get_object("daily-metrics", canonicalize=True)
    #     assert obj_canonical.meta.object_id == "metrics"
    #     assert obj_canonical.meta.issue_number == 102  # Verify issue number
        
    #     # Test canonicalize=False - should return alias object directly
    #     obj_direct = store.get_object("daily-metrics", canonicalize=False)
    #     assert obj_direct.meta.object_id == "daily-metrics"
    #     assert obj_direct.meta.issue_number == 101  # Verify issue number



---
File: tests/unit/test_cli.py
---
# tests/unit/test_cli.py (Updated for Iterator Support)

import json
from pathlib import Path
from datetime import datetime, timedelta, timezone
import pytest
from unittest.mock import Mock, patch

from gh_store.__main__ import CLI
from gh_store.cli import commands
from gh_store.core.exceptions import GitHubStoreError

class TestCLIBasicOperations:
    """Test basic CLI operations like create, get, update, delete"""
    
    def test_create_object(self, mock_cli, mock_store_response, tmp_path, caplog):
        """Test creating a new object via CLI"""
        data = json.dumps({"name": "test", "value": 42})
        
        with patch('gh_store.cli.commands.get_store') as mock_get_store:
            mock_store = Mock()
            mock_get_store.return_value = mock_store
            mock_store.create.return_value = mock_store_response
            
            # Execute command
            mock_cli.create("test-123", data)
            
            # Verify store interactions
            mock_store.create.assert_called_once_with(
                "test-123",
                {"name": "test", "value": 42}
            )
            assert "Created object test-123" in caplog.text
    
    def test_get_object(self, mock_cli, mock_store_response, tmp_path):
        """Test retrieving an object via CLI"""
        output_file = tmp_path / "output.json"
        
        with patch('gh_store.cli.commands.get_store') as mock_get_store:
            mock_store = Mock()
            mock_get_store.return_value = mock_store
            mock_store.get.return_value = mock_store_response
            
            # Execute command
            mock_cli.get("test-123", output=str(output_file))
            
            # Verify output file
            assert output_file.exists()
            content = json.loads(output_file.read_text())
            assert content["object_id"] == "test-123"
            assert content["data"] == {"name": "test", "value": 42}
    
    def test_delete_object(self, mock_cli, mock_store_response, caplog):
        """Test deleting an object via CLI"""
        with patch('gh_store.cli.commands.get_store') as mock_get_store:
            mock_store = Mock()
            mock_get_store.return_value = mock_store
            
            # Execute command
            mock_cli.delete("test-123")
            
            # Verify store interactions
            mock_store.delete.assert_called_once_with("test-123")
            assert "Deleted object test-123" in caplog.text

class TestCLIUpdateOperations:
    """Test update-related CLI operations"""
    
    def test_update_object(self, mock_cli, mock_store_response, caplog):
        """Test updating an object via CLI"""
        changes = json.dumps({"value": 43})
        
        with patch('gh_store.cli.commands.get_store') as mock_get_store:
            mock_store = Mock()
            mock_get_store.return_value = mock_store
            mock_store.update.return_value = mock_store_response
            
            # Execute command
            mock_cli.update("test-123", changes)
            
            # Verify store interactions
            mock_store.update.assert_called_once_with(
                "test-123",
                {"value": 43}
            )
            assert "Updated object" in caplog.text
    
    def test_process_updates(self, mock_cli, mock_store_response, caplog):
        """Test processing pending updates via CLI"""
        with patch('gh_store.cli.commands.get_store') as mock_get_store:
            mock_store = Mock()
            mock_get_store.return_value = mock_store
            mock_store.process_updates.return_value = mock_store_response
            
            # Execute command
            mock_cli.process_updates(123)
            
            # Verify store interactions
            mock_store.process_updates.assert_called_once_with(123)

# Add to tests/unit/test_cli.py - Enhanced snapshot tests

class TestCLISnapshotOperations:
    """Test snapshot-related CLI operations"""
    
    def test_create_snapshot(self, mock_cli, mock_stored_objects, tmp_path, caplog):
        """Test creating a snapshot via CLI"""
        output_path = tmp_path / "snapshot.json"
        
        with patch('gh_store.cli.commands.get_store') as mock_get_store:
            mock_store = Mock()
            mock_get_store.return_value = mock_store
            
            # Create iterator from mock_stored_objects
            mock_store.list_all.return_value = mock_stored_objects
            
            # Execute command
            mock_cli.snapshot(output=str(output_path))
            
            # Verify output
            assert output_path.exists()
            snapshot = json.loads(output_path.read_text())
            assert "snapshot_time" in snapshot
            assert len(snapshot["objects"]) == len(mock_stored_objects)
            assert "Snapshot written to" in caplog.text
    
    def test_update_snapshot_with_changes(self, mock_cli, mock_stored_objects, mock_snapshot_file_factory, caplog):
        """Test updating snapshot when objects have actually changed."""
        # Create a snapshot with a known timestamp
        one_day_ago = datetime.now(timezone.utc) - timedelta(days=1)
        snapshot_path = mock_snapshot_file_factory(snapshot_time=one_day_ago, include_objects=[0])
        
        with patch('gh_store.cli.commands.get_store') as mock_get_store:
            mock_store = Mock()
            mock_get_store.return_value = mock_store
            
            # Configure a mock object that's newer than the snapshot
            updated_obj = mock_stored_objects[1]
            updated_obj.meta.updated_at = one_day_ago + timedelta(hours=2)
            
            # Only return the "updated" object
            mock_store.list_updated_since.return_value = [updated_obj]
            
            # Store original snapshot data for comparison
            original_snapshot = json.loads(snapshot_path.read_text())
            
            # Execute command
            mock_cli.update_snapshot(str(snapshot_path))
            
            # Verify correct arguments
            mock_store.list_updated_since.assert_called_once()
            assert mock_store.list_updated_since.call_args[0][0] == one_day_ago
            
            # Read updated snapshot
            updated_snapshot = json.loads(snapshot_path.read_text())
            
            # Verify timestamp was updated
            assert updated_snapshot["snapshot_time"] != original_snapshot["snapshot_time"]
            
            # Verify updated object was added
            assert updated_obj.meta.object_id in updated_snapshot["objects"]
            
            # Verify log message
            assert "Updated 1 objects in snapshot" in caplog.text
    
    def test_update_snapshot_no_changes(self, mock_cli, mock_stored_objects, mock_snapshot_file_factory, caplog):
        """Test not updating snapshot when no objects have changed."""
        # Create a snapshot with a known timestamp
        one_day_ago = datetime.now(timezone.utc) - timedelta(days=1)
        snapshot_path = mock_snapshot_file_factory(snapshot_time=one_day_ago)
        
        with patch('gh_store.cli.commands.get_store') as mock_get_store:
            mock_store = Mock()
            mock_get_store.return_value = mock_store
            
            # Return empty iterator - no objects were updated
            mock_store.list_updated_since.return_value = []
            
            # Store original snapshot data for comparison
            original_snapshot = json.loads(snapshot_path.read_text())
            
            # Execute command
            mock_cli.update_snapshot(str(snapshot_path))
            
            # Read updated snapshot
            updated_snapshot = json.loads(snapshot_path.read_text())
            
            # Verify timestamp was NOT updated
            assert updated_snapshot["snapshot_time"] == original_snapshot["snapshot_time"]
            
            # Verify objects are unchanged
            assert updated_snapshot["objects"] == original_snapshot["objects"]
            
            # Verify log message
            assert "No updates found since last snapshot" in caplog.text
    
    def test_update_snapshot_empty_file(self, mock_cli, mock_stored_objects, tmp_path, caplog):
        """Test error handling when updating a snapshot with invalid content."""
        empty_file = tmp_path / "empty.json"
        empty_file.write_text("{}")
        
        with pytest.raises(Exception) as exc_info:
            mock_cli.update_snapshot(str(empty_file))
            

class TestCLIErrorHandling:
    """Test CLI error handling scenarios"""
    
    def test_invalid_json_data(self, mock_cli):
        """Test handling of invalid JSON input"""
        with pytest.raises(json.decoder.JSONDecodeError) as exc_info:
            mock_cli.create("test-123", "invalid json")
    
    def test_file_not_found(self, mock_cli, caplog):
        """Test handling of missing snapshot file"""
        with pytest.raises(FileNotFoundError) as exc_info:
            mock_cli.update_snapshot("/nonexistent/path")
            
        assert "Snapshot file not found" in caplog.text

# should probably just deprecate all the config stuff.
# class TestCLIConfigHandling:
#     """Test CLI configuration handling"""
    
#     def test_init_creates_config(self, mock_cli, tmp_path, caplog):
#         """Test initialization of new config file."""
#         config_path = tmp_path / "new_config.yml"
        
#         with patch('gh_store.cli.commands.ensure_config_exists') as mock_ensure:
#             # Run command
#             mock_cli.init(config=str(config_path))
            
#             # Verify config creation was attempted
#             mock_ensure.assert_called_once_with(config_path)
    
#     def test_custom_config_path(self, mock_cli, mock_config, mock_store_response):
#         """Test using custom config path"""
#         with patch('gh_store.cli.commands.get_store') as mock_get_store, \
#              patch('gh_store.cli.commands.ensure_config_exists') as mock_ensure:
#             mock_store = Mock()
#             mock_get_store.return_value = mock_store
#             mock_store.get.return_value = mock_store_response
            
#             # Execute command with custom config
#             mock_cli.get("test-123", config=str(mock_config))
            
#             # Verify store creation
#             mock_get_store.assert_called_with(
#                 token=None,
#                 repo=None,
#                 config=str(mock_config)
#             )



---
File: tests/unit/test_comment_handler.py
---
# tests/unit/test_comment_handler.py

import json
from datetime import datetime, timezone
from unittest.mock import Mock, patch

import pytest
from gh_store.handlers.comment import CommentHandler
from gh_store.core.version import CLIENT_VERSION

@pytest.fixture
def mock_repo():
    return Mock()

@pytest.fixture
def mock_config():
    return Mock(
        store=Mock(
            reactions=Mock(
                processed="+1",
                initial_state="rocket"
            )
        )
    )

@pytest.fixture
def comment_handler(mock_repo, mock_config):
    return CommentHandler(mock_repo, mock_config)

def test_get_unprocessed_updates_mixed_comments(comment_handler, mock_repo):
    """Test processing a mix of valid and invalid comments"""
    
    # Setup mock issue with various types of comments
    issue = Mock()
    issue.number = 123  # Add issue number
    mock_repo.get_issue.return_value = issue
    
    # Create a variety of comments to test filtering
    comments = [
        # Valid update with metadata from authorized user
        Mock(
            id=1,
            body=json.dumps({
                '_data': {'update': 'valid'},
                '_meta': {
                    'client_version': CLIENT_VERSION,
                    'timestamp': '2025-01-01T00:00:00Z',
                    'update_mode': 'append',
                    'issue_number': 123  # Add issue number
                }
            }),
            created_at=datetime(2025, 1, 1, tzinfo=timezone.utc),
            user=Mock(login="owner"),
            get_reactions=Mock(return_value=[])  # No reactions = unprocessed
        ),
        
        # Already processed update (should be skipped)
        Mock(
            id=2,
            body=json.dumps({
                '_data': {'update': 'processed'},
                '_meta': {
                    'client_version': CLIENT_VERSION,
                    'timestamp': '2025-01-01T00:00:00Z',
                    'update_mode': 'append',
                    'issue_number': 123  # Add issue number
                }
            }),
            user=Mock(login="owner"),
            get_reactions=Mock(return_value=[Mock(content="+1")])
        ),
        
        # Legacy format comment (should be handled with generated metadata)
        Mock(
            id=3,
            body='{"legacy": "update"}',
            created_at=datetime(2025, 1, 1, tzinfo=timezone.utc),
            user=Mock(login="owner"),
            get_reactions=Mock(return_value=[])
        ),
        
        # Initial state comment (should be skipped)
        Mock(
            id=4,
            body=json.dumps({
                'type': 'initial_state',
                '_data': {'initial': 'state'},
                '_meta': {
                    'client_version': CLIENT_VERSION,
                    'timestamp': '2025-01-01T00:00:00Z',
                    'update_mode': 'append',
                    'issue_number': 123  # Add issue number
                }
            }),
            user=Mock(login="owner"),
            get_reactions=Mock(return_value=[])
        ),
        
        # Invalid JSON comment (should be skipped)
        Mock(
            id=5,
            body='not json',
            user=Mock(login="owner"),
            get_reactions=Mock(return_value=[])
        ),
        
        # Valid JSON but unauthorized user (should be skipped)
        Mock(
            id=6,
            body=json.dumps({
                '_data': {'update': 'unauthorized'},
                '_meta': {
                    'client_version': CLIENT_VERSION,
                    'timestamp': '2025-01-02T00:00:00Z',
                    'update_mode': 'append',
                    'issue_number': 123  # Add issue number
                }
            }),
            created_at=datetime(2025, 1, 2, tzinfo=timezone.utc),
            user=Mock(login="random-user"),
            get_reactions=Mock(return_value=[])
        ),
        
        # Regular discussion comment (should be skipped)
        Mock(
            id=7,
            body='Just a regular comment',
            user=Mock(login="random-user"),
            get_reactions=Mock(return_value=[])
        )
    ]
    
    issue.get_comments.return_value = comments
    
    # Mock the access control to only authorize "owner"
    comment_handler.access_control._get_owner_info = Mock(
        return_value={"login": "owner", "type": "User"}
    )
    comment_handler.access_control._find_codeowners_file = Mock(return_value=None)
    
    # Get unprocessed updates
    updates = comment_handler.get_unprocessed_updates(123)
    
    # Should get two valid updates (new format and legacy format)
    assert len(updates) == 2
    assert updates[0].comment_id == 1
    assert updates[0].changes == {'update': 'valid'}
    assert updates[1].comment_id == 3
    assert updates[1].changes == {'legacy': 'update'}

def test_get_unprocessed_updates_unauthorized_json(comment_handler, mock_repo):
    """Test that valid JSON updates from unauthorized users are skipped"""
    issue = Mock()
    issue.number = 123  # Add issue number
    mock_repo.get_issue.return_value = issue
    
    # Create an unauthorized but valid JSON update
    comment = Mock(
        id=1,
        body=json.dumps({
            '_data': {'malicious': 'update'},
            '_meta': {
                'client_version': CLIENT_VERSION,
                'timestamp': '2025-01-01T00:00:00Z',
                'update_mode': 'append',
                'issue_number': 123  # Add issue number
            }
        }),
        created_at=datetime(2025, 1, 1, tzinfo=timezone.utc),
        user=Mock(login="attacker"),
        get_reactions=Mock(return_value=[])
    )
    
    issue.get_comments.return_value = [comment]
    
    # Mock access control to reject the user
    comment_handler.access_control._get_owner_info = Mock(
        return_value={"login": "owner", "type": "User"}
    )
    comment_handler.access_control._find_codeowners_file = Mock(return_value=None)
    
    updates = comment_handler.get_unprocessed_updates(123)
    assert len(updates) == 0

def test_get_unprocessed_updates_with_codeowners(comment_handler, mock_repo):
    """Test processing updates with CODEOWNERS authorization"""
    issue = Mock()
    issue.number = 123  # Add issue number
    mock_repo.get_issue.return_value = issue
    
    # Create comments from different users
    comments = [
        # From CODEOWNERS team member
        Mock(
            id=1,
            body=json.dumps({
                '_data': {'update': 'from-team'},
                '_meta': {
                    'client_version': CLIENT_VERSION,
                    'timestamp': '2025-01-01T00:00:00Z',
                    'update_mode': 'append',
                    'issue_number': 123  # Add issue number
                }
            }),
            created_at=datetime(2025, 1, 1, tzinfo=timezone.utc),
            user=Mock(login="team-member"),
            get_reactions=Mock(return_value=[])
        ),
        # From unauthorized user
        Mock(
            id=2,
            body=json.dumps({
                '_data': {'update': 'unauthorized'},
                '_meta': {
                    'client_version': CLIENT_VERSION,
                    'timestamp': '2025-01-02T00:00:00Z',
                    'update_mode': 'append',
                    'issue_number': 123  # Add issue number
                }
            }),
            created_at=datetime(2025, 1, 2, tzinfo=timezone.utc),
            user=Mock(login="random-user"),
            get_reactions=Mock(return_value=[])
        )
    ]
    
    issue.get_comments.return_value = comments
    
    # Mock CODEOWNERS to include team-member
    comment_handler.access_control._get_owner_info = Mock(
        return_value={"login": "owner", "type": "User"}
    )
    # Set up CODEOWNERS content
    codeowners_content = "* @team-member"
    comment_handler.access_control._find_codeowners_file = Mock(
        return_value=codeowners_content
    )
    
    updates = comment_handler.get_unprocessed_updates(123)
    
    # Should only get update from team member
    assert len(updates) == 1
    assert updates[0].comment_id == 1
    assert updates[0].changes == {'update': 'from-team'}

def test_get_unprocessed_updates_empty(comment_handler, mock_repo):
    """Test behavior with no comments"""
    issue = Mock()
    issue.number = 123  # Add issue number
    mock_repo.get_issue.return_value = issue
    issue.get_comments.return_value = []
    
    updates = comment_handler.get_unprocessed_updates(123)
    assert len(updates) == 0

def test_get_unprocessed_updates_all_processed(comment_handler, mock_repo):
    """Test behavior when all comments are already processed"""
    issue = Mock()
    issue.number = 123  # Add issue number
    mock_repo.get_issue.return_value = issue
    
    # Create some processed comments
    comments = [
        Mock(
            id=1,
            body=json.dumps({
                '_data': {'update': 'processed'},
                '_meta': {
                    'client_version': CLIENT_VERSION,
                    'timestamp': '2025-01-01T00:00:00Z',
                    'update_mode': 'append',
                    'issue_number': 123  # Add issue number
                }
            }),
            user=Mock(login="owner"),
            get_reactions=Mock(return_value=[Mock(content="+1")])
        ),
        Mock(
            id=2,
            body=json.dumps({
                '_data': {'another': 'processed'},
                '_meta': {
                    'client_version': CLIENT_VERSION,
                    'timestamp': '2025-01-01T00:00:00Z',
                    'update_mode': 'append',
                    'issue_number': 123  # Add issue number
                }
            }),
            user=Mock(login="owner"),
            get_reactions=Mock(return_value=[Mock(content="+1")])
        )
    ]
    
    issue.get_comments.return_value = comments
    
    updates = comment_handler.get_unprocessed_updates(123)
    assert len(updates) == 0

def test_create_comment_payload(comment_handler):
    """Test creation of properly structured comment payloads"""
    data = {'test': 'data'}
    issue_number = 123  # Add issue number parameter
    
    # Test regular update payload
    update_payload = comment_handler.create_comment_payload(data, issue_number)
    assert update_payload._data == data
    assert update_payload._meta.client_version == CLIENT_VERSION
    assert update_payload._meta.update_mode == 'append'
    assert update_payload._meta.issue_number == issue_number  # Verify issue_number
    assert update_payload.type is None
    
    # Test initial state payload
    initial_payload = comment_handler.create_comment_payload(data, issue_number, 'initial_state')
    assert initial_payload._data == data
    assert initial_payload._meta.client_version == CLIENT_VERSION
    assert initial_payload._meta.update_mode == 'append'
    assert initial_payload._meta.issue_number == issue_number  # Verify issue_number
    assert initial_payload.type == 'initial_state'

def test_get_unprocessed_updates_malformed_metadata(comment_handler, mock_repo):
    """Test handling of malformed metadata in comments"""
    issue = Mock()
    issue.number = 123  # Add issue number
    mock_repo.get_issue.return_value = issue
    
    # Create comment with malformed metadata
    malformed_comment = Mock(
        id=1,
        body=json.dumps({
            '_data': {'test': 'data'},
            '_meta': {
                # Missing required fields
                'client_version': CLIENT_VERSION,
                # missing issue_number
            }
        }),
        created_at=datetime(2025, 1, 1, tzinfo=timezone.utc),
        user=Mock(login="owner"),
        get_reactions=Mock(return_value=[])
    )
    
    issue.get_comments.return_value = [malformed_comment]
    
    # Mock access control
    comment_handler.access_control._get_owner_info = Mock(
        return_value={"login": "owner", "type": "User"}
    )
    comment_handler.access_control._find_codeowners_file = Mock(return_value=None)
    
    # Should skip malformed comment
    updates = comment_handler.get_unprocessed_updates(123)
    assert len(updates) == 0

def test_apply_update_preserves_metadata(comment_handler):
    """Test that applying updates preserves any existing metadata"""
    # Create mock object with existing metadata
    obj = Mock()
    obj.meta = Mock(object_id='test-123', issue_number=123)  # Add issue_number
    obj.data = {
        'value': 1,
        '_meta': {
            'some': 'metadata'
        }
    }
    
    # Create update that includes metadata
    update = Mock(
        comment_id=1,
        timestamp=datetime(2025, 1, 1, tzinfo=timezone.utc),
        changes={
            'value': 2,
            '_meta': {
                'new': 'metadata'
            }
        }
    )
    
    # Apply update
    result = comment_handler.apply_update(obj, update)
    
    # Verify metadata was updated correctly
    assert result.data['value'] == 2
    assert result.data['_meta']['some'] == 'metadata'
    assert result.data['_meta']['new'] == 'metadata'



---
File: tests/unit/test_config.py
---
# tests/unit/test_config.py

from pathlib import Path
import pytest
from unittest.mock import patch, mock_open
import yaml

from gh_store.core.store import GitHubStore, DEFAULT_CONFIG_PATH

def test_store_uses_default_config_when_no_path_provided(mock_github, mock_config_file):
    """Test that store uses packaged default config when no config exists"""
    _, mock_repo = mock_github
    
    # Mock the default config path to not exist
    with patch('pathlib.Path.exists', return_value=False):
        store = GitHubStore(token="fake-token", repo="owner/repo")
        
        # Updated assertions to match fixture config
        assert store.config.store.base_label == "stored-object"
        assert store.config.store.reactions.processed == "+1"

def test_store_uses_provided_config_path(mock_github, tmp_path):
    """Test that store uses provided config path when it exists"""
    _, mock_repo = mock_github
    
    # Create a test config file
    config_path = tmp_path / "test_config.yml"
    test_config = {
        "store": {
            "base_label": "stored-object",  # Matches fixture
            "uid_prefix": "UID:",
            "reactions": {
                "processed": "+1",  # Matches fixture
                "initial_state": "rocket"  # Matches fixture
            }
        }
    }
    config_path.write_text(yaml.dump(test_config))
    
    store = GitHubStore(token="fake-token", repo="owner/repo", config_path=config_path)
    
    assert store.config.store.base_label == "stored-object"
    assert store.config.store.reactions.processed == "+1"

def test_store_raises_error_for_nonexistent_custom_config(mock_github):
    """Test that store raises error when custom config path doesn't exist"""
    _, mock_repo = mock_github
    
    with pytest.raises(FileNotFoundError):
        GitHubStore(
            token="fake-token",
            repo="owner/repo",
            config_path=Path("/nonexistent/config.yml")
        )

def test_default_config_path_is_in_user_config_dir():
    """Test that default config path is in user's config directory"""
    expected_path = Path.home() / ".config" / "gh-store" / "config.yml"
    assert DEFAULT_CONFIG_PATH == expected_path



---
File: tests/unit/test_object_history.py
---
# tests/unit/test_object_history.py

from datetime import datetime, timezone
import json
import pytest
from unittest.mock import Mock

from gh_store.core.exceptions import ObjectNotFound

@pytest.fixture
def history_mock_comments(mock_comment):
    """Create series of comments representing object history"""
    comments = []
    
    # Initial state
    comments.append(mock_comment(
        user_login="repo-owner",
        body={
            "type": "initial_state",
            "data": {"name": "test", "value": 42},
            "timestamp": "2025-01-01T00:00:00Z"
        },
        comment_id=1,
        created_at=datetime(2025, 1, 1, tzinfo=timezone.utc)
    ))
    
    # First update
    comments.append(mock_comment(
        user_login="repo-owner",
        body={
            "_data": {"value": 43},
            "_meta": {
                "client_version": "0.5.1",
                "timestamp": "2025-01-02T00:00:00Z",
                "update_mode": "append"
            }
        },
        comment_id=2,
        created_at=datetime(2025, 1, 2, tzinfo=timezone.utc)
    ))
    
    # Second update
    comments.append(mock_comment(
        user_login="repo-owner",
        body={
            "_data": {"value": 44},
            "_meta": {
                "client_version": "0.5.1",
                "timestamp": "2025-01-03T00:00:00Z",
                "update_mode": "append"
            }
        },
        comment_id=3,
        created_at=datetime(2025, 1, 3, tzinfo=timezone.utc)
    ))
    
    return comments

def test_get_object_history_initial_state(store, mock_issue, history_mock_comments):
    """Test that initial state is correctly extracted from history"""
    issue = mock_issue(
        number=1,
        comments=history_mock_comments
    )
    store.repo.get_issues.return_value = [issue]
    
    history = store.issue_handler.get_object_history("test-123")
    
    initial_state = history[0]
    assert initial_state["type"] == "initial_state"
    assert initial_state["data"] == {"name": "test", "value": 42}
    assert initial_state["comment_id"] == 1

def test_get_object_history_updates_sequence(store, mock_issue, history_mock_comments):
    """Test that updates are returned in correct chronological order"""
    issue = mock_issue(
        number=1,
        comments=history_mock_comments
    )
    store.repo.get_issues.return_value = [issue]
    
    history = store.issue_handler.get_object_history("test-123")
    
    # Verify updates sequence
    assert len(history) == 3
    assert [entry["data"].get("value") for entry in history if "value" in entry["data"]] == [42, 43, 44]
    assert [entry["comment_id"] for entry in history] == [1, 2, 3]

def test_get_object_history_metadata_handling(store, mock_issue, history_mock_comments):
    """Test that metadata is correctly preserved in history"""
    issue = mock_issue(
        number=1,
        comments=history_mock_comments
    )
    store.repo.get_issues.return_value = [issue]
    
    history = store.issue_handler.get_object_history("test-123")
    
    # Check metadata for updates
    update = history[1]
    assert "metadata" in update
    assert update["metadata"]["client_version"] == "0.5.1"
    assert update["metadata"]["update_mode"] == "append"

def test_get_object_history_legacy_format(store, mock_issue, mock_comment):
    """Test handling of legacy format comments in history"""
    legacy_comment = mock_comment(
        user_login="repo-owner",
        body={"value": 43},  # Legacy format without metadata
        comment_id=1,
        created_at=datetime(2025, 1, 1, tzinfo=timezone.utc)
    )
    
    issue = mock_issue(
        number=1,
        comments=[legacy_comment]
    )
    store.repo.get_issues.return_value = [issue]
    
    history = store.issue_handler.get_object_history("test-123")
    
    assert len(history) == 1
    assert history[0]["type"] == "update"
    assert history[0]["data"] == {"value": 43}
    assert history[0]["metadata"]["client_version"] == "legacy"

def test_comment_history_json_handling(store, mock_issue, mock_comment):
    """Test processing of valid JSON comments in history"""
    comments = [
        # First comment
        mock_comment(
            user_login="repo-owner",
            body={"value": 42},
            comment_id=1
        ),
        # Second comment
        mock_comment(
            user_login="repo-owner",
            body={"value": 43},
            comment_id=2
        ),
        # Third comment
        mock_comment(
            user_login="repo-owner",
            body={"value": 44},
            comment_id=3
        )
    ]
    
    issue = mock_issue(
        number=1,
        comments=comments
    )
    store.repo.get_issues.return_value = [issue]
    
    history = store.issue_handler.get_object_history("test-123")
    
    # Verify all valid comments are processed
    assert len(history) == 3
    assert [entry["data"]["value"] for entry in history] == [42, 43, 44]
    assert [entry["comment_id"] for entry in history] == [1, 2, 3]

def test_get_object_history_nonexistent(store):
    """Test retrieving history for nonexistent object"""
    store.repo.get_issues.return_value = []
    
    with pytest.raises(ObjectNotFound):
        store.issue_handler.get_object_history("nonexistent")



---
File: tests/unit/test_security.py
---
# tests/unit/test_security.py

import json
from datetime import datetime, timezone
import pytest
from unittest.mock import Mock, patch
from github import GithubException

from gh_store.core.access import AccessControl
from gh_store.core.exceptions import AccessDeniedError
from gh_store.core.version import CLIENT_VERSION

# Authorization Tests

def test_owner_always_authorized(mock_github):
    """Test that repository owner is always authorized regardless of CODEOWNERS"""
    _, mock_repo = mock_github
    
    # Override get_contents to return no CODEOWNERS
    mock_repo.get_contents = Mock(side_effect=GithubException(404, "Not found"))
    
    ac = AccessControl(mock_repo)
    assert ac._is_authorized("repo-owner") is True

def test_codeowners_authorization(mock_github):
    """Test authorization via CODEOWNERS file"""
    _, mock_repo = mock_github
    
    # Override CODEOWNERS content
    mock_content = Mock()
    mock_content.decoded_content = b"* @maintainer @contributor"
    mock_repo.get_contents = Mock(return_value=mock_content)
    
    ac = AccessControl(mock_repo)
    assert ac._is_authorized("maintainer") is True
    assert ac._is_authorized("contributor") is True
    assert ac._is_authorized("random-user") is False

def test_organization_ownership(mock_github):
    """Test authorization with organization ownership"""
    _, mock_repo = mock_github
    
    # Override owner type
    owner = Mock()
    owner.login = "org-name"
    owner.type = "Organization"
    mock_repo.owner = owner
    
    ac = AccessControl(mock_repo)
    owner_info = ac._get_owner_info()
    
    assert owner_info["login"] == "org-name"
    assert owner_info["type"] == "Organization"
    assert ac._is_authorized("org-name") is True

def test_codeowners_file_locations(mock_github):
    """Test CODEOWNERS file location precedence"""
    _, mock_repo = mock_github
    
    for test_path in ['.github/CODEOWNERS', 'docs/CODEOWNERS', 'CODEOWNERS']:
        mock_content = Mock()
        mock_content.decoded_content = b"* @authorized-user"
        
        def get_contents_side_effect(path):
            if path == test_path:
                return mock_content
            raise GithubException(404, "Not found")
        
        mock_repo.get_contents = Mock(side_effect=get_contents_side_effect)
        ac = AccessControl(mock_repo)
        
        assert ac._is_authorized("authorized-user") is True

def test_unauthorized_update_rejection(store, mock_comment):
    """Test that updates from unauthorized users are rejected"""
    # Create unauthorized and authorized updates
    unauthorized_update = mock_comment(
        user_login="attacker",
        body={
            '_data': {'malicious': 'update'},
            '_meta': {
                'client_version': CLIENT_VERSION,
                'timestamp': '2025-01-01T00:00:00Z',
                'update_mode': 'append'
            }
        }
    )
    authorized_update = mock_comment(
        user_login="repo-owner",
        body={
            '_data': {'valid': 'update'},
            '_meta': {
                'client_version': CLIENT_VERSION,
                'timestamp': '2025-01-01T00:00:00Z',
                'update_mode': 'append'
            }
        }
    )
    
    # Setup mock issue
    issue = Mock()
    issue.get_comments = Mock(return_value=[unauthorized_update, authorized_update])
    issue.user = Mock(login="repo-owner")  # Authorized creator
    
    # Setup repo mock to return list of issues
    store.repo.get_issues = Mock(return_value=[issue])
    store.repo.get_issue = Mock(return_value=issue)
    
    # Get updates
    updates = store.comment_handler.get_unprocessed_updates(123)
    assert len(updates) == 1
    assert updates[0].changes == {'valid': 'update'}

def test_unauthorized_issue_creator_denied(store, mock_issue):
    """Test that updates are blocked for issues created by unauthorized users"""
    issue = mock_issue(
        user_login="infiltrator"
    )
    store.repo.get_issue.return_value = issue
    
    with pytest.raises(AccessDeniedError):
        store.process_updates(456)

def test_authorized_codeowners_updates(authorized_store, mock_comment):
    """Test that CODEOWNERS team members can make updates"""
    store = authorized_store(['repo-owner', 'team-member'])
    
    # Create update from team member
    team_update = mock_comment(
        user_login="team-member",
        body={
            '_data': {'team': 'update'},
            '_meta': {
                'client_version': CLIENT_VERSION,
                'timestamp': '2025-01-01T00:00:00Z',
                'update_mode': 'append'
            }
        }
    )
    
    # Setup mock issue
    issue = Mock()
    issue.get_comments = Mock(return_value=[team_update])
    issue.user = Mock(login="repo-owner")  # Authorized creator
    
    # Setup repo mock to return list of issues
    store.repo.get_issues = Mock(return_value=[issue])
    store.repo.get_issue = Mock(return_value=issue)
    
    # Get updates
    updates = store.comment_handler.get_unprocessed_updates(123)
    assert len(updates) == 1
    assert updates[0].changes == {'team': 'update'}

def test_metadata_tampering_protection(store, mock_comment):
    """Test protection against metadata tampering in updates"""
    # Create update with invalid metadata
    tampered_update = mock_comment(
        user_login="repo-owner",  # Even authorized users can't use invalid metadata
        body={
            '_data': {'update': 'data'},
            '_meta': {
                'client_version': CLIENT_VERSION
                # Missing required fields
            }
        }
    )
    
    # Setup mock issue
    issue = Mock()
    issue.get_comments = Mock(return_value=[tampered_update])
    issue.user = Mock(login="repo-owner")
    
    # Setup repo mock to return list of issues
    store.repo.get_issues = Mock(return_value=[issue])
    store.repo.get_issue = Mock(return_value=issue)
    
    # Get updates - should be empty due to invalid metadata
    updates = store.comment_handler.get_unprocessed_updates(123)
    assert len(updates) == 0

def test_reaction_based_processing_protection(store, mock_comment):
    """Test that processed updates cannot be reprocessed"""
    # Create a processed update with the processed reaction
    processed_update = mock_comment(
        user_login="repo-owner",
        body={
            '_data': {'already': 'processed'},
            '_meta': {
                'client_version': CLIENT_VERSION,
                'timestamp': '2025-01-01T00:00:00Z',
                'update_mode': 'append'
            }
        },
        reactions=[Mock(content="+1")]  # Add processed reaction
    )
    
    # Setup mock issue
    issue = Mock()
    issue.get_comments = Mock(return_value=[processed_update])
    issue.user = Mock(login="repo-owner")
    
    # Setup repo mock to return list of issues
    store.repo.get_issues = Mock(return_value=[issue])
    store.repo.get_issue = Mock(return_value=issue)
    
    # Get updates - should be empty since update is already processed
    updates = store.comment_handler.get_unprocessed_updates(123)
    assert len(updates) == 0



---
File: tests/unit/test_store_basic_ops.py
---
# tests/unit/test_store_basic_ops.py

import json
from datetime import datetime, timezone
import pytest
from unittest.mock import Mock

from gh_store.core.constants import LabelNames
from gh_store.core.exceptions import ObjectNotFound


def test_create_object_with_initial_state(store, mock_label_factory, mock_comment_factory, mock_issue_factory):
    """Test that creating an object stores the initial state in a comment"""
    object_id = "test-123"
    test_data = {"name": "test", "value": 42}
    issue_number = 456  # Define issue number
    labels=[
        mock_label_factory(name=LabelNames.GH_STORE),
        mock_label_factory(name=LabelNames.STORED_OBJECT),
    ]
    
    # Mock existing labels
    store.repo.get_labels.return_value = labels

    # .... I think this test might be mocked to the point of being useless.
    # Create a properly configured mock issue
    mock_issue = mock_issue_factory(
        number=issue_number,
        body=json.dumps(test_data),
        labels=labels+[f"{LabelNames.UID_PREFIX}{object_id}"],
    )
    
    # Set up the repo mock to return our issue when create_issue is called
    store.repo.create_issue.return_value = mock_issue
    
    # Make the create_comment method return a properly configured comment
    initial_comment = mock_comment_factory(
        comment_id=1,
        body={
            "type": "initial_state",
            "_data": test_data,
            "_meta": {
                "client_version": "1.2.3",
                "timestamp": datetime.now(timezone.utc).isoformat(),
                "update_mode": "append",
                "issue_number": issue_number
            }
        }
    )
    mock_issue.create_comment.return_value = initial_comment
    
    # Execute the method under test
    obj = store.create(object_id, test_data)
    
    # Verify issue creation
    store.repo.create_issue.assert_called_once()
    
    # Verify create_issue was called with the right arguments
    create_issue_args = store.repo.create_issue.call_args[1]
    assert create_issue_args["title"] == f"Stored Object: {object_id}"
    assert json.loads(create_issue_args["body"]) == test_data
    assert LabelNames.GH_STORE in create_issue_args["labels"]
    assert LabelNames.STORED_OBJECT in create_issue_args["labels"]
    assert f"{LabelNames.UID_PREFIX}{object_id}" in create_issue_args["labels"]
    
    # Verify initial state comment was created
    mock_issue.create_comment.assert_called_once()
    
    # Verify object metadata
    assert obj.meta.object_id == object_id
    assert obj.meta.issue_number == issue_number
    assert obj.data == test_data


def test_get_object(store):
    """Test retrieving an object"""
    test_data = {"name": "test", "value": 42}
    issue_number = 42  # Define issue number
    
    # Mock labels - should include both stored-object and gh-store
    stored_label = Mock()
    stored_label.name = "stored-object"
    gh_store_label = Mock()
    gh_store_label.name = LabelNames.GH_STORE
    uid_label = Mock()
    uid_label.name = "UID:test-obj"
    
    store.repo.get_labels.return_value = [stored_label, gh_store_label, uid_label]
    
    mock_issue = Mock()
    mock_issue.number = issue_number  # Set issue number
    mock_issue.body = json.dumps(test_data)
    mock_issue.get_comments = Mock(return_value=[])
    mock_issue.created_at = datetime.now(timezone.utc)
    mock_issue.updated_at = datetime.now(timezone.utc)
    mock_issue.labels = [stored_label, gh_store_label, uid_label]
    store.repo.get_issues.return_value = [mock_issue]
    
    obj = store.get("test-obj")
    assert obj.data == test_data
    assert obj.meta.issue_number == issue_number  # Verify issue_number in metadata
    
    # Verify correct query was made (now checking for all three labels)
    store.repo.get_issues.assert_called_with(
        labels=[LabelNames.GH_STORE, "stored-object", "UID:test-obj"],
        state="closed"
    )

def test_get_nonexistent_object(store):
    """Test getting an object that doesn't exist"""
    store.repo.get_issues.return_value = []
    
    with pytest.raises(ObjectNotFound):
        store.get("nonexistent")

def test_create_object_ensures_labels_exist(store, mock_issue_factory, mock_label_factory):
    pass



---
File: tests/unit/test_store_list_ops.py
---
# tests/unit/test_store_list_ops.py

from datetime import datetime, timedelta
from zoneinfo import ZoneInfo
import pytest
from unittest.mock import Mock

from gh_store.core.constants import LabelNames


def test_list_updated_since(store, mock_issue_factory):
    """Test fetching objects updated since timestamp"""
    timestamp = datetime.now(ZoneInfo("UTC")) - timedelta(hours=1)
    object_id = "test-123"
    
    # Create mock issue updated after timestamp - include gh-store label
    issue = mock_issue_factory(
        labels=[LabelNames.GH_STORE, LabelNames.STORED_OBJECT, f"{LabelNames.UID_PREFIX}{object_id}"],
        updated_at = timestamp + timedelta(minutes=30),
        created_at = timestamp - timedelta(minutes=30),
    )
    store.repo.get_issues.return_value = [issue]
    
    # Mock object retrieval
    # mock_obj = Mock()
    # mock_obj.meta.updated_at = timestamp + timedelta(minutes=30)
    # store.issue_handler.get_object_by_number = Mock(return_value=mock_obj)
    
    # Test listing
    updated = list(store.list_updated_since(timestamp))
    
    # Verify
    store.repo.get_issues.assert_called_once()
    call_kwargs = store.repo.get_issues.call_args[1]
    assert call_kwargs["since"] == timestamp
    assert call_kwargs["labels"] == [LabelNames.GH_STORE, LabelNames.STORED_OBJECT]  # Query by stored-object for active objects
    assert len(updated) == 1
    assert updated[0].meta.object_id == object_id

def test_list_updated_since_no_updates(store, mock_issue):
    """Test when no updates since timestamp"""
    timestamp = datetime.now(ZoneInfo("UTC")) - timedelta(hours=1)
    
    # Create mock issue updated before timestamp
    issue = mock_issue(
        created_at=timestamp - timedelta(minutes=30),
        updated_at=timestamp - timedelta(minutes=30),
        labels=[str(LabelNames.GH_STORE), "stored-object", f"UID:foo"],
    )
    store.repo.get_issues.return_value = [issue]
    
    # Mock object retrieval
    mock_obj = Mock()
    mock_obj.meta.updated_at = timestamp - timedelta(minutes=30)
    store.issue_handler.get_object_by_number = Mock(return_value=mock_obj)
    
    # Test listing
    updated = list(store.list_updated_since(timestamp))
    
    # Verify no updates found
    assert len(updated) == 0
# Updates needed for test_store_list_ops.py

def test_list_all_objects(store, mock_issue, mock_label_factory):
    """Test listing all objects in store"""
    # Create mock issues with proper labels - include gh-store label
    issues = [
        mock_issue(
            number=1,
            labels=["gh-store", "stored-object", f"UID:test-1"],
        ),
        mock_issue(
            number=2,
            labels=["gh-store", "stored-object", f"UID:test-2"],
        )
    ]
    store.repo.get_issues.return_value = issues
    
    # Mock object retrieval
    def get_object_by_number(number):
        mock_obj = Mock()
        mock_obj.meta.object_id = f"test-{number}"
        return mock_obj
    
    store.issue_handler.get_object_by_number = Mock(
        side_effect=get_object_by_number
    )
    
    # Test listing all
    objects = [obj.meta.object_id for obj in list(store.list_all())]
    
    # Verify
    assert len(objects) == 2
    assert "test-1" in objects
    assert "test-2" in objects
    
    # Verify the query was made with stored-object label
    store.repo.get_issues.assert_called_with(
        state="closed",
        labels=["gh-store", "stored-object"]
    )

def test_list_all_skips_archived(store, mock_issue, mock_label_factory):
    """Test that archived objects are skipped in listing"""
    # Create archived and active issues - include gh-store label
    archived_issue = mock_issue(
        number=1,
        labels=[
            "gh-store",
            "stored-object",
            "UID:test-1",
            "archived",
        ]
    )
    active_issue = mock_issue(
        number=2,
        labels=["gh-store", "stored-object", "UID:test-2"]
    )
    
    store.repo.get_issues.return_value = [archived_issue, active_issue]
    
    # Mock object retrieval
    def get_object_by_number(number):
        mock_obj = Mock()
        mock_obj.meta.object_id = f"test-{number}"
        return mock_obj
    
    store.issue_handler.get_object_by_number = Mock(
        side_effect=get_object_by_number
    )
    
    # Test listing
    objects = [obj.meta.object_id for obj in list(store.list_all())]
    
    # Verify only active object listed
    # 
    assert len(objects) == 1
    assert "test-2" in objects
    assert "test-1" not in objects

def test_list_all_handles_invalid_labels(store, mock_issue, mock_label_factory):
    """Test handling of issues with invalid label structure"""
    # Create issue missing UID label
    invalid_issue = mock_issue(
        number=1,
        labels=["gh-store", "stored-object"]  # Missing UID label
    )
    
    # Create valid issue with explicit labels including UID
    valid_issue = mock_issue(
        number=2,
        labels=["gh-store", "stored-object","UID:test-2"]  # Explicitly set UID label
    )
    
    store.repo.get_issues.return_value = [invalid_issue, valid_issue]
    
    # Mock object retrieval
    def get_object_by_number(number):
        mock_obj = Mock()
        mock_obj.meta.object_id = f"test-{number}"
        mock_obj.meta.label = f"UID:test-{number}"
        return mock_obj
    
    store.issue_handler.get_object_by_number = Mock(
        side_effect=get_object_by_number
    )
    
    # Test listing
    objects = [obj.meta.object_id for obj in list(store.list_all())]
    
    # Verify only valid object listed
    assert len(objects) == 1
    assert "test-2" in objects



---
File: tests/unit/test_store_update_ops.py
---
# tests/unit/test_store_update_ops.py

import json
from datetime import datetime, timezone
import pytest
from unittest.mock import Mock

from gh_store.core.constants import LabelNames
from gh_store.core.exceptions import ConcurrentUpdateError, ObjectNotFound
from gh_store.core.version import CLIENT_VERSION

def test_process_update(store, mock_issue_factory):
    """Test processing an update"""
    test_data = {"name": "test", "value": 42}
    mock_issue = mock_issue_factory(body=test_data, number=123, labels=[LabelNames.GH_STORE, LabelNames.STORED_OBJECT, f"{LabelNames.UID_PREFIX}:test-obj"])
    
    def get_issues_side_effect(**kwargs):
        if kwargs.get("state") == "open":
            return []  # No issues being processed
        return [mock_issue]
    
    store.repo.get_issues.side_effect = get_issues_side_effect
    store.repo.get_issue.return_value = mock_issue
    
    # Test update
    update_data = {"value": 43}
    store.update("test-obj", update_data)
    
    # Verify update comment
    mock_issue.create_comment.assert_called_once()
    comment_data = json.loads(mock_issue.create_comment.call_args[0][0])
    assert comment_data["_data"] == update_data
    assert "_meta" in comment_data
    assert all(key in comment_data["_meta"] for key in ["client_version", "timestamp", "update_mode"])
    
    # Verify issue reopened
    mock_issue.edit.assert_called_with(state="open")

def test_concurrent_update_prevention(store, mock_issue_factory, mock_comment_factory):
    """Test that concurrent updates are prevented"""
    def open_issue_with_n_comments(n):
        return mock_issue_factory(
            state="open",
            comments=[
                mock_comment_factory(
                    body={"value": 42},
                    comment_id=i
                ) for i in range(n)]
        )

    def set_store_with_issue_n_comments(n):
        mock_issue = open_issue_with_n_comments(n)
        def get_issues_side_effect(**kwargs):
            if kwargs.get("state") == "open":
                return [mock_issue]  # Return open issue to simulate processing
            return []
        
        store.repo.get_issues.side_effect = get_issues_side_effect

    set_store_with_issue_n_comments(1)
    store.update("test-obj", {"value": 43})
    set_store_with_issue_n_comments(2)
    store.update("test-obj", {"value": 43})
    set_store_with_issue_n_comments(3)
    with pytest.raises(ConcurrentUpdateError):
        store.update("test-obj", {"value": 43})

def test_update_metadata_structure(store, mock_issue_factory):
    """Test that updates include properly structured metadata"""
    # mock_issue = Mock()
    # mock_issue.body = json.dumps({"initial": "data"})
    # mock_issue.get_comments = Mock(return_value=[])
    # mock_issue.number = 123
    # mock_issue.user = Mock()
    # mock_issue.user.login = "repo-owner"  # Set authorized user
    mock_issue = mock_issue_factory(
        number = 123,
        body={"initial": "data"}, 
        labels=[LabelNames.GH_STORE, LabelNames.STORED_OBJECT, f"{LabelNames.UID_PREFIX}test-obj"]
    )
    
    def get_issues_side_effect(**kwargs):
        if kwargs.get("state") == "open":
            return []  # No concurrent processing
        return [mock_issue]
    
    store.repo.get_issues.side_effect = get_issues_side_effect
    store.repo.get_issue.return_value = mock_issue
    
    update_data = {"new": "value"}
    store.update("test-obj", update_data)
    
    # Verify comment structure
    mock_issue.create_comment.assert_called_once()
    comment_data = json.loads(mock_issue.create_comment.call_args[0][0])
    
    assert "_data" in comment_data
    assert "_meta" in comment_data
    assert comment_data["_meta"]["client_version"] == CLIENT_VERSION
    assert comment_data["_meta"]["update_mode"] == "append"
    assert "timestamp" in comment_data["_meta"]


def test_update_nonexistent_object(store):
    """Test updating an object that doesn't exist"""
    store.repo.get_issues.return_value = []
    
    with pytest.raises(ObjectNotFound):
        store.update("nonexistent", {"value": 43})



---
File: tests/unit/test_types.py
---
# tests/unit/test_types.py

import json
from datetime import datetime, timezone
import pytest
from unittest.mock import Mock

from gh_store.core.constants import LabelNames
from gh_store.core.types import StoredObject, get_object_id_from_labels

class TestStoredObject:
    """Tests for StoredObject class."""
    
    def test_from_issue(self, mock_issue_factory, mock_label_factory):
        """Test correctly creating a StoredObject from an issue."""
        # Create test data
        object_id = "test-123"
        issue_number = 42
        created_at = datetime(2025, 1, 1, tzinfo=timezone.utc)
        updated_at = datetime(2025, 1, 2, tzinfo=timezone.utc)
        data = {"name": "test", "value": 42}
        
        # Create a properly labeled mock issue
        issue = mock_issue_factory(
            number=issue_number,
            body=json.dumps(data),
            labels=[
                "gh-store",
                "stored-object",
                f"{LabelNames.UID_PREFIX}{object_id}"
            ],
            created_at=created_at,
            updated_at=updated_at
        )
        
        # Create StoredObject from the issue
        obj = StoredObject.from_issue(issue)
        
        # Verify metadata
        assert obj.meta.object_id == object_id
        assert obj.meta.label == object_id
        assert obj.meta.issue_number == issue_number
        assert obj.meta.created_at == created_at
        assert obj.meta.updated_at == updated_at
        assert obj.meta.version == 1
        
        # Verify data
        assert obj.data == data
        
    def test_from_issue_with_explicit_version(self, mock_issue_factory):
        """Test creating a StoredObject with explicit version number."""
        # Create test data
        object_id = "test-123"
        data = {"name": "test", "value": 42}
        version = 5
        
        # Create a properly labeled mock issue
        issue = mock_issue_factory(
            body=json.dumps(data),
            labels=[
                "gh-store",
                "stored-object",
                f"{LabelNames.UID_PREFIX}{object_id}"
            ]
        )
        
        # Create StoredObject with explicit version
        obj = StoredObject.from_issue(issue, version=version)
        
        # Verify version
        assert obj.meta.version == version
    
    def test_from_issue_missing_uid_label(self, mock_issue_factory):
        """Test that creating a StoredObject fails when UID label is missing."""
        # Create an issue missing the UID label
        issue = mock_issue_factory(
            body=json.dumps({"name": "test"}),
            labels=["gh-store", "stored-object"]  # No UID label
        )
        
        # Should raise ValueError when UID label is missing
        with pytest.raises(ValueError, match="No UID label found"):
            StoredObject.from_issue(issue)
    
    def test_from_issue_invalid_body(self, mock_issue_factory):
        """Test that creating a StoredObject fails with invalid JSON body."""
        # Create an issue with invalid JSON in body
        issue = mock_issue_factory(
            body="not valid json",
            labels=[
                "gh-store",
                "stored-object",
                f"{LabelNames.UID_PREFIX}test-123"
            ]
        )
        
        # Should raise JSON decode error
        with pytest.raises(json.JSONDecodeError):
            StoredObject.from_issue(issue)

class TestObjectIDFromLabels:
    """Tests for get_object_id_from_labels function."""
    
    def test_get_object_id_from_labels(self, mock_label_factory):
        """Test extracting object ID from issue labels."""
        # Create an issue with mock labels
        object_id = "test-123"
        issue = Mock()
        issue.labels = [
            mock_label_factory(name="stored-object"),
            mock_label_factory(name=f"{LabelNames.UID_PREFIX}{object_id}"),
            mock_label_factory(name="other-label")
        ]
        
        # Extract object ID
        extracted_id = get_object_id_from_labels(issue)
        
        # Verify extracted ID
        assert extracted_id == object_id
    
    def test_get_object_id_from_labels_no_match(self, mock_label_factory):
        """Test that ValueError is raised when no UID label exists."""
        # Create an issue with no UID label
        issue = Mock()
        issue.labels = [
            mock_label_factory(name="stored-object"),
            mock_label_factory(name="other-label")
        ]
        
        # Should raise ValueError
        with pytest.raises(ValueError):
            get_object_id_from_labels(issue)



---
File: typescript/.eslintrc.json
---
{
  "root": true,
  "parser": "@typescript-eslint/parser",
  "plugins": ["@typescript-eslint"],
  "extends": [
    "eslint:recommended",
    "plugin:@typescript-eslint/recommended",
    "prettier"
  ],
  "env": {
    "browser": true,
    "es2020": true,
    "node": true
  },
  "rules": {
    "@typescript-eslint/explicit-module-boundary-types": "error",
    "@typescript-eslint/no-explicit-any": "error",
    "@typescript-eslint/no-unused-vars": ["error", { "argsIgnorePattern": "^_" }],
    "no-console": ["error", { "allow": ["warn", "error"] }]
  }
}



---
File: typescript/README.md
---




---
File: typescript/jest.config.ts
---
// typescript/jest.config.ts
import type { JestConfigWithTsJest } from 'ts-jest';

const config: JestConfigWithTsJest = {
  preset: 'ts-jest',
  testEnvironment: 'jsdom',
  setupFiles: ['./jest.setup.ts'],
  testMatch: ['<rootDir>/src/**/*.test.ts'],
  collectCoverage: true,
  coverageDirectory: 'coverage',
  coverageReporters: ['text', 'lcov'],
  coverageThreshold: {
    global: {
      branches: 70,    // Lowered from 80%
      functions: 80,
      lines: 80,
      statements: 80
    }
  },
  transform: {
    '^.+\\.tsx?$': ['ts-jest', {
      useESM: true,
    }]
  },
  extensionsToTreatAsEsm: ['.ts']
};

export default config;



---
File: typescript/jest.setup.ts
---
// typescript/jest.setup.ts
import fetchMock from 'jest-fetch-mock';
fetchMock.enableMocks();



---
File: typescript/package.json
---
{
  "name": "gh-store-client",
  "version": "0.10.1",
  "description": "TypeScript client for GitHub Issue Store",
  "type": "module",
  "exports": {
    ".": {
      "import": "./dist/index.mjs",
      "require": "./dist/index.cjs",
      "types": "./dist/index.d.ts"
    }
  },
  "main": "./dist/index.cjs",
  "module": "./dist/index.mjs",
  "types": "./dist/index.d.ts",
  "files": [
    "dist",
    "README.md"
  ],
  "scripts": {
    "build": "tsup src/index.ts --format esm,cjs --dts",
    "format": "prettier --write \"src/**/*.ts\"",
    "lint": "eslint src --ext .ts",
    "prebuild": "tsx scripts/update-version.ts",
    "test": "node --experimental-vm-modules node_modules/jest/bin/jest.js",
    "test:packaging": "node scripts/test-packaging.js",
    "type-check": "tsc --noEmit",
    "prepublishOnly": "npm run build && npm run test:packaging"
  },
  "engines": {
    "node": ">=18"
  },
  "keywords": [
    "github",
    "storage",
    "typescript",
    "client"
  ],
  "repository": {
    "type": "git",
    "url": "git+https://github.com/dmarx/gh-store.git",
    "directory": "typescript"
  },
  "author": "David Marx <david.marx84@gmail.com>",
  "license": "MIT",
  "devDependencies": {
    "@types/jest": "^29.5.11",
    "@types/node": "^20.10.6",
    "@typescript-eslint/eslint-plugin": "^6.17.0",
    "@typescript-eslint/parser": "^6.17.0",
    "eslint": "^8.56.0",
    "eslint-config-prettier": "^9.1.0",
    "jest": "^29.7.0",
    "jest-environment-jsdom": "^29.7.0",
    "jest-fetch-mock": "^3.0.3",
    "prettier": "^3.1.1",
    "ts-jest": "^29.1.1",
    "ts-node": "^10.9.2",
    "tsup": "^8.0.1",
    "tsx": "^4.7.0",
    "typescript": "^5.3.3"
  },
  "peerDependencies": {
    "@types/chrome": ">=0.0.246"
  },
  "publishConfig": {
    "access": "public",
    "registry": "https://registry.npmjs.org/"
  }
}



---
File: typescript/scripts/test-packaging.js
---
// typescript/scripts/test-packaging.js
import { readFileSync, existsSync } from 'fs';
import { fileURLToPath } from 'url';
import { dirname, resolve } from 'path';

const __dirname = dirname(fileURLToPath(import.meta.url));
const rootDir = resolve(__dirname, '..');

// Verify dist directory exists
function checkDist() {
    const distDir = resolve(rootDir, 'dist');
    if (!existsSync(distDir)) {
        console.error('✗ dist directory not found - run build first');
        process.exit(1);
    }

    const requiredFiles = ['index.mjs', 'index.cjs', 'index.d.ts'];
    for (const file of requiredFiles) {
        if (!existsSync(resolve(distDir, file))) {
            console.error(`✗ Missing required build output: ${file}`);
            process.exit(1);
        }
    }
    console.log('✓ dist directory contains required files');
}

// Test ESM import
async function testESMImport() {
    try {
        const { GitHubStoreClient } = await import('../dist/index.mjs');
        if (!GitHubStoreClient) {
            throw new Error('GitHubStoreClient not exported from ESM build');
        }
        console.log('✓ ESM import successful');
    } catch (error) {
        console.error('✗ ESM import failed:', error);
        process.exit(1);
    }
}

// Test package.json exports
function testPackageExports() {
    const pkg = JSON.parse(readFileSync(resolve(rootDir, 'package.json'), 'utf8'));
    
    // Check required fields
    const requiredFields = ['exports', 'main', 'module', 'types'];
    for (const field of requiredFields) {
        if (!pkg[field]) {
            console.error(`✗ Missing required field: ${field}`);
            process.exit(1);
        }
    }
    
    // Check exports configuration
    const { exports } = pkg;
    if (!exports['.'].import || !exports['.'].require || !exports['.'].types) {
        console.error('✗ Exports must specify import, require, and types');
        process.exit(1);
    }

    // Verify paths in exports match files that should exist
    const paths = [
        exports['.'].import,
        exports['.'].require,
        exports['.'].types
    ].map(p => p.replace(/^\.\//, ''));

    for (const path of paths) {
        if (!existsSync(resolve(rootDir, path))) {
            console.error(`✗ Export path does not exist: ${path}`);
            process.exit(1);
        }
    }
    
    console.log('✓ package.json exports verified');
}

// Run tests
async function main() {
    console.log('Testing package configuration...');
    // First verify the build files exist
    checkDist();
    // Then test the configuration and imports
    testPackageExports();
    await testESMImport();
    console.log('All packaging tests passed!');
}

main().catch(error => {
    console.error('Test failed:', error);
    process.exit(1);
});



---
File: typescript/scripts/update-version.ts
---
// typescript/scripts/update-version.ts
import { readFileSync, writeFileSync } from 'fs';
import { resolve, dirname } from 'path';
import { fileURLToPath } from 'url';

const __filename = fileURLToPath(import.meta.url);
const __dirname = dirname(__filename);

// Read package.json
const pkgPath = resolve(__dirname, '../package.json');
const pkg = JSON.parse(readFileSync(pkgPath, 'utf8'));

// Read version.ts
const versionPath = resolve(__dirname, '../src/version.ts');
const versionContent = readFileSync(versionPath, 'utf8');

// Update version
const updatedContent = versionContent.replace(
  /export const CLIENT_VERSION = '.*'/,
  `export const CLIENT_VERSION = '${pkg.version}'`
);

// Write back
writeFileSync(versionPath, updatedContent);

console.log(`Updated CLIENT_VERSION to ${pkg.version}`);



---
File: typescript/src/__tests__/cache.test.ts
---
// src/__tests__/cache.test.ts
import { jest, describe, it, expect, beforeEach } from '@jest/globals';
import { IssueCache } from '../cache';

describe('IssueCache', () => {
  let cache: IssueCache;
  
  beforeEach(() => {
    cache = new IssueCache();
  });

  it('should store and retrieve issue numbers', () => {
    const metadata = {
      createdAt: new Date(),
      updatedAt: new Date()
    };
    
    cache.set('test-1', 123, metadata);
    expect(cache.get('test-1')).toBe(123);
  });

  it('should respect maxSize limit', () => {
    const cache = new IssueCache({ maxSize: 2 });
    const metadata = {
      createdAt: new Date(),
      updatedAt: new Date()
    };

    cache.set('test-1', 123, metadata);
    cache.set('test-2', 456, metadata);
    cache.set('test-3', 789, metadata); // Should evict oldest

    expect(cache.get('test-1')).toBeUndefined();
    expect(cache.get('test-2')).toBe(456);
    expect(cache.get('test-3')).toBe(789);
  });

  it('should respect TTL', () => {
    jest.useFakeTimers();
    const cache = new IssueCache({ ttl: 1000 }); // 1 second TTL
    const metadata = {
      createdAt: new Date(),
      updatedAt: new Date()
    };

    cache.set('test-1', 123, metadata);
    expect(cache.get('test-1')).toBe(123);

    // Advance time past TTL
    jest.advanceTimersByTime(1001);
    expect(cache.get('test-1')).toBeUndefined();

    jest.useRealTimers();
  });

  it('should clear all entries', () => {
    const metadata = {
      createdAt: new Date(),
      updatedAt: new Date()
    };

    cache.set('test-1', 123, metadata);
    cache.set('test-2', 456, metadata);
    
    cache.clear();
    
    expect(cache.get('test-1')).toBeUndefined();
    expect(cache.get('test-2')).toBeUndefined();
    expect(cache.getStats().size).toBe(0);
  });

  it('should report correct stats', () => {
    const cache = new IssueCache({ maxSize: 100, ttl: 3600000 });
    const metadata = {
      createdAt: new Date(),
      updatedAt: new Date()
    };

    cache.set('test-1', 123, metadata);
    cache.set('test-2', 456, metadata);

    const stats = cache.getStats();
    expect(stats.size).toBe(2);
    expect(stats.maxSize).toBe(100);
    expect(stats.ttl).toBe(3600000);
  });

  it('should correctly determine if refresh is needed', () => {
    const createdAt = new Date('2025-01-01');
    const updatedAt = new Date('2025-01-02');
    const metadata = { createdAt, updatedAt };

    cache.set('test-1', 123, metadata);

    // No refresh needed for same or older update time
    expect(cache.shouldRefresh('test-1', updatedAt)).toBe(false);
    expect(cache.shouldRefresh('test-1', new Date('2025-01-01'))).toBe(false);

    // Refresh needed for newer update time
    expect(cache.shouldRefresh('test-1', new Date('2025-01-03'))).toBe(true);

    // Always refresh for non-existent entries
    expect(cache.shouldRefresh('nonexistent', new Date())).toBe(true);
  });
});



---
File: typescript/src/__tests__/canonical.test.ts
---
// typescript/src/__tests__/canonical.test.ts

import { describe, it, expect, beforeEach } from '@jest/globals';
import { CanonicalStoreClient } from '../canonical';
import { LabelNames } from '../types';
import fetchMock from 'jest-fetch-mock';

// Create a test version by extending and adding protected methods for exposure
class TestCanonicalStoreClient extends CanonicalStoreClient {
  // Override fetchFromGitHub to make it accessible
  public testFetchFromGitHub<T>(path: string, options?: RequestInit & { params?: Record<string, string> }): Promise<T> {
    return this.fetchFromGitHub<T>(path, options);
  }
  
  // We need to recreate these protected methods for testing
  public testExtractObjectIdFromLabels(issue: { labels: Array<{ name: string }> }): string {
    return this._extractObjectIdFromLabels(issue);
  }
}

describe('CanonicalStoreClient', () => {
  const token = 'test-token';
  const repo = 'owner/repo';
  let client: TestCanonicalStoreClient;

  beforeEach(() => {
    fetchMock.resetMocks();
    // Create the client without passing cache - it's not in CanonicalStoreConfig
    client = new TestCanonicalStoreClient(token, repo);
  });

  describe('resolveCanonicalObjectId', () => {
    it('should resolve direct object ID', async () => {
      // Mock to find the object directly (not an alias)
      fetchMock.mockResponseOnce(JSON.stringify([])); // No issues with alias labels

      const result = await client.resolveCanonicalObjectId('test-object');
      expect(result).toBe('test-object');
    });

    it('should resolve alias to canonical ID', async () => {
      // Mock to find an issue with alias label
      const mockIssue = {
        number: 123,
        labels: [
          { name: `${LabelNames.UID_PREFIX}test-alias` },
          { name: `${LabelNames.ALIAS_TO_PREFIX}test-canonical` }
        ]
      };
      fetchMock.mockResponseOnce(JSON.stringify([mockIssue]));

      const result = await client.resolveCanonicalObjectId('test-alias');
      expect(result).toBe('test-canonical');
    });

    it('should follow alias chain but prevent infinite loops', async () => {
      // Mock the first lookup (test-alias-1 -> test-alias-2)
      const mockIssue1 = {
        number: 123,
        labels: [
          { name: `${LabelNames.UID_PREFIX}test-alias-1` },
          { name: `${LabelNames.ALIAS_TO_PREFIX}test-alias-2` }
        ]
      };
      fetchMock.mockResponseOnce(JSON.stringify([mockIssue1]));

      // Mock the second lookup (test-alias-2 -> test-canonical)
      const mockIssue2 = {
        number: 124,
        labels: [
          { name: `${LabelNames.UID_PREFIX}test-alias-2` },
          { name: `${LabelNames.ALIAS_TO_PREFIX}test-canonical` }
        ]
      };
      fetchMock.mockResponseOnce(JSON.stringify([mockIssue2]));

      // Mock the last lookup (no more aliases)
      fetchMock.mockResponseOnce(JSON.stringify([]));

      const result = await client.resolveCanonicalObjectId('test-alias-1');
      expect(result).toBe('test-canonical');
    });

    it('should detect and break circular references', async () => {
      // Mock circular references (test-alias-a -> test-alias-b -> test-alias-a)
      const mockIssueA = {
        number: 123,
        labels: [
          { name: `${LabelNames.UID_PREFIX}test-alias-a` },
          { name: `${LabelNames.ALIAS_TO_PREFIX}test-alias-b` }
        ]
      };
      fetchMock.mockResponseOnce(JSON.stringify([mockIssueA]));

      const mockIssueB = {
        number: 124,
        labels: [
          { name: `${LabelNames.UID_PREFIX}test-alias-b` },
          { name: `${LabelNames.ALIAS_TO_PREFIX}test-alias-a` }
        ]
      };
      fetchMock.mockResponseOnce(JSON.stringify([mockIssueB]));

      // We should detect the circularity and return test-alias-b (the first level)
      const result = await client.resolveCanonicalObjectId('test-alias-a');
      expect(result).toBe('test-alias-a'); // Return original ID on circular reference
    });
  });

  describe('getObject with canonicalization', () => {
    it('should resolve and use canonical ID by default', async () => {
      // Mock to find the alias
      const mockAliasIssue = {
        number: 123,
        labels: [
          { name: `${LabelNames.UID_PREFIX}test-alias` },
          { name: `${LabelNames.ALIAS_TO_PREFIX}test-canonical` }
        ]
      };
      fetchMock.mockResponseOnce(JSON.stringify([mockAliasIssue]));

      // Mock for empty response (no more aliases)
      fetchMock.mockResponseOnce(JSON.stringify([]));

      // Mock for finding canonical issue
      const mockCanonicalIssue = {
        number: 456,
        body: JSON.stringify({ value: 42 }),
        created_at: '2025-01-01T00:00:00Z',
        updated_at: '2025-01-02T00:00:00Z',
        labels: [
          { name: LabelNames.STORED_OBJECT },
          { name: `${LabelNames.UID_PREFIX}test-canonical` }
        ]
      };
      fetchMock.mockResponseOnce(JSON.stringify([mockCanonicalIssue]));

      // Mock for comments count
      fetchMock.mockResponseOnce(JSON.stringify([]));

      const result = await client.getObject('test-alias');
      
      expect(result.meta.objectId).toBe('test-canonical');
      expect(result.data).toEqual({ value: 42 });
    });

    it('should get alias directly when canonicalize=false', async () => {
      // Mock for direct lookup with UID label
      const mockIssues = [{
        number: 123,
        body: JSON.stringify({ alias_value: 'direct' }),
        created_at: '2025-01-01T00:00:00Z',
        updated_at: '2025-01-02T00:00:00Z',
        labels: [
          { name: LabelNames.STORED_OBJECT },
          { name: `${LabelNames.UID_PREFIX}test-alias` },
          { name: `${LabelNames.ALIAS_TO_PREFIX}test-canonical` }
        ]
      }];
      fetchMock.mockResponseOnce(JSON.stringify(mockIssues));

      // Mock for comments count
      fetchMock.mockResponseOnce(JSON.stringify([]));

      const result = await client.getObject('test-alias', { canonicalize: false });
      
      expect(result.meta.objectId).toBe('test-alias');
      expect(result.data).toEqual({ alias_value: 'direct' });
    });
  });

  describe('createAlias', () => {
    it('should create alias relationship between objects', async () => {
      // Mock for source object lookup
      const mockSourceIssues = [{
        number: 123,
        labels: [
          { name: LabelNames.STORED_OBJECT },
          { name: `${LabelNames.UID_PREFIX}source-id` }
        ]
      }];
      fetchMock.mockResponseOnce(JSON.stringify(mockSourceIssues));

      // Mock for target object lookup
      const mockTargetIssues = [{
        number: 456,
        labels: [
          { name: LabelNames.STORED_OBJECT },
          { name: `${LabelNames.UID_PREFIX}target-id` }
        ]
      }];
      fetchMock.mockResponseOnce(JSON.stringify(mockTargetIssues));

      // Mock for existing labels check
      fetchMock.mockResponseOnce(JSON.stringify([
        { name: LabelNames.STORED_OBJECT },
        { name: `${LabelNames.UID_PREFIX}source-id` }
      ]));

      // Mock for creating alias label
      fetchMock.mockResponseOnce(JSON.stringify({}));

      // Mock for adding label to issue
      fetchMock.mockResponseOnce(JSON.stringify({}));

      const result = await client.createAlias('source-id', 'target-id');
      
      expect(result.success).toBe(true);
      expect(result.sourceId).toBe('source-id');
      expect(result.targetId).toBe('target-id');

      // Verify correct URL for the label creation
      expect(fetchMock.mock.calls[3][0]).toContain('/labels');
    });

    it('should reject if source is already an alias', async () => {
      // Mock for source object lookup
      const mockSourceIssues = [{
        number: 123,
        labels: [
          { name: LabelNames.STORED_OBJECT },
          { name: `${LabelNames.UID_PREFIX}source-id` }
        ]
      }];
      fetchMock.mockResponseOnce(JSON.stringify(mockSourceIssues));

      // Mock for target object lookup
      const mockTargetIssues = [{
        number: 456,
        labels: [
          { name: LabelNames.STORED_OBJECT },
          { name: `${LabelNames.UID_PREFIX}target-id` }
        ]
      }];
      fetchMock.mockResponseOnce(JSON.stringify(mockTargetIssues));

      // Mock for existing labels check - already has an alias
      fetchMock.mockResponseOnce(JSON.stringify([
        { name: LabelNames.STORED_OBJECT },
        { name: `${LabelNames.UID_PREFIX}source-id` },
        { name: `${LabelNames.ALIAS_TO_PREFIX}other-id` }
      ]));

      await expect(client.createAlias('source-id', 'target-id'))
        .rejects
        .toThrow('Object source-id is already an alias');
    });
  });

  describe('findAliases', () => {
    it('should find all aliases in the repository', async () => {
      // Mock for all alias issues
      const mockIssues = [
        {
          labels: [
            { name: `${LabelNames.UID_PREFIX}alias-1` },
            { name: `${LabelNames.ALIAS_TO_PREFIX}canonical-1` }
          ]
        },
        {
          labels: [
            { name: `${LabelNames.UID_PREFIX}alias-2` },
            { name: `${LabelNames.ALIAS_TO_PREFIX}canonical-2` }
          ]
        }
      ];
      fetchMock.mockResponseOnce(JSON.stringify(mockIssues));

      const aliases = await client.findAliases();
      
      // Should find both aliases
      expect(Object.keys(aliases).length).toBe(2);
      expect(aliases['alias-1']).toBe('canonical-1');
      expect(aliases['alias-2']).toBe('canonical-2');
    });

    it('should find aliases for a specific object', async () => {
      // Mock for specific alias issues
      const mockIssues = [
        {
          labels: [
            { name: `${LabelNames.UID_PREFIX}alias-1` },
            { name: `${LabelNames.ALIAS_TO_PREFIX}target-id` }
          ]
        },
        {
          labels: [
            { name: `${LabelNames.UID_PREFIX}alias-2` },
            { name: `${LabelNames.ALIAS_TO_PREFIX}target-id` }
          ]
        }
      ];
      fetchMock.mockResponseOnce(JSON.stringify(mockIssues));

      const aliases = await client.findAliases('target-id');
      
      // Should find both aliases for the target
      expect(Object.keys(aliases).length).toBe(2);
      expect(aliases['alias-1']).toBe('target-id');
      expect(aliases['alias-2']).toBe('target-id');
    });
  });
});



---
File: typescript/src/__tests__/client.test.ts
---
// typescript/src/__tests__/client.test.ts

// In typescript/src/__tests__/client.test.ts:
import { describe, it, expect, beforeEach } from '@jest/globals';
import { GitHubStoreClient } from '../client';
import { LabelNames } from '../types'; // Add this import
import { CLIENT_VERSION } from '../version';
import fetchMock from 'jest-fetch-mock';

describe('GitHubStoreClient', () => {
  const token = 'test-token';
  const repo = 'owner/repo';
  let client: GitHubStoreClient;

  beforeEach(() => {
    fetchMock.resetMocks();
    client = new GitHubStoreClient(token, repo, {
      cache: {
        maxSize: 100,
        ttl: 3600000
      }
    });
  });

  describe('getObject with cache', () => {
    const mockIssue = {
      number: 123,
      body: JSON.stringify({ key: 'value' }),
      created_at: '2025-01-01T00:00:00Z',
      updated_at: '2025-01-02T00:00:00Z',
      labels: [
        { name: 'stored-object' },
        { name: 'UID:test-object' }
      ]
    };

    it('should use cached issue number on subsequent requests', async () => {
      // First request - should query by labels
      fetchMock
        .mockResponseOnce(JSON.stringify([mockIssue])) // Initial labels query
        .mockResponseOnce(JSON.stringify([])); // Comments query for version

      await client.getObject('test-object');
      expect(fetchMock.mock.calls[0][0]).toContain('/issues?labels=');

      // Reset mock to verify cache hit
      fetchMock.resetMocks();
      fetchMock
        .mockResponseOnce(JSON.stringify(mockIssue)) // Direct issue fetch
        .mockResponseOnce(JSON.stringify([])); // Comments query for version

      await client.getObject('test-object');
      
      // Should use direct issue number fetch instead of labels query
      expect(fetchMock.mock.calls[0][0]).toContain('/issues/123');
    });

    it('should fall back to label query if cached issue is not found', async () => {
      // First request succeeds
      fetchMock
        .mockResponseOnce(JSON.stringify([mockIssue]))
        .mockResponseOnce(JSON.stringify([]));

      await client.getObject('test-object');

      // Reset mock to simulate deleted issue
      fetchMock.resetMocks();
      fetchMock
        .mockResponseOnce('', { status: 404 }) // Cached issue not found
        .mockResponseOnce(JSON.stringify([mockIssue])) // Fallback label query
        .mockResponseOnce(JSON.stringify([])); // Comments query

      await client.getObject('test-object');

      // Should have attempted direct fetch, then fallen back to labels
      expect(fetchMock.mock.calls[0][0]).toContain('/issues/123');
      expect(fetchMock.mock.calls[1][0]).toContain('/issues?labels=');
    });

    it('should fetch and parse object correctly', async () => {
      const mockComments = [{ id: 1 }, { id: 2 }];

      fetchMock
        .mockResponseOnce(JSON.stringify([mockIssue]))
        .mockResponseOnce(JSON.stringify(mockComments));

      const obj = await client.getObject('test-object');

      expect(obj.meta.objectId).toBe('test-object');
      expect(obj.meta.version).toBe(3);
      expect(obj.data).toEqual({ key: 'value' });
    });
  });
  
  // In client.test.ts, update the createObject test:
  describe('createObject', () => {
    it('should create new object with initial state and metadata', async () => {
      const mockIssue = {
        number: 456,
        created_at: '2025-01-01T00:00:00Z',
        updated_at: '2025-01-01T00:00:00Z',
        html_url: 'https://github.com/owner/repo/issues/456',
        body: JSON.stringify({ test: 'data' }),
        labels: [
          { name: 'stored-object' },
          { name: 'UID:test-object' }
        ]
      };
  
      const mockComment = { id: 123 };
  
      fetchMock
        .mockResponseOnce(JSON.stringify(mockIssue)) // Create issue
        .mockResponseOnce(JSON.stringify(mockComment)) // Create comment
        .mockResponseOnce(JSON.stringify({ id: 1 })) // Add processed reaction
        .mockResponseOnce(JSON.stringify({ id: 2 })) // Add initial state reaction
        .mockResponseOnce(JSON.stringify({ state: 'closed' })); // Close issue
  
      const data = { test: 'data' };
      const obj = await client.createObject('test-object', data);
  
      expect(obj.meta.objectId).toBe('test-object');
      expect(obj.meta.version).toBe(1);
      expect(obj.data).toEqual(data);
  
      // Verify issue creation includes all required labels
      expect(fetchMock.mock.calls[0][1]?.body).toContain('"stored-object"');
      expect(fetchMock.mock.calls[0][1]?.body).toContain('"UID:test-object"');
      expect(fetchMock.mock.calls[0][1]?.body).toContain('"gh-store"'); // Verify gh-store label is included
  
      // Verify initial state comment with metadata
      const commentBody = JSON.parse(JSON.parse(fetchMock.mock.calls[1][1]?.body as string).body);
      expect(commentBody.type).toBe('initial_state');
      expect(commentBody._data).toEqual(data);
      expect(commentBody._meta).toBeDefined();
      expect(commentBody._meta.client_version).toBe(CLIENT_VERSION);
      expect(commentBody._meta.timestamp).toBeDefined();
      expect(commentBody._meta.update_mode).toBe('append');
    });
  });
  
  // Add a specific test to verify label structure:
  it('should include gh-store label when creating objects', async () => {
    const mockIssue = {
      number: 789,
      created_at: '2025-01-01T00:00:00Z',
      updated_at: '2025-01-01T00:00:00Z',
      html_url: 'https://github.com/owner/repo/issues/789',
      body: '{}',
      labels: []
    };
    
    // Mock all the required responses
    fetchMock
      .mockResponseOnce(JSON.stringify(mockIssue))
      .mockResponseOnce(JSON.stringify({ id: 1 }))
      .mockResponseOnce(JSON.stringify({ id: 1 }))
      .mockResponseOnce(JSON.stringify({ id: 2 }))
      .mockResponseOnce(JSON.stringify({ state: 'closed' }));
    
    await client.createObject('test-label-object', {});
    
    // Parse the request body from the first call (create issue)
    const requestBody = JSON.parse(fetchMock.mock.calls[0][1]?.body as string);
    
    // Verify the labels array includes all required labels
    expect(requestBody.labels).toContain(LabelNames.GH_STORE);
    expect(requestBody.labels).toContain('stored-object');
    expect(requestBody.labels).toContain('UID:test-label-object');
    expect(requestBody.labels.length).toBe(3); // Should only be these three labels
  });

  describe('updateObject', () => {
    it('should add update comment with metadata', async () => {
      const mockIssue = {
        number: 1,
        state: 'closed',
        body: JSON.stringify({ key: 'value' }),
        created_at: '2025-01-01T00:00:00Z',
        updated_at: '2025-01-02T00:00:00Z',
        labels: [
          { name: 'stored-object' },
          { name: 'UID:test-object' }
        ]
      };

      fetchMock
        .mockResponseOnce(JSON.stringify([mockIssue])) // Get issue
        .mockResponseOnce(JSON.stringify({ id: 123 })) // Add comment
        .mockResponseOnce(JSON.stringify({ state: 'open' })) // Reopen issue
        .mockResponseOnce(JSON.stringify([mockIssue])) // Get updated object
        .mockResponseOnce(JSON.stringify([])); // Get comments for version

      const changes = { key: 'updated' };
      await client.updateObject('test-object', changes);

      // Verify update comment with metadata
      const commentPayload = JSON.parse(fetchMock.mock.calls[1][1]?.body as string);
      const commentBody = JSON.parse(commentPayload.body);
      expect(commentBody._data).toEqual(changes);
      expect(commentBody._meta).toBeDefined();
      expect(commentBody._meta.client_version).toBe(CLIENT_VERSION);
      expect(commentBody._meta.timestamp).toBeDefined();
      expect(commentBody._meta.update_mode).toBe('append');
    });
  });

  describe('getObjectHistory', () => {
    it('should return full object history with metadata', async () => {
      const mockIssue = {
        number: 1,
        labels: [
          { name: 'stored-object' },
          { name: 'UID:test-object' }
        ]
      };

      const mockComments = [
        {
          id: 1,
          created_at: '2025-01-01T00:00:00Z',
          body: JSON.stringify({
            type: 'initial_state',
            _data: { status: 'new' },
            _meta: {
              client_version: CLIENT_VERSION,
              timestamp: '2025-01-01T00:00:00Z',
              update_mode: 'append'
            }
          })
        },
        {
          id: 2,
          created_at: '2025-01-02T00:00:00Z',
          body: JSON.stringify({
            _data: { status: 'updated' },
            _meta: {
              client_version: CLIENT_VERSION,
              timestamp: '2025-01-02T00:00:00Z',
              update_mode: 'append'
            }
          })
        }
      ];

      fetchMock
        .mockResponseOnce(JSON.stringify([mockIssue]))
        .mockResponseOnce(JSON.stringify(mockComments));

      const history = await client.getObjectHistory('test-object');

      expect(history).toHaveLength(2);
      expect(history[0].type).toBe('initial_state');
      expect(history[0].data).toEqual({ status: 'new' });
      expect(history[1].type).toBe('update');
      expect(history[1].data).toEqual({ status: 'updated' });
    });
  });

  describe('API Error Handling', () => {
    it('should throw error on API failure', async () => {
      fetchMock.mockResponseOnce('', { 
        status: 500,
        statusText: 'Internal Server Error'
      });

      await expect(client.getObject('test-obj'))
        .rejects
        .toThrow('GitHub API error: 500');
    });

    it('should handle malformed JSON responses', async () => {
      fetchMock.mockResponseOnce('invalid json');

      await expect(client.getObject('test-obj'))
        .rejects
        .toThrow();
    });
  });

  describe('listAll', () => {
    it('should handle empty repository', async () => {
      fetchMock.mockResponseOnce(JSON.stringify([]));

      const objects = await client.listAll();
      expect(Object.keys(objects)).toHaveLength(0);
    });

    it('should handle invalid issue data', async () => {
      const mockIssues = [{
        number: 1,
        body: 'invalid json',
        labels: [
          { name: 'stored-object' },
          { name: 'UID:test-1' }
        ],
        created_at: '2025-01-01T00:00:00Z',
        updated_at: '2025-01-02T00:00:00Z'
      }];

      fetchMock.mockResponseOnce(JSON.stringify(mockIssues));

      const objects = await client.listAll();
      expect(Object.keys(objects)).toHaveLength(0);
    });

    it('should skip issues without proper labels', async () => {
      const mockIssues = [{
        number: 1,
        body: JSON.stringify({ test: 'data' }),
        labels: [
          { name: 'stored-object' }  // Missing UID label
        ],
        created_at: '2025-01-01T00:00:00Z',
        updated_at: '2025-01-02T00:00:00Z'
      }];

      fetchMock.mockResponseOnce(JSON.stringify(mockIssues));

      const objects = await client.listAll();
      expect(Object.keys(objects)).toHaveLength(0);
    });
  });

  describe('listUpdatedSince', () => {
    it('should handle no updates', async () => {
      const timestamp = new Date('2025-01-01T00:00:00Z');
      fetchMock.mockResponseOnce(JSON.stringify([]));

      const objects = await client.listUpdatedSince(timestamp);
      expect(Object.keys(objects)).toHaveLength(0);
    });

    it('should ignore updates before timestamp', async () => {
      const timestamp = new Date('2025-01-02T00:00:00Z');
      const mockIssues = [{
        number: 1,
        body: JSON.stringify({ test: 'data' }),
        created_at: '2025-01-01T00:00:00Z',
        updated_at: '2025-01-01T12:00:00Z',  // Before timestamp
        labels: [
          { name: 'stored-object' },
          { name: 'UID:test-1' }
        ]
      }];

      fetchMock.mockResponseOnce(JSON.stringify(mockIssues));

      const objects = await client.listUpdatedSince(timestamp);
      expect(Object.keys(objects)).toHaveLength(0);
    });
  });

  describe('getObjectHistory', () => {
    it('should handle missing object', async () => {
      fetchMock.mockResponseOnce(JSON.stringify([]));

      await expect(client.getObjectHistory('nonexistent'))
        .rejects
        .toThrow('No object found with ID: nonexistent');
    });

    it('should handle invalid comments', async () => {
      const mockIssue = {
        number: 1,
        labels: [
          { name: 'stored-object' },
          { name: 'UID:test-object' }
        ]
      };

      const mockComments = [
        {
          id: 1,
          created_at: '2025-01-01T00:00:00Z',
          body: 'invalid json'  // Invalid comment
        },
        {
          id: 2,
          created_at: '2025-01-02T00:00:00Z',
          body: JSON.stringify({
            _data: { status: 'valid' },
            _meta: {
              client_version: CLIENT_VERSION,
              timestamp: '2025-01-02T00:00:00Z',
              update_mode: 'append'
            }
          })
        }
      ];

      fetchMock
        .mockResponseOnce(JSON.stringify([mockIssue]))
        .mockResponseOnce(JSON.stringify(mockComments));

      const history = await client.getObjectHistory('test-object');

      expect(history).toHaveLength(1);  // Only valid comment included
      expect(history[0].data).toEqual({ status: 'valid' });
    });

    it('should process legacy format comments', async () => {
      const mockIssue = {
        number: 1,
        labels: [
          { name: 'stored-object' },
          { name: 'UID:test-object' }
        ]
      };

      const mockComments = [
        {
          id: 1,
          created_at: '2025-01-01T00:00:00Z',
          body: JSON.stringify({ status: 'legacy' })  // Legacy format
        }
      ];

      fetchMock
        .mockResponseOnce(JSON.stringify([mockIssue]))
        .mockResponseOnce(JSON.stringify(mockComments));

      const history = await client.getObjectHistory('test-object');

      expect(history).toHaveLength(1);
      expect(history[0].type).toBe('update');
      expect(history[0].data).toEqual({ status: 'legacy' });
    });
  });
});



---
File: typescript/src/__tests__/public-mode.test.ts
---
// typescript/src/__tests__/public-mode.test.ts
import { describe, it, expect, beforeEach } from '@jest/globals';
import { GitHubStoreClient } from '../client';
import fetchMock from 'jest-fetch-mock';

describe('GitHubStoreClient in Public Mode', () => {
  let client: GitHubStoreClient;
  const repo = 'owner/repo';

  beforeEach(() => {
    fetchMock.resetMocks();
    // Initialize in public mode (no token)
    client = new GitHubStoreClient(null, repo);
  });

  it('should correctly identify as public mode', () => {
    expect(client.isPublic()).toBe(true);
  });

  it('should fetch objects without authentication headers', async () => {
    const mockIssue = {
      number: 123,
      body: JSON.stringify({ key: 'value' }),
      created_at: '2025-01-01T00:00:00Z',
      updated_at: '2025-01-02T00:00:00Z',
      labels: [
        { name: 'stored-object' },
        { name: 'UID:test-object' }
      ]
    };

    fetchMock
      .mockResponseOnce(JSON.stringify([mockIssue]))
      .mockResponseOnce(JSON.stringify([]));

    await client.getObject('test-object');
    
    // Verify no auth header was sent
    expect(fetchMock.mock.calls[0][1]?.headers).not.toHaveProperty('Authorization');
  });

  it('should reject create operations in public mode', async () => {
    await expect(client.createObject('test-object', { key: 'value' }))
      .rejects
      .toThrow('Authentication required for creating objects');
  });

  it('should reject update operations in public mode', async () => {
    await expect(client.updateObject('test-object', { key: 'value' }))
      .rejects
      .toThrow('Authentication required for updating objects');
  });

  it('should fetch object history in public mode', async () => {
    const mockIssue = {
      number: 123,
      labels: [
        { name: 'stored-object' },
        { name: 'UID:test-object' }
      ]
    };

    const mockComments = [
      {
        id: 1,
        created_at: '2025-01-01T00:00:00Z',
        body: JSON.stringify({
          type: 'initial_state',
          _data: { status: 'new' },
          _meta: {
            client_version: '0.9.0',
            timestamp: '2025-01-01T00:00:00Z',
            update_mode: 'append'
          }
        })
      }
    ];

    fetchMock
      .mockResponseOnce(JSON.stringify([mockIssue]))
      .mockResponseOnce(JSON.stringify(mockComments));

    const history = await client.getObjectHistory('test-object');
    
    expect(history).toHaveLength(1);
    expect(history[0].type).toBe('initial_state');
    expect(history[0].data).toEqual({ status: 'new' });
    
    // Verify no auth header was sent
    expect(fetchMock.mock.calls[0][1]?.headers).not.toHaveProperty('Authorization');
  });
});



---
File: typescript/src/cache.ts
---
// src/cache.ts
export interface CacheEntry {
  issueNumber: number;
  lastAccessed: number; // Using timestamp instead of Date for easier comparison
  createdAt: Date;
  updatedAt: Date;
}

export interface CacheConfig {
  maxSize?: number;
  ttl?: number; // Time-to-live in milliseconds
}

export class IssueCache {
  private cache: Map<string, CacheEntry>;
  private maxSize: number;
  private ttl: number;
  private accessOrder: string[]; // Track order of access

  constructor(config: CacheConfig = {}) {
    this.cache = new Map();
    this.maxSize = config.maxSize ?? 1000;
    this.ttl = config.ttl ?? 1000 * 60 * 60; // Default 1 hour TTL
    this.accessOrder = [];
  }

  get(objectId: string): number | undefined {
    const entry = this.cache.get(objectId);
    
    if (!entry) {
      return undefined;
    }

    // Check if entry has expired
    if (Date.now() - entry.lastAccessed > this.ttl) {
      this.cache.delete(objectId);
      this.removeFromAccessOrder(objectId);
      return undefined;
    }

    // Update last accessed time and move to front of access order
    entry.lastAccessed = Date.now();
    this.updateAccessOrder(objectId);
    return entry.issueNumber;
  }

  set(objectId: string, issueNumber: number, metadata: { createdAt: Date; updatedAt: Date }): void {
    // Evict least recently used entry if cache is full
    if (this.cache.size >= this.maxSize && !this.cache.has(objectId)) {
      const lru = this.accessOrder[this.accessOrder.length - 1];
      if (lru) {
        this.cache.delete(lru);
        this.removeFromAccessOrder(lru);
      }
    }

    // Add/update entry
    this.cache.set(objectId, {
      issueNumber,
      lastAccessed: Date.now(),
      createdAt: metadata.createdAt,
      updatedAt: metadata.updatedAt
    });

    this.updateAccessOrder(objectId);
  }

  remove(objectId: string): void {
    this.cache.delete(objectId);
    this.removeFromAccessOrder(objectId);
  }

  clear(): void {
    this.cache.clear();
    this.accessOrder = [];
  }

  getStats(): { size: number; maxSize: number; ttl: number } {
    return {
      size: this.cache.size,
      maxSize: this.maxSize,
      ttl: this.ttl
    };
  }

  shouldRefresh(objectId: string, latestUpdate: Date): boolean {
    const entry = this.cache.get(objectId);
    if (!entry) return true;

    return latestUpdate > entry.updatedAt;
  }

  private updateAccessOrder(objectId: string): void {
    this.removeFromAccessOrder(objectId);
    this.accessOrder.unshift(objectId); // Add to front
  }

  private removeFromAccessOrder(objectId: string): void {
    const index = this.accessOrder.indexOf(objectId);
    if (index > -1) {
      this.accessOrder.splice(index, 1);
    }
  }
}



---
File: typescript/src/canonical.ts
---
// typescript/src/canonical.ts
import { GitHubStoreClient } from './client';
import { GitHubStoreConfig, LabelNames, StoredObject } from './types';
import { Logger } from './logging'; // Import a logger utility

// Create a logger instance
const logger = new Logger('CanonicalStore');

// Configuration for CanonicalStore
export interface CanonicalStoreConfig extends GitHubStoreConfig {
  canonicalize?: boolean; // Whether to perform canonicalization by default
}

// Result type for alias creation
export interface AliasResult {
  success: boolean;
  sourceId: string;
  targetId: string;
}

// The main CanonicalStore class
export class CanonicalStoreClient extends GitHubStoreClient {
  private canonicalizeByDefault: boolean;
  private visitedIds: Set<string>; // For circular reference detection

  constructor(
    token: string,
    repo: string,
    config: CanonicalStoreConfig = {}
  ) {
    super(token, repo, config);
    this.canonicalizeByDefault = config.canonicalize ?? true;
    this.visitedIds = new Set<string>();
    
    // Ensure special labels exist
    this._ensureSpecialLabels().catch(err => {
      logger.warn(`Could not ensure special labels exist: ${(err as Error).message}`);
    });
  }
  
  // Create special labels needed by the system
  private async _ensureSpecialLabels(): Promise<void> {
    const specialLabels = [
      { name: LabelNames.GH_STORE, color: "6f42c1", description: "All issues managed by gh-store system" }
    ];

    try {
      // Get existing labels
      const existingLabelsResponse = await this.fetchFromGitHub<Array<{ name: string }>>("/labels");
      const existingLabels = new Set(existingLabelsResponse.map(label => label.name));

      // Create any missing labels
      for (const label of specialLabels) {
        if (!existingLabels.has(label.name)) {
          try {
            await this.fetchFromGitHub("/labels", {
              method: "POST",
              body: JSON.stringify(label)
            });
          } catch (error) {
            logger.warn(`Could not create label ${label.name}: ${(error as Error).message}`);
          }
        }
      }
    } catch (error) {
      logger.warn(`Could not ensure special labels exist: ${(error as Error).message}`);
    }
  }

  // Resolve object ID to its canonical form
  async resolveCanonicalObjectId(objectId: string, maxDepth: number = 5): Promise<string> {
    // Reset visited IDs for each top-level resolution attempt
    this.visitedIds = new Set<string>();
    return this._resolveCanonicalIdInternal(objectId, maxDepth);
  }

  // Internal method for alias resolution with cycle detection
  private async _resolveCanonicalIdInternal(objectId: string, maxDepth: number): Promise<string> {
    if (maxDepth <= 0) {
      logger.warn(`Maximum alias resolution depth reached for ${objectId}`);
      return objectId;
    }

    // Detect circular references
    if (this.visitedIds.has(objectId)) {
      logger.warn(`Circular reference detected for ${objectId}`);
      return objectId;
    }

    // Mark this ID as visited
    this.visitedIds.add(objectId);

    // Check if this is an alias
    try {
      const issues = await this.fetchFromGitHub<Array<{
        number: number;
        labels: Array<{ name: string }>;
      }>>("/issues", {
        method: "GET",
        params: {
          labels: `${LabelNames.UID_PREFIX}${objectId},${LabelNames.ALIAS_TO_PREFIX}*`,
          state: "all",
        },
      });

      if (issues && issues.length > 0) {
        for (const issue of issues) {
          for (const label of issue.labels) {
            if (label.name.startsWith(LabelNames.ALIAS_TO_PREFIX)) {
              // Extract canonical object ID from label
              const canonicalId = label.name.slice(LabelNames.ALIAS_TO_PREFIX.length);
              
              // Prevent self-referential loops
              if (canonicalId === objectId) {
                logger.error(`Self-referential alias detected for ${objectId}`);
                return objectId;
              }
              
              // Recurse to follow alias chain
              return this._resolveCanonicalIdInternal(canonicalId, maxDepth - 1);
            }
          }
        }
      }
    } catch (error) {
      logger.warn(`Error resolving canonical ID for ${objectId}: ${(error as Error).message}`);
    }

    // Not an alias, or couldn't resolve - assume it's canonical
    return objectId;
  }

  // Override getObject to implement canonicalization
  async getObject(objectId: string, options: { canonicalize?: boolean } = {}): Promise<StoredObject> {
    const canonicalize = options.canonicalize ?? this.canonicalizeByDefault;
    
    if (canonicalize) {
      const canonicalId = await this.resolveCanonicalObjectId(objectId);
      if (canonicalId !== objectId) {
        logger.info(`Object ${objectId} resolved to canonical object ${canonicalId}`);
      }
      return super.getObject(canonicalId);
    } else {
      // Direct fetch without canonicalization
      return super.getObject(objectId);
    }
  }

  // Create an alias relationship
  async createAlias(sourceId: string, targetId: string): Promise<AliasResult> {
    // 1. Verify source object exists
    let sourceIssue;
    try {
      const sourceIssues = await this.fetchFromGitHub<Array<{ number: number }>>("/issues", {
        method: "GET",
        params: {
          labels: `${LabelNames.UID_PREFIX}${sourceId},${LabelNames.STORED_OBJECT}`,
          state: "all",
        },
      });
      
      if (!sourceIssues || sourceIssues.length === 0) {
        throw new Error(`Source object not found: ${sourceId}`);
      }
      
      sourceIssue = sourceIssues[0];
    } catch (error) {
      throw new Error(`Error finding source object: ${(error as Error).message}`);
    }
    
    // 2. Verify target object exists
    try {
      const targetIssues = await this.fetchFromGitHub<Array<{ number: number }>>("/issues", {
        method: "GET",
        params: {
          labels: `${LabelNames.UID_PREFIX}${targetId},${LabelNames.STORED_OBJECT}`,
          state: "all",
        },
      });
      
      if (!targetIssues || targetIssues.length === 0) {
        throw new Error(`Target object not found: ${targetId}`);
      }
    } catch (error) {
      throw new Error(`Error finding target object: ${(error as Error).message}`);
    }
    
    // 3. Check if this is already an alias
    try {
      const existingAliasLabels = await this.fetchFromGitHub<Array<{ name: string }>>(`/issues/${sourceIssue.number}/labels`);
      
      for (const label of existingAliasLabels) {
        if (label.name.startsWith(LabelNames.ALIAS_TO_PREFIX)) {
          throw new Error(`Object ${sourceId} is already an alias`);
        }
      }
    } catch (error) {
      if (!(error as Error).message.includes('already an alias')) {
        throw new Error(`Error checking existing aliases: ${(error as Error).message}`);
      } else {
        throw error; // Rethrow "already an alias" error
      }
    }
    
    // 4. Create alias label if it doesn't exist
    const aliasLabel = `${LabelNames.ALIAS_TO_PREFIX}${targetId}`;
    try {
      // Try to create the label - might fail if it already exists
      try {
        await this.fetchFromGitHub("/labels", {
          method: "POST",
          body: JSON.stringify({
            name: aliasLabel,
            color: "fbca04"
          })
        });
      } catch (error) {
        // Label might already exist, continue
        logger.warn(`Could not create label ${aliasLabel}: ${(error as Error).message}`);
      }
      
      // Add label to source issue
      await this.fetchFromGitHub(`/issues/${sourceIssue.number}/labels`, {
        method: "POST",
        body: JSON.stringify({
          labels: [aliasLabel]
        })
      });
      
      return {
        success: true,
        sourceId,
        targetId
      };
    } catch (error) {
      throw new Error(`Failed to create alias: ${(error as Error).message}`);
    }
  }

  // Find aliases in the repository
  async findAliases(objectId?: string): Promise<Record<string, string>> {
    const aliases: Record<string, string> = {};
    
    try {
      if (objectId) {
        // Find aliases for specific object
        const aliasIssues = await this.fetchFromGitHub<Array<{
          labels: Array<{ name: string }>;
        }>>("/issues", {
          method: "GET",
          params: {
            labels: `${LabelNames.ALIAS_TO_PREFIX}${objectId}`,
            state: "all",
          },
        });
        
        for (const issue of aliasIssues || []) {
          const aliasId = this._extractObjectIdFromLabels(issue);
          if (aliasId) {
            aliases[aliasId] = objectId;
          }
        }
      } else {
        // Find all aliases
        const aliasIssues = await this.fetchFromGitHub<Array<{
          labels: Array<{ name: string }>;
        }>>("/issues", {
          method: "GET",
          params: {
            labels: `${LabelNames.ALIAS_TO_PREFIX}*`,
            state: "all",
          },
        });
        
        for (const issue of aliasIssues || []) {
          const aliasId = this._extractObjectIdFromLabels(issue);
          if (!aliasId) continue;
          
          // Find target of alias
          for (const label of issue.labels) {
            if (label.name.startsWith(LabelNames.ALIAS_TO_PREFIX)) {
              const canonicalId = label.name.slice(LabelNames.ALIAS_TO_PREFIX.length);
              aliases[aliasId] = canonicalId;
              break;
            }
          }
        }
      }
      
      return aliases;
    } catch (error) {
      logger.warn(`Error finding aliases: ${(error as Error).message}`);
      return {};
    }
  }

  // Helper to extract object ID from labels
  protected _extractObjectIdFromLabels(issue: { labels: Array<{ name: string }> }): string {
    for (const label of issue.labels) {
      if (label.name.startsWith(LabelNames.UID_PREFIX)) {
        return label.name.slice(LabelNames.UID_PREFIX.length);
      }
    }
    
    throw new Error(`No UID label found with prefix ${LabelNames.UID_PREFIX}`);
  }
}



---
File: typescript/src/client.ts
---
// typescript/src/client.ts
import { 
  CommentPayload, 
  ObjectMeta, 
  GitHubStoreConfig, 
  Json, 
  LabelNames, 
  StoredObject 
} from './types';
import { IssueCache, CacheConfig } from './cache';
import { CLIENT_VERSION } from './version';

interface GitHubIssue {
  number: number;
  body: string;
  created_at: string;
  updated_at: string;
  labels: Array<{ name: string }>;
  state?: string;
}

export class GitHubStoreClient {
  private token: string | null;
  private repo: string;
  private config: Required<GitHubStoreConfig>;
  private cache: IssueCache;

  constructor(
    token: string | null, 
    repo: string,
    config: GitHubStoreConfig & { cache?: CacheConfig } = {}
  ) {
    this.token = token;
    this.repo = repo;
    
    if (!this.repo) {
      throw new Error('Repository is required');
    }

    this.config = {
      baseLabel: config.baseLabel ?? "stored-object",
      uidPrefix: config.uidPrefix ?? "UID:",
      reactions: {
        processed: config.reactions?.processed ?? "+1",
        initialState: config.reactions?.initialState ?? "rocket",
      },
    };
    this.cache = new IssueCache(config.cache);
  }
  
  /**
   * Check if the client is operating in public (unauthenticated) mode
   * @returns True if client is using unauthenticated mode
   */
  public isPublic(): boolean {
    return this.token === null;
  }

  /**
   * Makes a request to the GitHub API
   * 
   * @param path - The API path to request (e.g., "/issues")
   * @param options - Request options including optional params
   * @returns The JSON response from the API
   */
  protected async fetchFromGitHub<T>(path: string, options: RequestInit & { params?: Record<string, string> } = {}): Promise<T> {
    const url = new URL(`https://api.github.com/repos/${this.repo}${path}`);
    
    if (options.params) {
      Object.entries(options.params).forEach(([key, value]) => {
        url.searchParams.append(key, value);
      });
      delete options.params;
    }
  
    // Create a new headers object
    const headersObj: Record<string, string> = {
      "Accept": "application/vnd.github.v3+json"
    };
    
    // Add any existing headers from options
    if (options.headers) {
      const existingHeaders = options.headers as Record<string, string>;
      Object.keys(existingHeaders).forEach(key => {
        headersObj[key] = existingHeaders[key];
      });
    }
    
    // Add authorization header only if token is provided
    if (this.token) {
      headersObj["Authorization"] = `token ${this.token}`;
    }
  
    const response = await fetch(url.toString(), {
      ...options,
      headers: headersObj
    });
  
    if (!response.ok) {
      throw new Error(`GitHub API error: ${response.status}`);
    }
  
    return response.json() as Promise<T>;
  }

  private createCommentPayload(data: Json, issueNumber: number, type?: string): CommentPayload {
    const payload: CommentPayload = {
      _data: data,
      _meta: {
        client_version: CLIENT_VERSION,
        timestamp: new Date().toISOString(),
        update_mode: "append",
        issue_number: issueNumber  // Include issue number in metadata
      }
    };
    
    if (type) {
      payload.type = type;
    }
    
    return payload;
  }

  async getObject(objectId: string): Promise<StoredObject> {
    // Try to get issue number from cache
    const cachedIssueNumber = this.cache.get(objectId);
    let issue: GitHubIssue | undefined;

    if (cachedIssueNumber) {
      // Try to fetch directly using cached issue number
      try {
        issue = await this.fetchFromGitHub<GitHubIssue>(`/issues/${cachedIssueNumber}`);

        // Verify it's the correct issue
        if (!this._verifyIssueLabels(issue, objectId)) {
          this.cache.remove(objectId);
          issue = undefined;
        }
      } catch (error) {
        // If issue not found, remove from cache
        this.cache.remove(objectId);
      }
    }

    if (!issue) {
      // Fall back to searching by labels
      const issues = await this.fetchFromGitHub<GitHubIssue[]>("/issues", {
        method: "GET",
        params: {
          labels: [LabelNames.GH_STORE, this.config.baseLabel, `${this.config.uidPrefix}${objectId}`].join(","),
          state: "closed",
        },
      });

      if (!issues || issues.length === 0) {
        throw new Error(`No object found with ID: ${objectId}`);
      }

      issue = issues[0];
    }

    if (!issue?.body) {
      throw new Error(`Invalid issue data received for ID: ${objectId}`);
    }

    const data = JSON.parse(issue.body) as Json;
    const createdAt = new Date(issue.created_at);
    const updatedAt = new Date(issue.updated_at);

    // Update cache
    this.cache.set(objectId, issue.number, { createdAt, updatedAt });

    const meta: ObjectMeta = {
      objectId,
      label: `${this.config.uidPrefix}${objectId}`,
      issueNumber: issue.number,
      createdAt,
      updatedAt,
      version: await this._getVersion(issue.number)
    };

    return { meta, data };
  }

  async createObject(objectId: string, data: Json): Promise<StoredObject> {
    if (!this.token) {
      throw new Error('Authentication required for creating objects');
    }

    const uidLabel = `${this.config.uidPrefix}${objectId}`;
    
    const issue = await this.fetchFromGitHub<{
      number: number;
      created_at: string;
      updated_at: string;
      html_url: string;
    }>("/issues", {
      method: "POST",
      body: JSON.stringify({
        title: `Stored Object: ${objectId}`,
        body: JSON.stringify(data, null, 2),
        labels: [LabelNames.GH_STORE, this.config.baseLabel, uidLabel]
      })
    });

    // Add to cache immediately
    this.cache.set(objectId, issue.number, {
      createdAt: new Date(issue.created_at),
      updatedAt: new Date(issue.updated_at)
    });

    // Create and add initial state comment
    const initialState = this.createCommentPayload(data, issue.number, "initial_state");
    
    const comment = await this.fetchFromGitHub<{ id: number }>(`/issues/${issue.number}/comments`, {
      method: "POST",
      body: JSON.stringify({
        body: JSON.stringify(initialState, null, 2)
      })
    });

    await this.fetchFromGitHub(`/issues/comments/${comment.id}/reactions`, {
      method: "POST",
      body: JSON.stringify({ content: this.config.reactions.processed })
    });

    await this.fetchFromGitHub(`/issues/comments/${comment.id}/reactions`, {
      method: "POST",
      body: JSON.stringify({ content: this.config.reactions.initialState })
    });

    await this.fetchFromGitHub(`/issues/${issue.number}`, {
      method: "PATCH",
      body: JSON.stringify({ state: "closed" })
    });

    const meta: ObjectMeta = {
      objectId,
      label: uidLabel,
      issueNumber: issue.number,
      createdAt: new Date(issue.created_at),
      updatedAt: new Date(issue.updated_at),
      version: 1
    };

    return { meta, data };
  }
  
  private _verifyIssueLabels(issue: { labels: Array<{ name: string }> }, objectId: string): boolean {
    const expectedLabels = new Set([
      this.config.baseLabel,
      `${this.config.uidPrefix}${objectId}`
    ]);

    return issue.labels.some(label => expectedLabels.has(label.name));
  }
  
  async updateObject(objectId: string, changes: Json): Promise<StoredObject> {
    if (!this.token) {
      throw new Error('Authentication required for updating objects');
    }

    // Get the object's issue first
    const issues = await this.fetchFromGitHub<Array<{
      number: number;
      state: string;
    }>>("/issues", {
      method: "GET",
      params: {
        labels: [this.config.baseLabel, `${this.config.uidPrefix}${objectId}`].join(","),
        state: "all",
      },
    });

    if (!issues || issues.length === 0) {
      throw new Error(`No object found with ID: ${objectId}`);
    }

    const issue = issues[0];
    
    // Create update payload with metadata
    const updatePayload = this.createCommentPayload(changes, issue.number);

    // Add update comment
    await this.fetchFromGitHub(`/issues/${issue.number}/comments`, {
      method: "POST",
      body: JSON.stringify({
        body: JSON.stringify(updatePayload, null, 2)
      })
    });

    // Reopen issue to trigger processing
    await this.fetchFromGitHub(`/issues/${issue.number}`, {
      method: "PATCH",
      body: JSON.stringify({ state: "open" })
    });

    // Return current state (before update is processed)
    return this.getObject(objectId);
  }

  // Rest of methods remain the same...
  
  async listAll(): Promise<Record<string, StoredObject>> {
    const issues = await this.fetchFromGitHub<Array<{
      number: number;
      body: string;
      created_at: string;
      updated_at: string;
      labels: Array<{ name: string }>;
    }>>("/issues", {
      method: "GET",
      params: {
        labels: this.config.baseLabel,
        state: "closed",
      },
    });

    const objects: Record<string, StoredObject> = {};

    for (const issue of issues) {
      // Skip archived objects
      if (issue.labels.some((label) => label.name === "archived")) {
        continue;
      }

      try {
        const objectId = this._getObjectIdFromLabels(issue);
        const data = JSON.parse(issue.body) as Json;

        const meta: ObjectMeta = {
          objectId,
          label: objectId,
          issueNumber: issue.number,
          createdAt: new Date(issue.created_at),
          updatedAt: new Date(issue.updated_at),
          version: await this._getVersion(issue.number) // shuold this just be issue._meta.version or something ilke that?
        };

        objects[objectId] = { meta, data };
      } catch (error) {
        // Skip issues that can't be processed
        continue;
      }
    }

    return objects;
  }

  async listUpdatedSince(timestamp: Date): Promise<Record<string, StoredObject>> {
    const issues = await this.fetchFromGitHub<Array<{
      number: number;
      body: string;
      created_at: string;
      updated_at: string;
      labels: Array<{ name: string }>;
    }>>("/issues", {
      method: "GET",
      params: {
        labels: this.config.baseLabel,
        state: "closed",
        since: timestamp.toISOString(),
      },
    });

    const objects: Record<string, StoredObject> = {};

    for (const issue of issues) {
      if (issue.labels.some((label) => label.name === "archived")) {
        continue;
      }

      try {
        const objectId = this._getObjectIdFromLabels(issue);
        const data = JSON.parse(issue.body) as Json;
        const updatedAt = new Date(issue.updated_at);

        if (updatedAt > timestamp) {
          const meta: ObjectMeta = {
            objectId,
            label: objectId,
            issueNumber: issue.number,
            createdAt: new Date(issue.created_at),
            updatedAt,
            version: await this._getVersion(issue.number)
          };

          objects[objectId] = { meta, data };
        }
      } catch (error) {
        // Skip issues that can't be processed
        continue;
      }
    }

    return objects;
  }

  async getObjectHistory(objectId: string): Promise<Array<{
    timestamp: string;
    type: string;
    data: Json;
    commentId: number;
  }>> {
    const issues = await this.fetchFromGitHub<Array<{
      number: number;
      labels: Array<{ name: string }>;
    }>>("/issues", {
      method: "GET",
      params: {
        labels: [this.config.baseLabel, `${this.config.uidPrefix}${objectId}`].join(","),
        state: "all",
      },
    });

    if (!issues || issues.length === 0) {
      throw new Error(`No object found with ID: ${objectId}`);
    }

    const issue = issues[0];
    const comments = await this.fetchFromGitHub<Array<{
      id: number;
      created_at: string;
      body: string;
    }>>(`/issues/${issue.number}/comments`);
    
    const history = [];

    for (const comment of comments) {
      try {
        const payload = JSON.parse(comment.body);
        let commentType = 'update';
        let commentData: Json;
        let metadata = {
          client_version: 'legacy',
          timestamp: comment.created_at,
          update_mode: 'append'
        };

        if (typeof payload === 'object') {
          if ('_data' in payload) {
            // New format with metadata
            commentType = payload.type || 'update';
            commentData = payload._data;
            metadata = payload._meta || metadata;
          } else if ('type' in payload && payload.type === 'initial_state') {
            // Old initial state format
            commentType = 'initial_state';
            commentData = payload.data;
          } else {
            // Legacy format
            commentData = payload;
          }
        } else {
          commentData = payload;
        }

        history.push({
          timestamp: comment.created_at,
          type: commentType,
          data: commentData,
          commentId: comment.id,
        });
      } catch (error) {
        // Skip comments with invalid JSON
        continue;
      }
    }

    return history;
  }

  private async _getVersion(issueNumber: number): Promise<number> {
    const comments = await this.fetchFromGitHub<Array<unknown>>(`/issues/${issueNumber}/comments`);
    return comments.length + 1;
  }

  private _getObjectIdFromLabels(issue: { labels: Array<{ name: string }> }): string {
      for (const label of issue.labels) {
        if (label.name !== this.config.baseLabel && label.name.startsWith(this.config.uidPrefix)) {
          return label.name.slice(this.config.uidPrefix.length);
        }
      }
      throw new Error(`No UID label found with prefix ${this.config.uidPrefix}`);
    }
}



---
File: typescript/src/config.ts
---
// typescript/src/config.ts
export interface StoreConfig {
  // Base configuration
  baseLabel: string;
  uidPrefix: string;
  
  // Reaction settings
  reactions: {
    processed: string;
    initialState: string;
  };
  
  // API retry settings
  retries: {
    maxAttempts: number;
    backoffFactor: number;
  };
  
  // Rate limiting
  rateLimit: {
    maxRequestsPerHour: number;
  };
}

export const DEFAULT_CONFIG: StoreConfig = {
  baseLabel: "stored-object",
  uidPrefix: "UID:",
  reactions: {
    processed: "+1",
    initialState: "rocket"
  },
  retries: {
    maxAttempts: 3,
    backoffFactor: 2
  },
  rateLimit: {
    maxRequestsPerHour: 1000
  }
};

export function mergeConfig(userConfig: Partial<StoreConfig>): StoreConfig {
  return {
    ...DEFAULT_CONFIG,
    ...userConfig,
    reactions: {
      ...DEFAULT_CONFIG.reactions,
      ...userConfig.reactions
    },
    retries: {
      ...DEFAULT_CONFIG.retries,
      ...userConfig.retries
    },
    rateLimit: {
      ...DEFAULT_CONFIG.rateLimit,
      ...userConfig.rateLimit
    }
  };
}

// Helper to validate token format
export function validateToken(token: string): boolean {
  // Check if it's a valid GitHub token format
  return /^gh[ps]_[a-zA-Z0-9]{36}$/.test(token);
}

// Helper to validate repository format
export function validateRepo(repo: string): boolean {
  return /^[\w-]+\/[\w-]+$/.test(repo);
}

// Error types for configuration issues
export class ConfigError extends Error {
  constructor(message: string) {
    super(message);
    this.name = 'ConfigError';
  }
}

export class TokenError extends ConfigError {
  constructor(message = 'Invalid GitHub token format') {
    super(message);
    this.name = 'TokenError';
  }
}

export class RepoError extends ConfigError {
  constructor(message = 'Invalid repository format. Use owner/repo') {
    super(message);
    this.name = 'RepoError';
  }
}



---
File: typescript/src/index.ts
---
// typescript/src/index.ts
export * from './client';
export * from './types';
export * from './config';
export * from './canonical'; 



---
File: typescript/src/logging.ts
---
// typescript/src/logging.ts
/**
 * Simple logger utility that avoids console statements
 * but collects messages for potential later use
 */

// Log levels
export enum LogLevel {
  ERROR = 'error',
  WARN = 'warn',
  INFO = 'info',
  DEBUG = 'debug'
}

// Logger configuration
export interface LoggerConfig {
  level: LogLevel;
  silent?: boolean;
  prefix?: string;
}

// Log entry structure
export interface LogEntry {
  timestamp: string;
  level: LogLevel;
  module: string;
  message: string;
  metadata?: Record<string, unknown>;
}

// Default configuration
const DEFAULT_CONFIG: LoggerConfig = {
  level: LogLevel.INFO,
  silent: false
};

// Mapping of log levels to numeric values for comparison
const LOG_LEVEL_VALUES: Record<LogLevel, number> = {
  [LogLevel.ERROR]: 3,
  [LogLevel.WARN]: 2,
  [LogLevel.INFO]: 1,
  [LogLevel.DEBUG]: 0
};

/**
 * Logger utility class that avoids direct console usage
 */
export class Logger {
  private moduleName: string;
  private config: LoggerConfig;
  private entries: LogEntry[] = [];

  /**
   * Create a new logger
   * @param moduleName Name of the module using this logger
   * @param config Optional configuration
   */
  constructor(moduleName: string, config: Partial<LoggerConfig> = {}) {
    this.moduleName = moduleName;
    this.config = {
      ...DEFAULT_CONFIG,
      ...config
    };
  }

  /**
   * Log a debug message
   * @param message Message content
   * @param meta Optional metadata
   */
  debug(message: string, meta?: Record<string, unknown>): void {
    this.log(LogLevel.DEBUG, message, meta);
  }

  /**
   * Log an info message
   * @param message Message content
   * @param meta Optional metadata
   */
  info(message: string, meta?: Record<string, unknown>): void {
    this.log(LogLevel.INFO, message, meta);
  }

  /**
   * Log a warning message
   * @param message Message content
   * @param meta Optional metadata
   */
  warn(message: string, meta?: Record<string, unknown>): void {
    this.log(LogLevel.WARN, message, meta);
  }

  /**
   * Log an error message
   * @param message Message content
   * @param meta Optional metadata
   */
  error(message: string, meta?: Record<string, unknown>): void {
    this.log(LogLevel.ERROR, message, meta);
  }

  /**
   * Internal helper method to record logs
   */
  private log(level: LogLevel, message: string, meta?: Record<string, unknown>): void {
    // Check if this log level should be processed
    if (LOG_LEVEL_VALUES[level] < LOG_LEVEL_VALUES[this.config.level]) {
      return;
    }

    const entry: LogEntry = {
      timestamp: new Date().toISOString(),
      level,
      module: this.moduleName,
      message,
      metadata: meta
    };

    this.entries.push(entry);

    // In production, you would implement external logging here
    // For example:
    // - Write to a database
    // - Send to a logging service
    // - Write to a file
  }

  /**
   * Get collected log entries
   */
  getEntries(): LogEntry[] {
    return [...this.entries];
  }

  /**
   * Clear collected log entries
   */
  clearEntries(): void {
    this.entries = [];
  }

  /**
   * Configure the logger
   * @param config Configuration options to apply
   */
  configure(config: Partial<LoggerConfig>): void {
    this.config = {
      ...this.config,
      ...config
    };
  }

  /**
   * Get the current logger configuration
   */
  getConfig(): LoggerConfig {
    return { ...this.config };
  }
}



---
File: typescript/src/types.ts
---
// typescript/src/types.ts
export type Json = { [key: string]: Json } | Json[] | string | number | boolean | null;

export interface ObjectMeta {
  objectId: string;
  label: string;
  issueNumber: number;  // Added field to track GitHub issue number
  createdAt: Date;
  updatedAt: Date;
  version: number;
}

export interface StoredObject {
  meta: ObjectMeta;
  data: Json;
}

export interface GitHubStoreConfig {
  baseLabel?: string;
  uidPrefix?: string;
  reactions?: {
    processed?: string;
    initialState?: string;
  };
}

export interface CommentMeta {
  client_version: string;
  timestamp: string;
  update_mode: string;
  issue_number: number;  // Added field to track GitHub issue number
}

export interface CommentPayload {
  _data: Json;
  _meta: CommentMeta;
  type?: string;
}

/* Constants */

export enum LabelNames {
  GH_STORE = "gh-store",
  STORED_OBJECT = "stored-object",
  DEPRECATED = "deprecated-object",
  UID_PREFIX = "UID:",
  ALIAS_TO_PREFIX = "ALIAS-TO:"
}



---
File: typescript/src/version.ts
---
// typescript/src/version.ts
// Version is updated by build process
export const CLIENT_VERSION = '0.3.1';



---
File: typescript/tsconfig.json
---
{
  "compilerOptions": {
    "target": "ES2020",
    "module": "ESNext",
    "lib": ["ES2020", "DOM"],
    "moduleResolution": "node",
    "esModuleInterop": true,
    "strict": true,
    "strictNullChecks": true,
    "strictFunctionTypes": true,
    "strictBindCallApply": true,
    "strictPropertyInitialization": true,
    "noImplicitThis": true,
    "alwaysStrict": true,
    "noUnusedLocals": true,
    "noUnusedParameters": true,
    "noImplicitReturns": true,
    "noFallthroughCasesInSwitch": true,
    "declaration": true,
    "declarationMap": true,
    "sourceMap": true,
    "outDir": "./dist",
    "rootDir": "./src",
    "baseUrl": ".",
    "skipLibCheck": true,
    "forceConsistentCasingInFileNames": true
  },
  "include": ["src/**/*"],
  "exclude": ["node_modules", "dist", "**/*.test.ts"]
}



---
File: typescript/tsup.config.ts
---
// tsup.config.ts
import { defineConfig } from 'tsup';

export default defineConfig({
  entry: ['src/index.ts'],
  format: ['cjs', 'esm'],
  dts: true,
  splitting: false,
  sourcemap: true,
  clean: true,
  minify: true,
  outDir: 'dist',
  outExtension: ({ format }) => ({
    js: format === 'esm' ? '.mjs' : '.cjs'
  })
});


